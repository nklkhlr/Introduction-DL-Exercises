{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNN)\n",
    "====================\n",
    "\n",
    "In this exercise we will work with Recurrent Neural Networks (RNN). A RNN is class of neural networks where the output not only depends on the current input but also on previous inputs along a given input sequence. This allows to exhibit temporal dynamic behaviour and contextual information in a sequence. Common applications for RNN are:\n",
    "\n",
    "- time series analysis\n",
    "- speech recognition\n",
    "- machine translation\n",
    "- image captioning\n",
    "\n",
    "\n",
    "Goal of this exercise\n",
    "========\n",
    "\n",
    "This exercise notebook should help you to experiment how recurrent neural networks are implemented, trained, and used for computer vision problems. Therefore, this notebook is structured as follows:\n",
    "1. Implement your own simple RNN class in Pytorch.\n",
    "2. Explore the backpropagation of the gradients in the RNN and discuss the vanishing gradient problem.\n",
    "3. Implement your own LSTM (Long-Short Term Memory) Network and show that this architecture improves the vanishing gradient problem.\n",
    "4. Build a RNN classifier for the MNIST dataset and train your model.\n",
    "5. Tune the hyperparameters of your model and submit your best model to the server to get bonus points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python:  3.6.4\n",
      "Using torch version:  0.4.1\n",
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print('Using python: ', platform.python_version())\n",
    "print('Using torch version: ', torch.__version__)\n",
    "print('Using device: ', device)\n",
    "# Machine: 2015 13\" Macbook Pro, i5 dual core"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Recurrent Neural Network\n",
    "\n",
    "The recurrent loops in a RNN allow relevant information to persist over time. A simple RNN architecture is shown here:\n",
    "<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png width=\"150\">\n",
    "\n",
    "A simple RNN takes not only an input X at time step t but also passes a hidden state that is the output of the previous time step into the network. The output of a RNN cell at time step t reads in Eq. 1:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "In this task you have to implement a simple one-layer RNN as a class in Pytorch, where you can choose a relu or tanh activation in the cell.You can see the architecture of a simple RNN in the figure below.\n",
    "\n",
    "\n",
    "<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Implement the RNN class\n",
    "from exercise_code.rnn.rnn_nn import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, Pytorch already has implemented a simple RNN in their library and you can call the RNN with <code>nn.RNN</code>. We will use the Pytorch RNN function to check if we have built the correct cell and compare the output of both functions. We also compare the running time of both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differnce between pytorch and your RNN implementation: 0.0\n",
      "Cool, you implemented a correct model.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import timeit\n",
    "\n",
    "# choose your network parameters\n",
    "input_size=3\n",
    "hidden_dim=3\n",
    "seq_len= 10 \n",
    "\n",
    "# define the two models\n",
    "pytorch_rnn=nn.RNN(input_size, hidden_dim)\n",
    "i2dl_rnn=RNN(input_size, hidden_dim)\n",
    "\n",
    "# initialise both rnn with same values\n",
    "for p in pytorch_rnn.parameters():\n",
    "    nn.init.constant_(p, val=0.3)\n",
    "for p in i2dl_rnn.parameters():\n",
    "    nn.init.constant_(p, val=0.3)\n",
    "    \n",
    "X=torch.randn(seq_len, 1, input_size)\n",
    "\n",
    "output_pytorch, h_pytorch = pytorch_rnn(X)\n",
    "output_i2dl, h_i2dl = i2dl_rnn(X)\n",
    "\n",
    "\n",
    "# The difference of outputs should be 0!!\n",
    "\n",
    "diff = torch.sum((output_pytorch-output_i2dl)**2)\n",
    "print(\"Differnce between pytorch and your RNN implementation: %s\" %diff.item())\n",
    "if diff.item()<10**-10:\n",
    "    print(\"Cool, you implemented a correct model.\")\n",
    "else:\n",
    "    print(\"Upps! There is something wrong in your model. Try again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Pytorch RNN 10000 runs: 5.552s\n",
      "Time I2DL RNN 10000 run: 5.728s\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "runs=10**4\n",
    "\n",
    "print(\"Time Pytorch RNN {} runs: {:.3f}s\".format(runs, timeit.timeit(\"pytorch_rnn(X)\", \n",
    "                                       setup=\"from __main__ import pytorch_rnn, X\", \n",
    "                                       number=runs))\n",
    "     )\n",
    "\n",
    "print(\"Time I2DL RNN {} run: {:.3f}s\".format(runs, timeit.timeit(\"i2dl_rnn(X)\", \n",
    "                                       setup=\"from __main__ import i2dl_rnn, X\", \n",
    "                                       number=runs))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we will use the Pytorch module that is faster and optimised in performance. However, it is always a good exercise to build the functions by yourself and we really advice you to do the exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradient\n",
    "\n",
    "As discussed in the lecture, the simple RNN suffers from vanishing gradients in the backpropagation. The hidden state is manipulated in every time step along the sequence and the effect of the past inputs to the final output vanishes with the distance in time. In the next cell we will explore the vanishing effect of previous inputs in the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11471b438>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############################################################################\n",
    "# TODO: Define a RNN and explore the gradients on the output h_T wrt. the  #\n",
    "# input at time t and plot your result. What behaviour do you observe?     #\n",
    "# Hints:                                                                   #\n",
    "#   - use one input feature                                                #\n",
    "#   - pytorch allows backward() pass wrt. to any vector                    #\n",
    "#   - backward() can only be applied to scalars and not to output tensors  #\n",
    "#   - choose a good representation of the gradient plot                    #\n",
    "############################################################################\n",
    "import torch.nn as nn\n",
    "import timeit\n",
    "\n",
    "hidden_size=1\n",
    "input_size= 1\n",
    "time_steps=50\n",
    "rnn=nn.RNN(input_size, hidden_size)\n",
    "X = torch.randn(time_steps, 1, input_size)\n",
    "X.requires_grad=True\n",
    "_,h=rnn(X)\n",
    "h.requires_grad\n",
    "h.sum().backward() \n",
    "grad_rnn=X.grad.view(-1)\n",
    "\n",
    "lstm=nn.LSTM(input_size, hidden_size)\n",
    "X = torch.randn(time_steps, 1, input_size) \n",
    "X.requires_grad=True\n",
    "_,(h, c)=lstm(X)\n",
    "h.sum().backward() \n",
    "grad_lstm=X.grad.view(-1)\n",
    "plt.semilogy(np.flip(abs(grad_lstm.detach().cpu().numpy()), axis = 0) , label=\"LSTM\") \n",
    "plt.semilogy(np.flip(abs(grad_rnn.detach().cpu().numpy()), axis = 0), label=\"RNN\") \n",
    "plt.legend()\n",
    "plt.xlabel(\"Time step t\")\n",
    "plt.ylabel(\"d h_T/d x_t\")\n",
    "plt.title(\"Log plot of gradient of output wrt. input\") \n",
    "plt.show()\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>It can be seen that the gradient of the of the output at time t wrt. to a previous input decreases exponentially. Hence, the final output does not change significantly for changes in the previous input and hence the RNN does not have memory.</p> \n",
    "<h3>Question</h3> \n",
    "<p>In order to better understand the vanishing gradient problem, calculate the gradients \n",
    "dh_t/dV, dh_t /dW, and dh_t/dX_0 analytically for t=3 and h_0=0 using Eq. 1. This exercise might seem a little bit tedious but it is really useful. Can you explain the vanishing gradient mathematically based on your findings?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Short Term Memory Network (LSTM)\n",
    "The vanishing gradient problem had been known for some time until Schmidhuber (1997) developed the Long-Short Term Memory Network and showed that this architecture can overcome the problem. <br> \n",
    "A LSTM is a more advanced recurrent network architecture that is able to learn long time dependencies. The architecture of a LSTM is composed of a forget, input, and output gate and the cell can remember values over arbitrary time intervals. Despite various different and exotic LSTM architectures, the standard LSTM cell is shwon in the figure below:\n",
    "\n",
    "\n",
    "<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to a simple RNN the LSTM cell has a hidden vector and an additional cell state vector. __What size does the cell state have?__ <br>\n",
    "The operations inside the LSTM are given as \n",
    "\n",
    "<img src=https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1  width=\"600\">\n",
    "where \n",
    "f_t: forget gate,  <br>\n",
    "i_t: input gate, <br>\n",
    "o_t: output gate, <br>\n",
    "h_t: hidden state vector, <br>\n",
    "c_t: cell state vector, <br>\n",
    "x_t: input vector, <br>\n",
    "t is time step, \n",
    "<br> \n",
    "<br> \n",
    "and<br> \n",
    "sigma_g: sigmoid activation <br> \n",
    "sigma_c and sigma_h: hyperbolic tangent function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step you should implement your own LSTM with the operations stated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of exercise_code.rnn.rnn_nn failed: Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 246, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 385, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 324, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 289, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 324, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 267, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: __init__() requires a code object with 0 free vars, not 1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# ToDo: Implement the RNN class\n",
    "from exercise_code.rnn.rnn_nn import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differnce between pytorch and your RNN implementation: 0.0\n",
      "Cool, you implemented a correct model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# choose your input parameters\n",
    "input_size=3\n",
    "hidden_dim=3\n",
    "seq_len= 10 \n",
    "\n",
    "# define the two models\n",
    "pytorch_lstm=nn.LSTM(input_size, hidden_dim)\n",
    "i2dl_lstm=LSTM(input_size, hidden_dim)\n",
    "\n",
    "# initialise both lstms with same values\n",
    "for p in pytorch_lstm.parameters():\n",
    "    nn.init.constant_(p, val=0.3)\n",
    "for p in i2dl_lstm.parameters():\n",
    "    nn.init.constant_(p, val=0.3)\n",
    "    \n",
    "X=torch.randn(seq_len, 1, input_size)\n",
    "\n",
    "output_pytorch, (h_pytorch, _) = pytorch_lstm(X)\n",
    "output_i2dl , (h_i2dl, _ )= i2dl_lstm(X)\n",
    "\n",
    "# The difference of outputs should be 0!!\n",
    "diff = torch.sum((output_pytorch-output_i2dl)**2)\n",
    "print(\"Differnce between pytorch and your RNN implementation: %s\" %diff.item())\n",
    "if diff.item()<10**-10:\n",
    "    print(\"Cool, you implemented a correct model.\")\n",
    "else:\n",
    "    print(\"Upps! There is something wrong in your model. Try again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of exercise_code.rnn.rnn_nn failed: Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 246, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 385, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 324, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 289, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 324, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 267, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: __init__() requires a code object with 0 free vars, not 1\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Pytorch LSTM 10000 runs: 11.494s\n",
      "Time I2DL LSTM 10000 run: 11.687s\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "runs=10**4\n",
    "\n",
    "print(\"Time Pytorch LSTM {} runs: {:.3f}s\".format(runs, timeit.timeit(\"pytorch_lstm(X)\", \n",
    "                                       setup=\"from __main__ import pytorch_lstm, X\", \n",
    "                                       number=runs))\n",
    "     )\n",
    "\n",
    "print(\"Time I2DL LSTM {} run: {:.3f}s\".format(runs, timeit.timeit(\"i2dl_lstm(X)\", \n",
    "                                       setup=\"from __main__ import i2dl_lstm, X\", \n",
    "                                       number=runs))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Gradients \n",
    "Analogously to the RNN, calculate the gradients of the input wrt. to the output of the LSTM and compare it against the RNN gradients. __What do you see?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# TODO: Define a RNN and LSTM and explore the gradients on the output h_T   #\n",
    "# wrt. the input at time t and plot your result.                           #\n",
    "############################################################################\n",
    "pass\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST image classification with RNNs\n",
    "\n",
    "In the previous exercises we already have classified images with a Fully Connected and Convolutional Network. In this exercise, we will solve the problem of image classification with a recurrent neural network.  \n",
    "\n",
    "For the experiment we use the MNIST handwritten digits dataset which we already know from the autoencoder exercise. This dataset consists of images of the 10 different digits (10 classes). The images have the resolution 28 x 28. The idea for the RNN classifier is to interpret the image as a sequence of rows. This means that we pass the rows through the RNN and use the final hidden state for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>\n",
    "    In this semester you have seen three different types of neural networks, namely Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and now Recurrent Neural Networks (RNNs). We have seen that we can use all three architectures for image classification. However, it turned out that some models are better than others for image classification. Try to think about advantages and disadvantages of the models, regarding # of parameters, transformations of the object in the image (scaling, rotation, translation,...), training time, testing time, over-fitting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "\n",
    "class Unsqueeze(object):\n",
    "    \"\"\"Adds a channel dimension that that our 2 dimensional input (H, W), \n",
    "    fits the 3 dimensional (H, W, C) expectations of pytorch's ToTensor function which\n",
    "    expects a PIL image. This is very inefficient but you most probably will use pytorch's\n",
    "    PIL image loader. Check out the documentation and make it more efficient :)\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension=0):\n",
    "        self.dimension = dimension\n",
    "    def __call__(self, numpy_array):\n",
    "        extended_array = np.expand_dims(numpy_array, self.dimension)\n",
    "        return extended_array\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + 'dimension={}'.format(dimension)\n",
    "\n",
    "    \n",
    "# transformation of data\n",
    "transform = transforms.Compose([\n",
    "    Unsqueeze(dimension=3),     \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "class MnistDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, \n",
    "                 transform=None):\n",
    "        super(MnistDataset, self).__init__()\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform: \n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "# loading the train data\n",
    "with open(\"datasets/mnist_train.p\", \"rb\") as f:\n",
    "    mnist_raw = pickle.load(f)\n",
    "\n",
    "X, y= mnist_raw\n",
    "############################################################################\n",
    "# TODO: Set a useful training/ validation split                            #\n",
    "############################################################################    \n",
    "\n",
    "train_split=0.75\n",
    "\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "train_dset=MnistDataset(X[:int(len(X)*train_split)], y[:int(len(X)*train_split)], transform=transform)\n",
    "val_dset=MnistDataset(X[int(len(X)*train_split):], y[int(len(X)*train_split):], transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1141e22e8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some examples from the dataset. Stolen from other notebooks\n",
    "# We show a few examples of training images from each class.\n",
    "X=train_dset.images\n",
    "y=train_dset.labels\n",
    "\n",
    "\n",
    "classes = list(range(10))\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 5\n",
    "for y_hat, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(train_dset.labels == y_hat)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y_hat + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X[idx])\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier based on a RNN where you sequentially feed the rows in the network and use the final hidden state for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://cdn-images-1.medium.com/max/800/1*Cm_c-I02rBa1rtLZXBhNUw.png width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.rnn.rnn_nn import RNN_Classifier\n",
    "model_rnn = RNN_Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.rnn.solver import Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN.\n",
      "torch.Size([28, 128, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (28) to match target batch_size (128).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-db369ff999ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# train rnn model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_nth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/i2dl/exercise_4/exercise_code/rnn/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_loader, val_loader, num_epochs, log_nth)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 862\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1405\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (28) to match target batch_size (128)."
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_dset,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                dataset=val_dset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "solver = Solver(optim_args={\"lr\": 1e-3})\n",
    "\n",
    "# train rnn model\n",
    "solver.train(model_rnn, train_loader, val_loader, log_nth=100, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your RNN classifier and try to tune the hyperparameters. With your simple RNN classifier you should exceed an accuracy higher than __90%__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to imporve your model by using a LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN.\n",
      "[Iteration 1/3520] TRAIN loss: 2.309\n",
      "[Iteration 2/3520] TRAIN loss: 2.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 3/3520] TRAIN loss: 2.302\n",
      "[Iteration 4/3520] TRAIN loss: 2.306\n",
      "[Iteration 5/3520] TRAIN loss: 2.316\n",
      "[Iteration 6/3520] TRAIN loss: 2.292\n",
      "[Iteration 7/3520] TRAIN loss: 2.309\n",
      "[Iteration 8/3520] TRAIN loss: 2.290\n",
      "[Iteration 9/3520] TRAIN loss: 2.282\n",
      "[Iteration 10/3520] TRAIN loss: 2.296\n",
      "[Iteration 11/3520] TRAIN loss: 2.283\n",
      "[Iteration 12/3520] TRAIN loss: 2.294\n",
      "[Iteration 13/3520] TRAIN loss: 2.301\n",
      "[Iteration 14/3520] TRAIN loss: 2.284\n",
      "[Iteration 15/3520] TRAIN loss: 2.273\n",
      "[Iteration 16/3520] TRAIN loss: 2.282\n",
      "[Iteration 17/3520] TRAIN loss: 2.260\n",
      "[Iteration 18/3520] TRAIN loss: 2.259\n",
      "[Iteration 19/3520] TRAIN loss: 2.267\n",
      "[Iteration 20/3520] TRAIN loss: 2.246\n",
      "[Iteration 21/3520] TRAIN loss: 2.225\n",
      "[Iteration 22/3520] TRAIN loss: 2.208\n",
      "[Iteration 23/3520] TRAIN loss: 2.190\n",
      "[Iteration 24/3520] TRAIN loss: 2.166\n",
      "[Iteration 25/3520] TRAIN loss: 2.155\n",
      "[Iteration 26/3520] TRAIN loss: 2.074\n",
      "[Iteration 27/3520] TRAIN loss: 2.115\n",
      "[Iteration 28/3520] TRAIN loss: 2.035\n",
      "[Iteration 29/3520] TRAIN loss: 2.010\n",
      "[Iteration 30/3520] TRAIN loss: 2.038\n",
      "[Iteration 31/3520] TRAIN loss: 1.975\n",
      "[Iteration 32/3520] TRAIN loss: 1.913\n",
      "[Iteration 33/3520] TRAIN loss: 1.880\n",
      "[Iteration 34/3520] TRAIN loss: 1.747\n",
      "[Iteration 35/3520] TRAIN loss: 1.777\n",
      "[Iteration 36/3520] TRAIN loss: 1.805\n",
      "[Iteration 37/3520] TRAIN loss: 1.627\n",
      "[Iteration 38/3520] TRAIN loss: 1.743\n",
      "[Iteration 39/3520] TRAIN loss: 1.624\n",
      "[Iteration 40/3520] TRAIN loss: 1.543\n",
      "[Iteration 41/3520] TRAIN loss: 1.575\n",
      "[Iteration 42/3520] TRAIN loss: 1.527\n",
      "[Iteration 43/3520] TRAIN loss: 1.594\n",
      "[Iteration 44/3520] TRAIN loss: 1.532\n",
      "[Iteration 45/3520] TRAIN loss: 1.367\n",
      "[Iteration 46/3520] TRAIN loss: 1.265\n",
      "[Iteration 47/3520] TRAIN loss: 1.412\n",
      "[Iteration 48/3520] TRAIN loss: 1.406\n",
      "[Iteration 49/3520] TRAIN loss: 1.378\n",
      "[Iteration 50/3520] TRAIN loss: 1.391\n",
      "[Iteration 51/3520] TRAIN loss: 1.235\n",
      "[Iteration 52/3520] TRAIN loss: 1.209\n",
      "[Iteration 53/3520] TRAIN loss: 1.184\n",
      "[Iteration 54/3520] TRAIN loss: 1.358\n",
      "[Iteration 55/3520] TRAIN loss: 1.076\n",
      "[Iteration 56/3520] TRAIN loss: 1.096\n",
      "[Iteration 57/3520] TRAIN loss: 1.069\n",
      "[Iteration 58/3520] TRAIN loss: 1.035\n",
      "[Iteration 59/3520] TRAIN loss: 1.052\n",
      "[Iteration 60/3520] TRAIN loss: 0.896\n",
      "[Iteration 61/3520] TRAIN loss: 0.980\n",
      "[Iteration 62/3520] TRAIN loss: 0.997\n",
      "[Iteration 63/3520] TRAIN loss: 0.986\n",
      "[Iteration 64/3520] TRAIN loss: 0.799\n",
      "[Iteration 65/3520] TRAIN loss: 0.833\n",
      "[Iteration 66/3520] TRAIN loss: 0.812\n",
      "[Iteration 67/3520] TRAIN loss: 0.948\n",
      "[Iteration 68/3520] TRAIN loss: 0.912\n",
      "[Iteration 69/3520] TRAIN loss: 0.838\n",
      "[Iteration 70/3520] TRAIN loss: 0.803\n",
      "[Iteration 71/3520] TRAIN loss: 0.910\n",
      "[Iteration 72/3520] TRAIN loss: 0.755\n",
      "[Iteration 73/3520] TRAIN loss: 0.758\n",
      "[Iteration 74/3520] TRAIN loss: 0.801\n",
      "[Iteration 75/3520] TRAIN loss: 0.672\n",
      "[Iteration 76/3520] TRAIN loss: 0.641\n",
      "[Iteration 77/3520] TRAIN loss: 0.724\n",
      "[Iteration 78/3520] TRAIN loss: 0.671\n",
      "[Iteration 79/3520] TRAIN loss: 0.558\n",
      "[Iteration 80/3520] TRAIN loss: 0.752\n",
      "[Iteration 81/3520] TRAIN loss: 0.672\n",
      "[Iteration 82/3520] TRAIN loss: 0.785\n",
      "[Iteration 83/3520] TRAIN loss: 0.717\n",
      "[Iteration 84/3520] TRAIN loss: 0.604\n",
      "[Iteration 85/3520] TRAIN loss: 0.664\n",
      "[Iteration 86/3520] TRAIN loss: 0.534\n",
      "[Iteration 87/3520] TRAIN loss: 0.674\n",
      "[Iteration 88/3520] TRAIN loss: 0.621\n",
      "[Iteration 89/3520] TRAIN loss: 0.647\n",
      "[Iteration 90/3520] TRAIN loss: 0.877\n",
      "[Iteration 91/3520] TRAIN loss: 0.699\n",
      "[Iteration 92/3520] TRAIN loss: 0.782\n",
      "[Iteration 93/3520] TRAIN loss: 0.591\n",
      "[Iteration 94/3520] TRAIN loss: 0.505\n",
      "[Iteration 95/3520] TRAIN loss: 0.738\n",
      "[Iteration 96/3520] TRAIN loss: 0.503\n",
      "[Iteration 97/3520] TRAIN loss: 0.795\n",
      "[Iteration 98/3520] TRAIN loss: 0.505\n",
      "[Iteration 99/3520] TRAIN loss: 0.607\n",
      "[Iteration 100/3520] TRAIN loss: 0.563\n",
      "[Iteration 101/3520] TRAIN loss: 0.590\n",
      "[Iteration 102/3520] TRAIN loss: 0.671\n",
      "[Iteration 103/3520] TRAIN loss: 0.610\n",
      "[Iteration 104/3520] TRAIN loss: 0.527\n",
      "[Iteration 105/3520] TRAIN loss: 0.706\n",
      "[Iteration 106/3520] TRAIN loss: 0.621\n",
      "[Iteration 107/3520] TRAIN loss: 0.533\n",
      "[Iteration 108/3520] TRAIN loss: 0.739\n",
      "[Iteration 109/3520] TRAIN loss: 0.450\n",
      "[Iteration 110/3520] TRAIN loss: 0.569\n",
      "[Iteration 111/3520] TRAIN loss: 0.573\n",
      "[Iteration 112/3520] TRAIN loss: 0.569\n",
      "[Iteration 113/3520] TRAIN loss: 0.566\n",
      "[Iteration 114/3520] TRAIN loss: 0.481\n",
      "[Iteration 115/3520] TRAIN loss: 0.551\n",
      "[Iteration 116/3520] TRAIN loss: 0.541\n",
      "[Iteration 117/3520] TRAIN loss: 0.441\n",
      "[Iteration 118/3520] TRAIN loss: 0.602\n",
      "[Iteration 119/3520] TRAIN loss: 0.563\n",
      "[Iteration 120/3520] TRAIN loss: 0.618\n",
      "[Iteration 121/3520] TRAIN loss: 0.400\n",
      "[Iteration 122/3520] TRAIN loss: 0.416\n",
      "[Iteration 123/3520] TRAIN loss: 0.517\n",
      "[Iteration 124/3520] TRAIN loss: 0.406\n",
      "[Iteration 125/3520] TRAIN loss: 0.483\n",
      "[Iteration 126/3520] TRAIN loss: 0.541\n",
      "[Iteration 127/3520] TRAIN loss: 0.460\n",
      "[Iteration 128/3520] TRAIN loss: 0.457\n",
      "[Iteration 129/3520] TRAIN loss: 0.328\n",
      "[Iteration 130/3520] TRAIN loss: 0.593\n",
      "[Iteration 131/3520] TRAIN loss: 0.439\n",
      "[Iteration 132/3520] TRAIN loss: 0.493\n",
      "[Iteration 133/3520] TRAIN loss: 0.556\n",
      "[Iteration 134/3520] TRAIN loss: 0.472\n",
      "[Iteration 135/3520] TRAIN loss: 0.394\n",
      "[Iteration 136/3520] TRAIN loss: 0.559\n",
      "[Iteration 137/3520] TRAIN loss: 0.424\n",
      "[Iteration 138/3520] TRAIN loss: 0.725\n",
      "[Iteration 139/3520] TRAIN loss: 0.370\n",
      "[Iteration 140/3520] TRAIN loss: 0.367\n",
      "[Iteration 141/3520] TRAIN loss: 0.482\n",
      "[Iteration 142/3520] TRAIN loss: 0.431\n",
      "[Iteration 143/3520] TRAIN loss: 0.418\n",
      "[Iteration 144/3520] TRAIN loss: 0.410\n",
      "[Iteration 145/3520] TRAIN loss: 0.430\n",
      "[Iteration 146/3520] TRAIN loss: 0.463\n",
      "[Iteration 147/3520] TRAIN loss: 0.503\n",
      "[Iteration 148/3520] TRAIN loss: 0.506\n",
      "[Iteration 149/3520] TRAIN loss: 0.382\n",
      "[Iteration 150/3520] TRAIN loss: 0.398\n",
      "[Iteration 151/3520] TRAIN loss: 0.408\n",
      "[Iteration 152/3520] TRAIN loss: 0.305\n",
      "[Iteration 153/3520] TRAIN loss: 0.501\n",
      "[Iteration 154/3520] TRAIN loss: 0.414\n",
      "[Iteration 155/3520] TRAIN loss: 0.497\n",
      "[Iteration 156/3520] TRAIN loss: 0.304\n",
      "[Iteration 157/3520] TRAIN loss: 0.503\n",
      "[Iteration 158/3520] TRAIN loss: 0.507\n",
      "[Iteration 159/3520] TRAIN loss: 0.346\n",
      "[Iteration 160/3520] TRAIN loss: 0.306\n",
      "[Iteration 161/3520] TRAIN loss: 0.321\n",
      "[Iteration 162/3520] TRAIN loss: 0.338\n",
      "[Iteration 163/3520] TRAIN loss: 0.394\n",
      "[Iteration 164/3520] TRAIN loss: 0.325\n",
      "[Iteration 165/3520] TRAIN loss: 0.369\n",
      "[Iteration 166/3520] TRAIN loss: 0.346\n",
      "[Iteration 167/3520] TRAIN loss: 0.274\n",
      "[Iteration 168/3520] TRAIN loss: 0.223\n",
      "[Iteration 169/3520] TRAIN loss: 0.434\n",
      "[Iteration 170/3520] TRAIN loss: 0.476\n",
      "[Iteration 171/3520] TRAIN loss: 0.379\n",
      "[Iteration 172/3520] TRAIN loss: 0.191\n",
      "[Iteration 173/3520] TRAIN loss: 0.233\n",
      "[Iteration 174/3520] TRAIN loss: 0.311\n",
      "[Iteration 175/3520] TRAIN loss: 0.269\n",
      "[Iteration 176/3520] TRAIN loss: 0.342\n",
      "[Iteration 177/3520] TRAIN loss: 0.218\n",
      "[Iteration 178/3520] TRAIN loss: 0.374\n",
      "[Iteration 179/3520] TRAIN loss: 0.315\n",
      "[Iteration 180/3520] TRAIN loss: 0.302\n",
      "[Iteration 181/3520] TRAIN loss: 0.355\n",
      "[Iteration 182/3520] TRAIN loss: 0.192\n",
      "[Iteration 183/3520] TRAIN loss: 0.256\n",
      "[Iteration 184/3520] TRAIN loss: 0.256\n",
      "[Iteration 185/3520] TRAIN loss: 0.440\n",
      "[Iteration 186/3520] TRAIN loss: 0.302\n",
      "[Iteration 187/3520] TRAIN loss: 0.449\n",
      "[Iteration 188/3520] TRAIN loss: 0.272\n",
      "[Iteration 189/3520] TRAIN loss: 0.318\n",
      "[Iteration 190/3520] TRAIN loss: 0.433\n",
      "[Iteration 191/3520] TRAIN loss: 0.418\n",
      "[Iteration 192/3520] TRAIN loss: 0.303\n",
      "[Iteration 193/3520] TRAIN loss: 0.447\n",
      "[Iteration 194/3520] TRAIN loss: 0.286\n",
      "[Iteration 195/3520] TRAIN loss: 0.311\n",
      "[Iteration 196/3520] TRAIN loss: 0.275\n",
      "[Iteration 197/3520] TRAIN loss: 0.408\n",
      "[Iteration 198/3520] TRAIN loss: 0.366\n",
      "[Iteration 199/3520] TRAIN loss: 0.268\n",
      "[Iteration 200/3520] TRAIN loss: 0.396\n",
      "[Iteration 201/3520] TRAIN loss: 0.326\n",
      "[Iteration 202/3520] TRAIN loss: 0.403\n",
      "[Iteration 203/3520] TRAIN loss: 0.249\n",
      "[Iteration 204/3520] TRAIN loss: 0.255\n",
      "[Iteration 205/3520] TRAIN loss: 0.270\n",
      "[Iteration 206/3520] TRAIN loss: 0.439\n",
      "[Iteration 207/3520] TRAIN loss: 0.395\n",
      "[Iteration 208/3520] TRAIN loss: 0.288\n",
      "[Iteration 209/3520] TRAIN loss: 0.376\n",
      "[Iteration 210/3520] TRAIN loss: 0.291\n",
      "[Iteration 211/3520] TRAIN loss: 0.310\n",
      "[Iteration 212/3520] TRAIN loss: 0.425\n",
      "[Iteration 213/3520] TRAIN loss: 0.364\n",
      "[Iteration 214/3520] TRAIN loss: 0.450\n",
      "[Iteration 215/3520] TRAIN loss: 0.404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 216/3520] TRAIN loss: 0.300\n",
      "[Iteration 217/3520] TRAIN loss: 0.365\n",
      "[Iteration 218/3520] TRAIN loss: 0.260\n",
      "[Iteration 219/3520] TRAIN loss: 0.192\n",
      "[Iteration 220/3520] TRAIN loss: 0.312\n",
      "[Iteration 221/3520] TRAIN loss: 0.308\n",
      "[Iteration 222/3520] TRAIN loss: 0.272\n",
      "[Iteration 223/3520] TRAIN loss: 0.292\n",
      "[Iteration 224/3520] TRAIN loss: 0.355\n",
      "[Iteration 225/3520] TRAIN loss: 0.298\n",
      "[Iteration 226/3520] TRAIN loss: 0.441\n",
      "[Iteration 227/3520] TRAIN loss: 0.271\n",
      "[Iteration 228/3520] TRAIN loss: 0.272\n",
      "[Iteration 229/3520] TRAIN loss: 0.351\n",
      "[Iteration 230/3520] TRAIN loss: 0.432\n",
      "[Iteration 231/3520] TRAIN loss: 0.242\n",
      "[Iteration 232/3520] TRAIN loss: 0.368\n",
      "[Iteration 233/3520] TRAIN loss: 0.280\n",
      "[Iteration 234/3520] TRAIN loss: 0.234\n",
      "[Iteration 235/3520] TRAIN loss: 0.263\n",
      "[Iteration 236/3520] TRAIN loss: 0.351\n",
      "[Iteration 237/3520] TRAIN loss: 0.350\n",
      "[Iteration 238/3520] TRAIN loss: 0.274\n",
      "[Iteration 239/3520] TRAIN loss: 0.336\n",
      "[Iteration 240/3520] TRAIN loss: 0.229\n",
      "[Iteration 241/3520] TRAIN loss: 0.301\n",
      "[Iteration 242/3520] TRAIN loss: 0.311\n",
      "[Iteration 243/3520] TRAIN loss: 0.232\n",
      "[Iteration 244/3520] TRAIN loss: 0.323\n",
      "[Iteration 245/3520] TRAIN loss: 0.248\n",
      "[Iteration 246/3520] TRAIN loss: 0.247\n",
      "[Iteration 247/3520] TRAIN loss: 0.298\n",
      "[Iteration 248/3520] TRAIN loss: 0.353\n",
      "[Iteration 249/3520] TRAIN loss: 0.234\n",
      "[Iteration 250/3520] TRAIN loss: 0.243\n",
      "[Iteration 251/3520] TRAIN loss: 0.206\n",
      "[Iteration 252/3520] TRAIN loss: 0.243\n",
      "[Iteration 253/3520] TRAIN loss: 0.182\n",
      "[Iteration 254/3520] TRAIN loss: 0.319\n",
      "[Iteration 255/3520] TRAIN loss: 0.215\n",
      "[Iteration 256/3520] TRAIN loss: 0.224\n",
      "[Iteration 257/3520] TRAIN loss: 0.252\n",
      "[Iteration 258/3520] TRAIN loss: 0.342\n",
      "[Iteration 259/3520] TRAIN loss: 0.341\n",
      "[Iteration 260/3520] TRAIN loss: 0.267\n",
      "[Iteration 261/3520] TRAIN loss: 0.242\n",
      "[Iteration 262/3520] TRAIN loss: 0.177\n",
      "[Iteration 263/3520] TRAIN loss: 0.147\n",
      "[Iteration 264/3520] TRAIN loss: 0.119\n",
      "[Iteration 265/3520] TRAIN loss: 0.230\n",
      "[Iteration 266/3520] TRAIN loss: 0.209\n",
      "[Iteration 267/3520] TRAIN loss: 0.286\n",
      "[Iteration 268/3520] TRAIN loss: 0.239\n",
      "[Iteration 269/3520] TRAIN loss: 0.205\n",
      "[Iteration 270/3520] TRAIN loss: 0.280\n",
      "[Iteration 271/3520] TRAIN loss: 0.279\n",
      "[Iteration 272/3520] TRAIN loss: 0.182\n",
      "[Iteration 273/3520] TRAIN loss: 0.135\n",
      "[Iteration 274/3520] TRAIN loss: 0.261\n",
      "[Iteration 275/3520] TRAIN loss: 0.224\n",
      "[Iteration 276/3520] TRAIN loss: 0.256\n",
      "[Iteration 277/3520] TRAIN loss: 0.194\n",
      "[Iteration 278/3520] TRAIN loss: 0.293\n",
      "[Iteration 279/3520] TRAIN loss: 0.190\n",
      "[Iteration 280/3520] TRAIN loss: 0.192\n",
      "[Iteration 281/3520] TRAIN loss: 0.238\n",
      "[Iteration 282/3520] TRAIN loss: 0.225\n",
      "[Iteration 283/3520] TRAIN loss: 0.265\n",
      "[Iteration 284/3520] TRAIN loss: 0.137\n",
      "[Iteration 285/3520] TRAIN loss: 0.198\n",
      "[Iteration 286/3520] TRAIN loss: 0.176\n",
      "[Iteration 287/3520] TRAIN loss: 0.148\n",
      "[Iteration 288/3520] TRAIN loss: 0.340\n",
      "[Iteration 289/3520] TRAIN loss: 0.279\n",
      "[Iteration 290/3520] TRAIN loss: 0.228\n",
      "[Iteration 291/3520] TRAIN loss: 0.280\n",
      "[Iteration 292/3520] TRAIN loss: 0.228\n",
      "[Iteration 293/3520] TRAIN loss: 0.180\n",
      "[Iteration 294/3520] TRAIN loss: 0.396\n",
      "[Iteration 295/3520] TRAIN loss: 0.289\n",
      "[Iteration 296/3520] TRAIN loss: 0.295\n",
      "[Iteration 297/3520] TRAIN loss: 0.165\n",
      "[Iteration 298/3520] TRAIN loss: 0.345\n",
      "[Iteration 299/3520] TRAIN loss: 0.242\n",
      "[Iteration 300/3520] TRAIN loss: 0.260\n",
      "[Iteration 301/3520] TRAIN loss: 0.266\n",
      "[Iteration 302/3520] TRAIN loss: 0.209\n",
      "[Iteration 303/3520] TRAIN loss: 0.226\n",
      "[Iteration 304/3520] TRAIN loss: 0.183\n",
      "[Iteration 305/3520] TRAIN loss: 0.253\n",
      "[Iteration 306/3520] TRAIN loss: 0.145\n",
      "[Iteration 307/3520] TRAIN loss: 0.226\n",
      "[Iteration 308/3520] TRAIN loss: 0.110\n",
      "[Iteration 309/3520] TRAIN loss: 0.198\n",
      "[Iteration 310/3520] TRAIN loss: 0.114\n",
      "[Iteration 311/3520] TRAIN loss: 0.176\n",
      "[Iteration 312/3520] TRAIN loss: 0.174\n",
      "[Iteration 313/3520] TRAIN loss: 0.325\n",
      "[Iteration 314/3520] TRAIN loss: 0.216\n",
      "[Iteration 315/3520] TRAIN loss: 0.197\n",
      "[Iteration 316/3520] TRAIN loss: 0.266\n",
      "[Iteration 317/3520] TRAIN loss: 0.256\n",
      "[Iteration 318/3520] TRAIN loss: 0.145\n",
      "[Iteration 319/3520] TRAIN loss: 0.191\n",
      "[Iteration 320/3520] TRAIN loss: 0.226\n",
      "[Iteration 321/3520] TRAIN loss: 0.263\n",
      "[Iteration 322/3520] TRAIN loss: 0.279\n",
      "[Iteration 323/3520] TRAIN loss: 0.295\n",
      "[Iteration 324/3520] TRAIN loss: 0.192\n",
      "[Iteration 325/3520] TRAIN loss: 0.164\n",
      "[Iteration 326/3520] TRAIN loss: 0.226\n",
      "[Iteration 327/3520] TRAIN loss: 0.255\n",
      "[Iteration 328/3520] TRAIN loss: 0.279\n",
      "[Iteration 329/3520] TRAIN loss: 0.160\n",
      "[Iteration 330/3520] TRAIN loss: 0.372\n",
      "[Iteration 331/3520] TRAIN loss: 0.174\n",
      "[Iteration 332/3520] TRAIN loss: 0.262\n",
      "[Iteration 333/3520] TRAIN loss: 0.267\n",
      "[Iteration 334/3520] TRAIN loss: 0.226\n",
      "[Iteration 335/3520] TRAIN loss: 0.296\n",
      "[Iteration 336/3520] TRAIN loss: 0.195\n",
      "[Iteration 337/3520] TRAIN loss: 0.180\n",
      "[Iteration 338/3520] TRAIN loss: 0.196\n",
      "[Iteration 339/3520] TRAIN loss: 0.213\n",
      "[Iteration 340/3520] TRAIN loss: 0.183\n",
      "[Iteration 341/3520] TRAIN loss: 0.283\n",
      "[Iteration 342/3520] TRAIN loss: 0.272\n",
      "[Iteration 343/3520] TRAIN loss: 0.252\n",
      "[Iteration 344/3520] TRAIN loss: 0.254\n",
      "[Iteration 345/3520] TRAIN loss: 0.244\n",
      "[Iteration 346/3520] TRAIN loss: 0.210\n",
      "[Iteration 347/3520] TRAIN loss: 0.290\n",
      "[Iteration 348/3520] TRAIN loss: 0.249\n",
      "[Iteration 349/3520] TRAIN loss: 0.328\n",
      "[Iteration 350/3520] TRAIN loss: 0.163\n",
      "[Iteration 351/3520] TRAIN loss: 0.173\n",
      "[Iteration 352/3520] TRAIN loss: 0.225\n",
      "[Epoch 1/10] TRAIN acc/loss: 0.958/0.225\n",
      "[Epoch 1/10] VAL   acc/loss: 0.938/0.205\n",
      "[Iteration 353/3520] TRAIN loss: 0.088\n",
      "[Iteration 354/3520] TRAIN loss: 0.157\n",
      "[Iteration 355/3520] TRAIN loss: 0.191\n",
      "[Iteration 356/3520] TRAIN loss: 0.120\n",
      "[Iteration 357/3520] TRAIN loss: 0.177\n",
      "[Iteration 358/3520] TRAIN loss: 0.163\n",
      "[Iteration 359/3520] TRAIN loss: 0.238\n",
      "[Iteration 360/3520] TRAIN loss: 0.346\n",
      "[Iteration 361/3520] TRAIN loss: 0.223\n",
      "[Iteration 362/3520] TRAIN loss: 0.306\n",
      "[Iteration 363/3520] TRAIN loss: 0.272\n",
      "[Iteration 364/3520] TRAIN loss: 0.150\n",
      "[Iteration 365/3520] TRAIN loss: 0.151\n",
      "[Iteration 366/3520] TRAIN loss: 0.240\n",
      "[Iteration 367/3520] TRAIN loss: 0.165\n",
      "[Iteration 368/3520] TRAIN loss: 0.228\n",
      "[Iteration 369/3520] TRAIN loss: 0.174\n",
      "[Iteration 370/3520] TRAIN loss: 0.153\n",
      "[Iteration 371/3520] TRAIN loss: 0.158\n",
      "[Iteration 372/3520] TRAIN loss: 0.151\n",
      "[Iteration 373/3520] TRAIN loss: 0.198\n",
      "[Iteration 374/3520] TRAIN loss: 0.124\n",
      "[Iteration 375/3520] TRAIN loss: 0.250\n",
      "[Iteration 376/3520] TRAIN loss: 0.092\n",
      "[Iteration 377/3520] TRAIN loss: 0.097\n",
      "[Iteration 378/3520] TRAIN loss: 0.190\n",
      "[Iteration 379/3520] TRAIN loss: 0.195\n",
      "[Iteration 380/3520] TRAIN loss: 0.122\n",
      "[Iteration 381/3520] TRAIN loss: 0.194\n",
      "[Iteration 382/3520] TRAIN loss: 0.205\n",
      "[Iteration 383/3520] TRAIN loss: 0.130\n",
      "[Iteration 384/3520] TRAIN loss: 0.274\n",
      "[Iteration 385/3520] TRAIN loss: 0.211\n",
      "[Iteration 386/3520] TRAIN loss: 0.135\n",
      "[Iteration 387/3520] TRAIN loss: 0.248\n",
      "[Iteration 388/3520] TRAIN loss: 0.205\n",
      "[Iteration 389/3520] TRAIN loss: 0.248\n",
      "[Iteration 390/3520] TRAIN loss: 0.204\n",
      "[Iteration 391/3520] TRAIN loss: 0.195\n",
      "[Iteration 392/3520] TRAIN loss: 0.145\n",
      "[Iteration 393/3520] TRAIN loss: 0.190\n",
      "[Iteration 394/3520] TRAIN loss: 0.183\n",
      "[Iteration 395/3520] TRAIN loss: 0.259\n",
      "[Iteration 396/3520] TRAIN loss: 0.167\n",
      "[Iteration 397/3520] TRAIN loss: 0.302\n",
      "[Iteration 398/3520] TRAIN loss: 0.189\n",
      "[Iteration 399/3520] TRAIN loss: 0.129\n",
      "[Iteration 400/3520] TRAIN loss: 0.209\n",
      "[Iteration 401/3520] TRAIN loss: 0.237\n",
      "[Iteration 402/3520] TRAIN loss: 0.160\n",
      "[Iteration 403/3520] TRAIN loss: 0.104\n",
      "[Iteration 404/3520] TRAIN loss: 0.118\n",
      "[Iteration 405/3520] TRAIN loss: 0.200\n",
      "[Iteration 406/3520] TRAIN loss: 0.262\n",
      "[Iteration 407/3520] TRAIN loss: 0.127\n",
      "[Iteration 408/3520] TRAIN loss: 0.228\n",
      "[Iteration 409/3520] TRAIN loss: 0.157\n",
      "[Iteration 410/3520] TRAIN loss: 0.221\n",
      "[Iteration 411/3520] TRAIN loss: 0.153\n",
      "[Iteration 412/3520] TRAIN loss: 0.251\n",
      "[Iteration 413/3520] TRAIN loss: 0.219\n",
      "[Iteration 414/3520] TRAIN loss: 0.168\n",
      "[Iteration 415/3520] TRAIN loss: 0.291\n",
      "[Iteration 416/3520] TRAIN loss: 0.111\n",
      "[Iteration 417/3520] TRAIN loss: 0.150\n",
      "[Iteration 418/3520] TRAIN loss: 0.184\n",
      "[Iteration 419/3520] TRAIN loss: 0.198\n",
      "[Iteration 420/3520] TRAIN loss: 0.131\n",
      "[Iteration 421/3520] TRAIN loss: 0.199\n",
      "[Iteration 422/3520] TRAIN loss: 0.227\n",
      "[Iteration 423/3520] TRAIN loss: 0.266\n",
      "[Iteration 424/3520] TRAIN loss: 0.249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 425/3520] TRAIN loss: 0.163\n",
      "[Iteration 426/3520] TRAIN loss: 0.153\n",
      "[Iteration 427/3520] TRAIN loss: 0.267\n",
      "[Iteration 428/3520] TRAIN loss: 0.114\n",
      "[Iteration 429/3520] TRAIN loss: 0.209\n",
      "[Iteration 430/3520] TRAIN loss: 0.189\n",
      "[Iteration 431/3520] TRAIN loss: 0.117\n",
      "[Iteration 432/3520] TRAIN loss: 0.219\n",
      "[Iteration 433/3520] TRAIN loss: 0.183\n",
      "[Iteration 434/3520] TRAIN loss: 0.157\n",
      "[Iteration 435/3520] TRAIN loss: 0.188\n",
      "[Iteration 436/3520] TRAIN loss: 0.126\n",
      "[Iteration 437/3520] TRAIN loss: 0.177\n",
      "[Iteration 438/3520] TRAIN loss: 0.224\n",
      "[Iteration 439/3520] TRAIN loss: 0.206\n",
      "[Iteration 440/3520] TRAIN loss: 0.161\n",
      "[Iteration 441/3520] TRAIN loss: 0.247\n",
      "[Iteration 442/3520] TRAIN loss: 0.113\n",
      "[Iteration 443/3520] TRAIN loss: 0.092\n",
      "[Iteration 444/3520] TRAIN loss: 0.269\n",
      "[Iteration 445/3520] TRAIN loss: 0.174\n",
      "[Iteration 446/3520] TRAIN loss: 0.101\n",
      "[Iteration 447/3520] TRAIN loss: 0.095\n",
      "[Iteration 448/3520] TRAIN loss: 0.205\n",
      "[Iteration 449/3520] TRAIN loss: 0.244\n",
      "[Iteration 450/3520] TRAIN loss: 0.140\n",
      "[Iteration 451/3520] TRAIN loss: 0.170\n",
      "[Iteration 452/3520] TRAIN loss: 0.195\n",
      "[Iteration 453/3520] TRAIN loss: 0.199\n",
      "[Iteration 454/3520] TRAIN loss: 0.116\n",
      "[Iteration 455/3520] TRAIN loss: 0.177\n",
      "[Iteration 456/3520] TRAIN loss: 0.116\n",
      "[Iteration 457/3520] TRAIN loss: 0.158\n",
      "[Iteration 458/3520] TRAIN loss: 0.092\n",
      "[Iteration 459/3520] TRAIN loss: 0.101\n",
      "[Iteration 460/3520] TRAIN loss: 0.208\n",
      "[Iteration 461/3520] TRAIN loss: 0.152\n",
      "[Iteration 462/3520] TRAIN loss: 0.242\n",
      "[Iteration 463/3520] TRAIN loss: 0.154\n",
      "[Iteration 464/3520] TRAIN loss: 0.114\n",
      "[Iteration 465/3520] TRAIN loss: 0.235\n",
      "[Iteration 466/3520] TRAIN loss: 0.086\n",
      "[Iteration 467/3520] TRAIN loss: 0.230\n",
      "[Iteration 468/3520] TRAIN loss: 0.166\n",
      "[Iteration 469/3520] TRAIN loss: 0.084\n",
      "[Iteration 470/3520] TRAIN loss: 0.223\n",
      "[Iteration 471/3520] TRAIN loss: 0.218\n",
      "[Iteration 472/3520] TRAIN loss: 0.046\n",
      "[Iteration 473/3520] TRAIN loss: 0.114\n",
      "[Iteration 474/3520] TRAIN loss: 0.146\n",
      "[Iteration 475/3520] TRAIN loss: 0.206\n",
      "[Iteration 476/3520] TRAIN loss: 0.091\n",
      "[Iteration 477/3520] TRAIN loss: 0.078\n",
      "[Iteration 478/3520] TRAIN loss: 0.104\n",
      "[Iteration 479/3520] TRAIN loss: 0.194\n",
      "[Iteration 480/3520] TRAIN loss: 0.194\n",
      "[Iteration 481/3520] TRAIN loss: 0.109\n",
      "[Iteration 482/3520] TRAIN loss: 0.143\n",
      "[Iteration 483/3520] TRAIN loss: 0.187\n",
      "[Iteration 484/3520] TRAIN loss: 0.210\n",
      "[Iteration 485/3520] TRAIN loss: 0.084\n",
      "[Iteration 486/3520] TRAIN loss: 0.125\n",
      "[Iteration 487/3520] TRAIN loss: 0.091\n",
      "[Iteration 488/3520] TRAIN loss: 0.099\n",
      "[Iteration 489/3520] TRAIN loss: 0.089\n",
      "[Iteration 490/3520] TRAIN loss: 0.108\n",
      "[Iteration 491/3520] TRAIN loss: 0.191\n",
      "[Iteration 492/3520] TRAIN loss: 0.186\n",
      "[Iteration 493/3520] TRAIN loss: 0.251\n",
      "[Iteration 494/3520] TRAIN loss: 0.165\n",
      "[Iteration 495/3520] TRAIN loss: 0.226\n",
      "[Iteration 496/3520] TRAIN loss: 0.180\n",
      "[Iteration 497/3520] TRAIN loss: 0.123\n",
      "[Iteration 498/3520] TRAIN loss: 0.131\n",
      "[Iteration 499/3520] TRAIN loss: 0.180\n",
      "[Iteration 500/3520] TRAIN loss: 0.140\n",
      "[Iteration 501/3520] TRAIN loss: 0.239\n",
      "[Iteration 502/3520] TRAIN loss: 0.120\n",
      "[Iteration 503/3520] TRAIN loss: 0.247\n",
      "[Iteration 504/3520] TRAIN loss: 0.111\n",
      "[Iteration 505/3520] TRAIN loss: 0.124\n",
      "[Iteration 506/3520] TRAIN loss: 0.125\n",
      "[Iteration 507/3520] TRAIN loss: 0.143\n",
      "[Iteration 508/3520] TRAIN loss: 0.148\n",
      "[Iteration 509/3520] TRAIN loss: 0.257\n",
      "[Iteration 510/3520] TRAIN loss: 0.189\n",
      "[Iteration 511/3520] TRAIN loss: 0.181\n",
      "[Iteration 512/3520] TRAIN loss: 0.189\n",
      "[Iteration 513/3520] TRAIN loss: 0.242\n",
      "[Iteration 514/3520] TRAIN loss: 0.249\n",
      "[Iteration 515/3520] TRAIN loss: 0.163\n",
      "[Iteration 516/3520] TRAIN loss: 0.182\n",
      "[Iteration 517/3520] TRAIN loss: 0.201\n",
      "[Iteration 518/3520] TRAIN loss: 0.130\n",
      "[Iteration 519/3520] TRAIN loss: 0.084\n",
      "[Iteration 520/3520] TRAIN loss: 0.150\n",
      "[Iteration 521/3520] TRAIN loss: 0.093\n",
      "[Iteration 522/3520] TRAIN loss: 0.170\n",
      "[Iteration 523/3520] TRAIN loss: 0.160\n",
      "[Iteration 524/3520] TRAIN loss: 0.107\n",
      "[Iteration 525/3520] TRAIN loss: 0.101\n",
      "[Iteration 526/3520] TRAIN loss: 0.089\n",
      "[Iteration 527/3520] TRAIN loss: 0.251\n",
      "[Iteration 528/3520] TRAIN loss: 0.136\n",
      "[Iteration 529/3520] TRAIN loss: 0.134\n",
      "[Iteration 530/3520] TRAIN loss: 0.148\n",
      "[Iteration 531/3520] TRAIN loss: 0.211\n",
      "[Iteration 532/3520] TRAIN loss: 0.191\n",
      "[Iteration 533/3520] TRAIN loss: 0.095\n",
      "[Iteration 534/3520] TRAIN loss: 0.191\n",
      "[Iteration 535/3520] TRAIN loss: 0.066\n",
      "[Iteration 536/3520] TRAIN loss: 0.067\n",
      "[Iteration 537/3520] TRAIN loss: 0.144\n",
      "[Iteration 538/3520] TRAIN loss: 0.054\n",
      "[Iteration 539/3520] TRAIN loss: 0.180\n",
      "[Iteration 540/3520] TRAIN loss: 0.159\n",
      "[Iteration 541/3520] TRAIN loss: 0.203\n",
      "[Iteration 542/3520] TRAIN loss: 0.167\n",
      "[Iteration 543/3520] TRAIN loss: 0.283\n",
      "[Iteration 544/3520] TRAIN loss: 0.159\n",
      "[Iteration 545/3520] TRAIN loss: 0.196\n",
      "[Iteration 546/3520] TRAIN loss: 0.215\n",
      "[Iteration 547/3520] TRAIN loss: 0.191\n",
      "[Iteration 548/3520] TRAIN loss: 0.100\n",
      "[Iteration 549/3520] TRAIN loss: 0.222\n",
      "[Iteration 550/3520] TRAIN loss: 0.177\n",
      "[Iteration 551/3520] TRAIN loss: 0.195\n",
      "[Iteration 552/3520] TRAIN loss: 0.251\n",
      "[Iteration 553/3520] TRAIN loss: 0.082\n",
      "[Iteration 554/3520] TRAIN loss: 0.193\n",
      "[Iteration 555/3520] TRAIN loss: 0.153\n",
      "[Iteration 556/3520] TRAIN loss: 0.111\n",
      "[Iteration 557/3520] TRAIN loss: 0.234\n",
      "[Iteration 558/3520] TRAIN loss: 0.134\n",
      "[Iteration 559/3520] TRAIN loss: 0.137\n",
      "[Iteration 560/3520] TRAIN loss: 0.184\n",
      "[Iteration 561/3520] TRAIN loss: 0.142\n",
      "[Iteration 562/3520] TRAIN loss: 0.143\n",
      "[Iteration 563/3520] TRAIN loss: 0.156\n",
      "[Iteration 564/3520] TRAIN loss: 0.183\n",
      "[Iteration 565/3520] TRAIN loss: 0.175\n",
      "[Iteration 566/3520] TRAIN loss: 0.194\n",
      "[Iteration 567/3520] TRAIN loss: 0.178\n",
      "[Iteration 568/3520] TRAIN loss: 0.104\n",
      "[Iteration 569/3520] TRAIN loss: 0.119\n",
      "[Iteration 570/3520] TRAIN loss: 0.229\n",
      "[Iteration 571/3520] TRAIN loss: 0.170\n",
      "[Iteration 572/3520] TRAIN loss: 0.225\n",
      "[Iteration 573/3520] TRAIN loss: 0.204\n",
      "[Iteration 574/3520] TRAIN loss: 0.124\n",
      "[Iteration 575/3520] TRAIN loss: 0.150\n",
      "[Iteration 576/3520] TRAIN loss: 0.088\n",
      "[Iteration 577/3520] TRAIN loss: 0.144\n",
      "[Iteration 578/3520] TRAIN loss: 0.082\n",
      "[Iteration 579/3520] TRAIN loss: 0.135\n",
      "[Iteration 580/3520] TRAIN loss: 0.171\n",
      "[Iteration 581/3520] TRAIN loss: 0.145\n",
      "[Iteration 582/3520] TRAIN loss: 0.087\n",
      "[Iteration 583/3520] TRAIN loss: 0.066\n",
      "[Iteration 584/3520] TRAIN loss: 0.146\n",
      "[Iteration 585/3520] TRAIN loss: 0.153\n",
      "[Iteration 586/3520] TRAIN loss: 0.200\n",
      "[Iteration 587/3520] TRAIN loss: 0.159\n",
      "[Iteration 588/3520] TRAIN loss: 0.052\n",
      "[Iteration 589/3520] TRAIN loss: 0.172\n",
      "[Iteration 590/3520] TRAIN loss: 0.109\n",
      "[Iteration 591/3520] TRAIN loss: 0.099\n",
      "[Iteration 592/3520] TRAIN loss: 0.075\n",
      "[Iteration 593/3520] TRAIN loss: 0.179\n",
      "[Iteration 594/3520] TRAIN loss: 0.096\n",
      "[Iteration 595/3520] TRAIN loss: 0.111\n",
      "[Iteration 596/3520] TRAIN loss: 0.112\n",
      "[Iteration 597/3520] TRAIN loss: 0.075\n",
      "[Iteration 598/3520] TRAIN loss: 0.174\n",
      "[Iteration 599/3520] TRAIN loss: 0.090\n",
      "[Iteration 600/3520] TRAIN loss: 0.078\n",
      "[Iteration 601/3520] TRAIN loss: 0.144\n",
      "[Iteration 602/3520] TRAIN loss: 0.109\n",
      "[Iteration 603/3520] TRAIN loss: 0.086\n",
      "[Iteration 604/3520] TRAIN loss: 0.178\n",
      "[Iteration 605/3520] TRAIN loss: 0.135\n",
      "[Iteration 606/3520] TRAIN loss: 0.161\n",
      "[Iteration 607/3520] TRAIN loss: 0.080\n",
      "[Iteration 608/3520] TRAIN loss: 0.227\n",
      "[Iteration 609/3520] TRAIN loss: 0.295\n",
      "[Iteration 610/3520] TRAIN loss: 0.222\n",
      "[Iteration 611/3520] TRAIN loss: 0.095\n",
      "[Iteration 612/3520] TRAIN loss: 0.126\n",
      "[Iteration 613/3520] TRAIN loss: 0.102\n",
      "[Iteration 614/3520] TRAIN loss: 0.117\n",
      "[Iteration 615/3520] TRAIN loss: 0.139\n",
      "[Iteration 616/3520] TRAIN loss: 0.231\n",
      "[Iteration 617/3520] TRAIN loss: 0.160\n",
      "[Iteration 618/3520] TRAIN loss: 0.235\n",
      "[Iteration 619/3520] TRAIN loss: 0.156\n",
      "[Iteration 620/3520] TRAIN loss: 0.135\n",
      "[Iteration 621/3520] TRAIN loss: 0.153\n",
      "[Iteration 622/3520] TRAIN loss: 0.219\n",
      "[Iteration 623/3520] TRAIN loss: 0.123\n",
      "[Iteration 624/3520] TRAIN loss: 0.197\n",
      "[Iteration 625/3520] TRAIN loss: 0.165\n",
      "[Iteration 626/3520] TRAIN loss: 0.065\n",
      "[Iteration 627/3520] TRAIN loss: 0.120\n",
      "[Iteration 628/3520] TRAIN loss: 0.126\n",
      "[Iteration 629/3520] TRAIN loss: 0.069\n",
      "[Iteration 630/3520] TRAIN loss: 0.119\n",
      "[Iteration 631/3520] TRAIN loss: 0.203\n",
      "[Iteration 632/3520] TRAIN loss: 0.152\n",
      "[Iteration 633/3520] TRAIN loss: 0.127\n",
      "[Iteration 634/3520] TRAIN loss: 0.137\n",
      "[Iteration 635/3520] TRAIN loss: 0.135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 636/3520] TRAIN loss: 0.180\n",
      "[Iteration 637/3520] TRAIN loss: 0.144\n",
      "[Iteration 638/3520] TRAIN loss: 0.110\n",
      "[Iteration 639/3520] TRAIN loss: 0.102\n",
      "[Iteration 640/3520] TRAIN loss: 0.103\n",
      "[Iteration 641/3520] TRAIN loss: 0.055\n",
      "[Iteration 642/3520] TRAIN loss: 0.102\n",
      "[Iteration 643/3520] TRAIN loss: 0.115\n",
      "[Iteration 644/3520] TRAIN loss: 0.073\n",
      "[Iteration 645/3520] TRAIN loss: 0.199\n",
      "[Iteration 646/3520] TRAIN loss: 0.222\n",
      "[Iteration 647/3520] TRAIN loss: 0.037\n",
      "[Iteration 648/3520] TRAIN loss: 0.094\n",
      "[Iteration 649/3520] TRAIN loss: 0.113\n",
      "[Iteration 650/3520] TRAIN loss: 0.050\n",
      "[Iteration 651/3520] TRAIN loss: 0.229\n",
      "[Iteration 652/3520] TRAIN loss: 0.098\n",
      "[Iteration 653/3520] TRAIN loss: 0.107\n",
      "[Iteration 654/3520] TRAIN loss: 0.194\n",
      "[Iteration 655/3520] TRAIN loss: 0.199\n",
      "[Iteration 656/3520] TRAIN loss: 0.151\n",
      "[Iteration 657/3520] TRAIN loss: 0.134\n",
      "[Iteration 658/3520] TRAIN loss: 0.099\n",
      "[Iteration 659/3520] TRAIN loss: 0.120\n",
      "[Iteration 660/3520] TRAIN loss: 0.056\n",
      "[Iteration 661/3520] TRAIN loss: 0.259\n",
      "[Iteration 662/3520] TRAIN loss: 0.085\n",
      "[Iteration 663/3520] TRAIN loss: 0.070\n",
      "[Iteration 664/3520] TRAIN loss: 0.174\n",
      "[Iteration 665/3520] TRAIN loss: 0.185\n",
      "[Iteration 666/3520] TRAIN loss: 0.168\n",
      "[Iteration 667/3520] TRAIN loss: 0.104\n",
      "[Iteration 668/3520] TRAIN loss: 0.168\n",
      "[Iteration 669/3520] TRAIN loss: 0.105\n",
      "[Iteration 670/3520] TRAIN loss: 0.109\n",
      "[Iteration 671/3520] TRAIN loss: 0.224\n",
      "[Iteration 672/3520] TRAIN loss: 0.184\n",
      "[Iteration 673/3520] TRAIN loss: 0.051\n",
      "[Iteration 674/3520] TRAIN loss: 0.090\n",
      "[Iteration 675/3520] TRAIN loss: 0.084\n",
      "[Iteration 676/3520] TRAIN loss: 0.094\n",
      "[Iteration 677/3520] TRAIN loss: 0.098\n",
      "[Iteration 678/3520] TRAIN loss: 0.101\n",
      "[Iteration 679/3520] TRAIN loss: 0.231\n",
      "[Iteration 680/3520] TRAIN loss: 0.141\n",
      "[Iteration 681/3520] TRAIN loss: 0.182\n",
      "[Iteration 682/3520] TRAIN loss: 0.183\n",
      "[Iteration 683/3520] TRAIN loss: 0.151\n",
      "[Iteration 684/3520] TRAIN loss: 0.118\n",
      "[Iteration 685/3520] TRAIN loss: 0.142\n",
      "[Iteration 686/3520] TRAIN loss: 0.121\n",
      "[Iteration 687/3520] TRAIN loss: 0.104\n",
      "[Iteration 688/3520] TRAIN loss: 0.162\n",
      "[Iteration 689/3520] TRAIN loss: 0.188\n",
      "[Iteration 690/3520] TRAIN loss: 0.110\n",
      "[Iteration 691/3520] TRAIN loss: 0.089\n",
      "[Iteration 692/3520] TRAIN loss: 0.144\n",
      "[Iteration 693/3520] TRAIN loss: 0.067\n",
      "[Iteration 694/3520] TRAIN loss: 0.106\n",
      "[Iteration 695/3520] TRAIN loss: 0.182\n",
      "[Iteration 696/3520] TRAIN loss: 0.078\n",
      "[Iteration 697/3520] TRAIN loss: 0.083\n",
      "[Iteration 698/3520] TRAIN loss: 0.156\n",
      "[Iteration 699/3520] TRAIN loss: 0.074\n",
      "[Iteration 700/3520] TRAIN loss: 0.131\n",
      "[Iteration 701/3520] TRAIN loss: 0.057\n",
      "[Iteration 702/3520] TRAIN loss: 0.072\n",
      "[Iteration 703/3520] TRAIN loss: 0.079\n",
      "[Iteration 704/3520] TRAIN loss: 0.088\n",
      "[Epoch 2/10] TRAIN acc/loss: 0.972/0.088\n",
      "[Epoch 2/10] VAL   acc/loss: 0.957/0.143\n",
      "[Iteration 705/3520] TRAIN loss: 0.134\n",
      "[Iteration 706/3520] TRAIN loss: 0.111\n",
      "[Iteration 707/3520] TRAIN loss: 0.123\n",
      "[Iteration 708/3520] TRAIN loss: 0.087\n",
      "[Iteration 709/3520] TRAIN loss: 0.134\n",
      "[Iteration 710/3520] TRAIN loss: 0.098\n",
      "[Iteration 711/3520] TRAIN loss: 0.143\n",
      "[Iteration 712/3520] TRAIN loss: 0.159\n",
      "[Iteration 713/3520] TRAIN loss: 0.136\n",
      "[Iteration 714/3520] TRAIN loss: 0.067\n",
      "[Iteration 715/3520] TRAIN loss: 0.052\n",
      "[Iteration 716/3520] TRAIN loss: 0.066\n",
      "[Iteration 717/3520] TRAIN loss: 0.027\n",
      "[Iteration 718/3520] TRAIN loss: 0.097\n",
      "[Iteration 719/3520] TRAIN loss: 0.145\n",
      "[Iteration 720/3520] TRAIN loss: 0.127\n",
      "[Iteration 721/3520] TRAIN loss: 0.061\n",
      "[Iteration 722/3520] TRAIN loss: 0.062\n",
      "[Iteration 723/3520] TRAIN loss: 0.067\n",
      "[Iteration 724/3520] TRAIN loss: 0.154\n",
      "[Iteration 725/3520] TRAIN loss: 0.167\n",
      "[Iteration 726/3520] TRAIN loss: 0.075\n",
      "[Iteration 727/3520] TRAIN loss: 0.077\n",
      "[Iteration 728/3520] TRAIN loss: 0.222\n",
      "[Iteration 729/3520] TRAIN loss: 0.099\n",
      "[Iteration 730/3520] TRAIN loss: 0.080\n",
      "[Iteration 731/3520] TRAIN loss: 0.130\n",
      "[Iteration 732/3520] TRAIN loss: 0.126\n",
      "[Iteration 733/3520] TRAIN loss: 0.117\n",
      "[Iteration 734/3520] TRAIN loss: 0.263\n",
      "[Iteration 735/3520] TRAIN loss: 0.160\n",
      "[Iteration 736/3520] TRAIN loss: 0.114\n",
      "[Iteration 737/3520] TRAIN loss: 0.132\n",
      "[Iteration 738/3520] TRAIN loss: 0.210\n",
      "[Iteration 739/3520] TRAIN loss: 0.077\n",
      "[Iteration 740/3520] TRAIN loss: 0.133\n",
      "[Iteration 741/3520] TRAIN loss: 0.167\n",
      "[Iteration 742/3520] TRAIN loss: 0.198\n",
      "[Iteration 743/3520] TRAIN loss: 0.212\n",
      "[Iteration 744/3520] TRAIN loss: 0.086\n",
      "[Iteration 745/3520] TRAIN loss: 0.089\n",
      "[Iteration 746/3520] TRAIN loss: 0.164\n",
      "[Iteration 747/3520] TRAIN loss: 0.091\n",
      "[Iteration 748/3520] TRAIN loss: 0.124\n",
      "[Iteration 749/3520] TRAIN loss: 0.177\n",
      "[Iteration 750/3520] TRAIN loss: 0.114\n",
      "[Iteration 751/3520] TRAIN loss: 0.179\n",
      "[Iteration 752/3520] TRAIN loss: 0.116\n",
      "[Iteration 753/3520] TRAIN loss: 0.105\n",
      "[Iteration 754/3520] TRAIN loss: 0.101\n",
      "[Iteration 755/3520] TRAIN loss: 0.250\n",
      "[Iteration 756/3520] TRAIN loss: 0.101\n",
      "[Iteration 757/3520] TRAIN loss: 0.089\n",
      "[Iteration 758/3520] TRAIN loss: 0.124\n",
      "[Iteration 759/3520] TRAIN loss: 0.060\n",
      "[Iteration 760/3520] TRAIN loss: 0.093\n",
      "[Iteration 761/3520] TRAIN loss: 0.152\n",
      "[Iteration 762/3520] TRAIN loss: 0.150\n",
      "[Iteration 763/3520] TRAIN loss: 0.111\n",
      "[Iteration 764/3520] TRAIN loss: 0.162\n",
      "[Iteration 765/3520] TRAIN loss: 0.137\n",
      "[Iteration 766/3520] TRAIN loss: 0.125\n",
      "[Iteration 767/3520] TRAIN loss: 0.143\n",
      "[Iteration 768/3520] TRAIN loss: 0.170\n",
      "[Iteration 769/3520] TRAIN loss: 0.048\n",
      "[Iteration 770/3520] TRAIN loss: 0.147\n",
      "[Iteration 771/3520] TRAIN loss: 0.100\n",
      "[Iteration 772/3520] TRAIN loss: 0.221\n",
      "[Iteration 773/3520] TRAIN loss: 0.164\n",
      "[Iteration 774/3520] TRAIN loss: 0.041\n",
      "[Iteration 775/3520] TRAIN loss: 0.119\n",
      "[Iteration 776/3520] TRAIN loss: 0.206\n",
      "[Iteration 777/3520] TRAIN loss: 0.107\n",
      "[Iteration 778/3520] TRAIN loss: 0.151\n",
      "[Iteration 779/3520] TRAIN loss: 0.114\n",
      "[Iteration 780/3520] TRAIN loss: 0.086\n",
      "[Iteration 781/3520] TRAIN loss: 0.196\n",
      "[Iteration 782/3520] TRAIN loss: 0.078\n",
      "[Iteration 783/3520] TRAIN loss: 0.252\n",
      "[Iteration 784/3520] TRAIN loss: 0.163\n",
      "[Iteration 785/3520] TRAIN loss: 0.153\n",
      "[Iteration 786/3520] TRAIN loss: 0.126\n",
      "[Iteration 787/3520] TRAIN loss: 0.181\n",
      "[Iteration 788/3520] TRAIN loss: 0.054\n",
      "[Iteration 789/3520] TRAIN loss: 0.116\n",
      "[Iteration 790/3520] TRAIN loss: 0.201\n",
      "[Iteration 791/3520] TRAIN loss: 0.144\n",
      "[Iteration 792/3520] TRAIN loss: 0.136\n",
      "[Iteration 793/3520] TRAIN loss: 0.122\n",
      "[Iteration 794/3520] TRAIN loss: 0.104\n",
      "[Iteration 795/3520] TRAIN loss: 0.120\n",
      "[Iteration 796/3520] TRAIN loss: 0.093\n",
      "[Iteration 797/3520] TRAIN loss: 0.186\n",
      "[Iteration 798/3520] TRAIN loss: 0.084\n",
      "[Iteration 799/3520] TRAIN loss: 0.104\n",
      "[Iteration 800/3520] TRAIN loss: 0.184\n",
      "[Iteration 801/3520] TRAIN loss: 0.290\n",
      "[Iteration 802/3520] TRAIN loss: 0.163\n",
      "[Iteration 803/3520] TRAIN loss: 0.067\n",
      "[Iteration 804/3520] TRAIN loss: 0.075\n",
      "[Iteration 805/3520] TRAIN loss: 0.062\n",
      "[Iteration 806/3520] TRAIN loss: 0.056\n",
      "[Iteration 807/3520] TRAIN loss: 0.173\n",
      "[Iteration 808/3520] TRAIN loss: 0.167\n",
      "[Iteration 809/3520] TRAIN loss: 0.134\n",
      "[Iteration 810/3520] TRAIN loss: 0.099\n",
      "[Iteration 811/3520] TRAIN loss: 0.089\n",
      "[Iteration 812/3520] TRAIN loss: 0.163\n",
      "[Iteration 813/3520] TRAIN loss: 0.185\n",
      "[Iteration 814/3520] TRAIN loss: 0.148\n",
      "[Iteration 815/3520] TRAIN loss: 0.201\n",
      "[Iteration 816/3520] TRAIN loss: 0.052\n",
      "[Iteration 817/3520] TRAIN loss: 0.075\n",
      "[Iteration 818/3520] TRAIN loss: 0.080\n",
      "[Iteration 819/3520] TRAIN loss: 0.093\n",
      "[Iteration 820/3520] TRAIN loss: 0.142\n",
      "[Iteration 821/3520] TRAIN loss: 0.120\n",
      "[Iteration 822/3520] TRAIN loss: 0.081\n",
      "[Iteration 823/3520] TRAIN loss: 0.078\n",
      "[Iteration 824/3520] TRAIN loss: 0.117\n",
      "[Iteration 825/3520] TRAIN loss: 0.098\n",
      "[Iteration 826/3520] TRAIN loss: 0.102\n",
      "[Iteration 827/3520] TRAIN loss: 0.084\n",
      "[Iteration 828/3520] TRAIN loss: 0.127\n",
      "[Iteration 829/3520] TRAIN loss: 0.178\n",
      "[Iteration 830/3520] TRAIN loss: 0.075\n",
      "[Iteration 831/3520] TRAIN loss: 0.067\n",
      "[Iteration 832/3520] TRAIN loss: 0.062\n",
      "[Iteration 833/3520] TRAIN loss: 0.114\n",
      "[Iteration 834/3520] TRAIN loss: 0.134\n",
      "[Iteration 835/3520] TRAIN loss: 0.094\n",
      "[Iteration 836/3520] TRAIN loss: 0.047\n",
      "[Iteration 837/3520] TRAIN loss: 0.068\n",
      "[Iteration 838/3520] TRAIN loss: 0.093\n",
      "[Iteration 839/3520] TRAIN loss: 0.108\n",
      "[Iteration 840/3520] TRAIN loss: 0.181\n",
      "[Iteration 841/3520] TRAIN loss: 0.224\n",
      "[Iteration 842/3520] TRAIN loss: 0.135\n",
      "[Iteration 843/3520] TRAIN loss: 0.098\n",
      "[Iteration 844/3520] TRAIN loss: 0.053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 845/3520] TRAIN loss: 0.168\n",
      "[Iteration 846/3520] TRAIN loss: 0.134\n",
      "[Iteration 847/3520] TRAIN loss: 0.041\n",
      "[Iteration 848/3520] TRAIN loss: 0.050\n",
      "[Iteration 849/3520] TRAIN loss: 0.119\n",
      "[Iteration 850/3520] TRAIN loss: 0.138\n",
      "[Iteration 851/3520] TRAIN loss: 0.106\n",
      "[Iteration 852/3520] TRAIN loss: 0.106\n",
      "[Iteration 853/3520] TRAIN loss: 0.111\n",
      "[Iteration 854/3520] TRAIN loss: 0.076\n",
      "[Iteration 855/3520] TRAIN loss: 0.220\n",
      "[Iteration 856/3520] TRAIN loss: 0.074\n",
      "[Iteration 857/3520] TRAIN loss: 0.163\n",
      "[Iteration 858/3520] TRAIN loss: 0.065\n",
      "[Iteration 859/3520] TRAIN loss: 0.151\n",
      "[Iteration 860/3520] TRAIN loss: 0.086\n",
      "[Iteration 861/3520] TRAIN loss: 0.064\n",
      "[Iteration 862/3520] TRAIN loss: 0.082\n",
      "[Iteration 863/3520] TRAIN loss: 0.048\n",
      "[Iteration 864/3520] TRAIN loss: 0.043\n",
      "[Iteration 865/3520] TRAIN loss: 0.114\n",
      "[Iteration 866/3520] TRAIN loss: 0.242\n",
      "[Iteration 867/3520] TRAIN loss: 0.166\n",
      "[Iteration 868/3520] TRAIN loss: 0.050\n",
      "[Iteration 869/3520] TRAIN loss: 0.088\n",
      "[Iteration 870/3520] TRAIN loss: 0.137\n",
      "[Iteration 871/3520] TRAIN loss: 0.128\n",
      "[Iteration 872/3520] TRAIN loss: 0.146\n",
      "[Iteration 873/3520] TRAIN loss: 0.064\n",
      "[Iteration 874/3520] TRAIN loss: 0.088\n",
      "[Iteration 875/3520] TRAIN loss: 0.139\n",
      "[Iteration 876/3520] TRAIN loss: 0.130\n",
      "[Iteration 877/3520] TRAIN loss: 0.152\n",
      "[Iteration 878/3520] TRAIN loss: 0.058\n",
      "[Iteration 879/3520] TRAIN loss: 0.047\n",
      "[Iteration 880/3520] TRAIN loss: 0.048\n",
      "[Iteration 881/3520] TRAIN loss: 0.188\n",
      "[Iteration 882/3520] TRAIN loss: 0.057\n",
      "[Iteration 883/3520] TRAIN loss: 0.126\n",
      "[Iteration 884/3520] TRAIN loss: 0.057\n",
      "[Iteration 885/3520] TRAIN loss: 0.151\n",
      "[Iteration 886/3520] TRAIN loss: 0.044\n",
      "[Iteration 887/3520] TRAIN loss: 0.054\n",
      "[Iteration 888/3520] TRAIN loss: 0.112\n",
      "[Iteration 889/3520] TRAIN loss: 0.076\n",
      "[Iteration 890/3520] TRAIN loss: 0.117\n",
      "[Iteration 891/3520] TRAIN loss: 0.170\n",
      "[Iteration 892/3520] TRAIN loss: 0.051\n",
      "[Iteration 893/3520] TRAIN loss: 0.117\n",
      "[Iteration 894/3520] TRAIN loss: 0.129\n",
      "[Iteration 895/3520] TRAIN loss: 0.143\n",
      "[Iteration 896/3520] TRAIN loss: 0.058\n",
      "[Iteration 897/3520] TRAIN loss: 0.111\n",
      "[Iteration 898/3520] TRAIN loss: 0.151\n",
      "[Iteration 899/3520] TRAIN loss: 0.029\n",
      "[Iteration 900/3520] TRAIN loss: 0.076\n",
      "[Iteration 901/3520] TRAIN loss: 0.124\n",
      "[Iteration 902/3520] TRAIN loss: 0.083\n",
      "[Iteration 903/3520] TRAIN loss: 0.251\n",
      "[Iteration 904/3520] TRAIN loss: 0.035\n",
      "[Iteration 905/3520] TRAIN loss: 0.060\n",
      "[Iteration 906/3520] TRAIN loss: 0.047\n",
      "[Iteration 907/3520] TRAIN loss: 0.181\n",
      "[Iteration 908/3520] TRAIN loss: 0.136\n",
      "[Iteration 909/3520] TRAIN loss: 0.094\n",
      "[Iteration 910/3520] TRAIN loss: 0.105\n",
      "[Iteration 911/3520] TRAIN loss: 0.056\n",
      "[Iteration 912/3520] TRAIN loss: 0.066\n",
      "[Iteration 913/3520] TRAIN loss: 0.114\n",
      "[Iteration 914/3520] TRAIN loss: 0.154\n",
      "[Iteration 915/3520] TRAIN loss: 0.062\n",
      "[Iteration 916/3520] TRAIN loss: 0.043\n",
      "[Iteration 917/3520] TRAIN loss: 0.143\n",
      "[Iteration 918/3520] TRAIN loss: 0.104\n",
      "[Iteration 919/3520] TRAIN loss: 0.117\n",
      "[Iteration 920/3520] TRAIN loss: 0.059\n",
      "[Iteration 921/3520] TRAIN loss: 0.120\n",
      "[Iteration 922/3520] TRAIN loss: 0.149\n",
      "[Iteration 923/3520] TRAIN loss: 0.120\n",
      "[Iteration 924/3520] TRAIN loss: 0.175\n",
      "[Iteration 925/3520] TRAIN loss: 0.089\n",
      "[Iteration 926/3520] TRAIN loss: 0.023\n",
      "[Iteration 927/3520] TRAIN loss: 0.110\n",
      "[Iteration 928/3520] TRAIN loss: 0.067\n",
      "[Iteration 929/3520] TRAIN loss: 0.067\n",
      "[Iteration 930/3520] TRAIN loss: 0.110\n",
      "[Iteration 931/3520] TRAIN loss: 0.096\n",
      "[Iteration 932/3520] TRAIN loss: 0.027\n",
      "[Iteration 933/3520] TRAIN loss: 0.132\n",
      "[Iteration 934/3520] TRAIN loss: 0.095\n",
      "[Iteration 935/3520] TRAIN loss: 0.191\n",
      "[Iteration 936/3520] TRAIN loss: 0.076\n",
      "[Iteration 937/3520] TRAIN loss: 0.140\n",
      "[Iteration 938/3520] TRAIN loss: 0.067\n",
      "[Iteration 939/3520] TRAIN loss: 0.163\n",
      "[Iteration 940/3520] TRAIN loss: 0.083\n",
      "[Iteration 941/3520] TRAIN loss: 0.046\n",
      "[Iteration 942/3520] TRAIN loss: 0.064\n",
      "[Iteration 943/3520] TRAIN loss: 0.106\n",
      "[Iteration 944/3520] TRAIN loss: 0.063\n",
      "[Iteration 945/3520] TRAIN loss: 0.032\n",
      "[Iteration 946/3520] TRAIN loss: 0.155\n",
      "[Iteration 947/3520] TRAIN loss: 0.205\n",
      "[Iteration 948/3520] TRAIN loss: 0.192\n",
      "[Iteration 949/3520] TRAIN loss: 0.127\n",
      "[Iteration 950/3520] TRAIN loss: 0.044\n",
      "[Iteration 951/3520] TRAIN loss: 0.120\n",
      "[Iteration 952/3520] TRAIN loss: 0.071\n",
      "[Iteration 953/3520] TRAIN loss: 0.119\n",
      "[Iteration 954/3520] TRAIN loss: 0.045\n",
      "[Iteration 955/3520] TRAIN loss: 0.234\n",
      "[Iteration 956/3520] TRAIN loss: 0.110\n",
      "[Iteration 957/3520] TRAIN loss: 0.059\n",
      "[Iteration 958/3520] TRAIN loss: 0.115\n",
      "[Iteration 959/3520] TRAIN loss: 0.109\n",
      "[Iteration 960/3520] TRAIN loss: 0.167\n",
      "[Iteration 961/3520] TRAIN loss: 0.106\n",
      "[Iteration 962/3520] TRAIN loss: 0.082\n",
      "[Iteration 963/3520] TRAIN loss: 0.059\n",
      "[Iteration 964/3520] TRAIN loss: 0.250\n",
      "[Iteration 965/3520] TRAIN loss: 0.148\n",
      "[Iteration 966/3520] TRAIN loss: 0.209\n",
      "[Iteration 967/3520] TRAIN loss: 0.029\n",
      "[Iteration 968/3520] TRAIN loss: 0.140\n",
      "[Iteration 969/3520] TRAIN loss: 0.161\n",
      "[Iteration 970/3520] TRAIN loss: 0.123\n",
      "[Iteration 971/3520] TRAIN loss: 0.134\n",
      "[Iteration 972/3520] TRAIN loss: 0.072\n",
      "[Iteration 973/3520] TRAIN loss: 0.141\n",
      "[Iteration 974/3520] TRAIN loss: 0.083\n",
      "[Iteration 975/3520] TRAIN loss: 0.101\n",
      "[Iteration 976/3520] TRAIN loss: 0.102\n",
      "[Iteration 977/3520] TRAIN loss: 0.126\n",
      "[Iteration 978/3520] TRAIN loss: 0.070\n",
      "[Iteration 979/3520] TRAIN loss: 0.156\n",
      "[Iteration 980/3520] TRAIN loss: 0.125\n",
      "[Iteration 981/3520] TRAIN loss: 0.122\n",
      "[Iteration 982/3520] TRAIN loss: 0.069\n",
      "[Iteration 983/3520] TRAIN loss: 0.062\n",
      "[Iteration 984/3520] TRAIN loss: 0.085\n",
      "[Iteration 985/3520] TRAIN loss: 0.172\n",
      "[Iteration 986/3520] TRAIN loss: 0.150\n",
      "[Iteration 987/3520] TRAIN loss: 0.096\n",
      "[Iteration 988/3520] TRAIN loss: 0.097\n",
      "[Iteration 989/3520] TRAIN loss: 0.252\n",
      "[Iteration 990/3520] TRAIN loss: 0.178\n",
      "[Iteration 991/3520] TRAIN loss: 0.060\n",
      "[Iteration 992/3520] TRAIN loss: 0.143\n",
      "[Iteration 993/3520] TRAIN loss: 0.064\n",
      "[Iteration 994/3520] TRAIN loss: 0.212\n",
      "[Iteration 995/3520] TRAIN loss: 0.077\n",
      "[Iteration 996/3520] TRAIN loss: 0.052\n",
      "[Iteration 997/3520] TRAIN loss: 0.104\n",
      "[Iteration 998/3520] TRAIN loss: 0.081\n",
      "[Iteration 999/3520] TRAIN loss: 0.080\n",
      "[Iteration 1000/3520] TRAIN loss: 0.122\n",
      "[Iteration 1001/3520] TRAIN loss: 0.133\n",
      "[Iteration 1002/3520] TRAIN loss: 0.094\n",
      "[Iteration 1003/3520] TRAIN loss: 0.063\n",
      "[Iteration 1004/3520] TRAIN loss: 0.174\n",
      "[Iteration 1005/3520] TRAIN loss: 0.091\n",
      "[Iteration 1006/3520] TRAIN loss: 0.024\n",
      "[Iteration 1007/3520] TRAIN loss: 0.090\n",
      "[Iteration 1008/3520] TRAIN loss: 0.069\n",
      "[Iteration 1009/3520] TRAIN loss: 0.126\n",
      "[Iteration 1010/3520] TRAIN loss: 0.107\n",
      "[Iteration 1011/3520] TRAIN loss: 0.099\n",
      "[Iteration 1012/3520] TRAIN loss: 0.131\n",
      "[Iteration 1013/3520] TRAIN loss: 0.066\n",
      "[Iteration 1014/3520] TRAIN loss: 0.098\n",
      "[Iteration 1015/3520] TRAIN loss: 0.094\n",
      "[Iteration 1016/3520] TRAIN loss: 0.079\n",
      "[Iteration 1017/3520] TRAIN loss: 0.128\n",
      "[Iteration 1018/3520] TRAIN loss: 0.091\n",
      "[Iteration 1019/3520] TRAIN loss: 0.098\n",
      "[Iteration 1020/3520] TRAIN loss: 0.074\n",
      "[Iteration 1021/3520] TRAIN loss: 0.053\n",
      "[Iteration 1022/3520] TRAIN loss: 0.097\n",
      "[Iteration 1023/3520] TRAIN loss: 0.047\n",
      "[Iteration 1024/3520] TRAIN loss: 0.143\n",
      "[Iteration 1025/3520] TRAIN loss: 0.109\n",
      "[Iteration 1026/3520] TRAIN loss: 0.163\n",
      "[Iteration 1027/3520] TRAIN loss: 0.134\n",
      "[Iteration 1028/3520] TRAIN loss: 0.061\n",
      "[Iteration 1029/3520] TRAIN loss: 0.086\n",
      "[Iteration 1030/3520] TRAIN loss: 0.097\n",
      "[Iteration 1031/3520] TRAIN loss: 0.124\n",
      "[Iteration 1032/3520] TRAIN loss: 0.221\n",
      "[Iteration 1033/3520] TRAIN loss: 0.101\n",
      "[Iteration 1034/3520] TRAIN loss: 0.176\n",
      "[Iteration 1035/3520] TRAIN loss: 0.117\n",
      "[Iteration 1036/3520] TRAIN loss: 0.028\n",
      "[Iteration 1037/3520] TRAIN loss: 0.153\n",
      "[Iteration 1038/3520] TRAIN loss: 0.133\n",
      "[Iteration 1039/3520] TRAIN loss: 0.078\n",
      "[Iteration 1040/3520] TRAIN loss: 0.067\n",
      "[Iteration 1041/3520] TRAIN loss: 0.149\n",
      "[Iteration 1042/3520] TRAIN loss: 0.136\n",
      "[Iteration 1043/3520] TRAIN loss: 0.159\n",
      "[Iteration 1044/3520] TRAIN loss: 0.092\n",
      "[Iteration 1045/3520] TRAIN loss: 0.120\n",
      "[Iteration 1046/3520] TRAIN loss: 0.094\n",
      "[Iteration 1047/3520] TRAIN loss: 0.101\n",
      "[Iteration 1048/3520] TRAIN loss: 0.107\n",
      "[Iteration 1049/3520] TRAIN loss: 0.084\n",
      "[Iteration 1050/3520] TRAIN loss: 0.048\n",
      "[Iteration 1051/3520] TRAIN loss: 0.179\n",
      "[Iteration 1052/3520] TRAIN loss: 0.106\n",
      "[Iteration 1053/3520] TRAIN loss: 0.106\n",
      "[Iteration 1054/3520] TRAIN loss: 0.124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1055/3520] TRAIN loss: 0.134\n",
      "[Iteration 1056/3520] TRAIN loss: 0.036\n",
      "[Epoch 3/10] TRAIN acc/loss: 0.986/0.036\n",
      "[Epoch 3/10] VAL   acc/loss: 0.967/0.111\n",
      "[Iteration 1057/3520] TRAIN loss: 0.042\n",
      "[Iteration 1058/3520] TRAIN loss: 0.067\n",
      "[Iteration 1059/3520] TRAIN loss: 0.076\n",
      "[Iteration 1060/3520] TRAIN loss: 0.108\n",
      "[Iteration 1061/3520] TRAIN loss: 0.116\n",
      "[Iteration 1062/3520] TRAIN loss: 0.110\n",
      "[Iteration 1063/3520] TRAIN loss: 0.050\n",
      "[Iteration 1064/3520] TRAIN loss: 0.036\n",
      "[Iteration 1065/3520] TRAIN loss: 0.043\n",
      "[Iteration 1066/3520] TRAIN loss: 0.022\n",
      "[Iteration 1067/3520] TRAIN loss: 0.072\n",
      "[Iteration 1068/3520] TRAIN loss: 0.033\n",
      "[Iteration 1069/3520] TRAIN loss: 0.130\n",
      "[Iteration 1070/3520] TRAIN loss: 0.036\n",
      "[Iteration 1071/3520] TRAIN loss: 0.092\n",
      "[Iteration 1072/3520] TRAIN loss: 0.129\n",
      "[Iteration 1073/3520] TRAIN loss: 0.119\n",
      "[Iteration 1074/3520] TRAIN loss: 0.097\n",
      "[Iteration 1075/3520] TRAIN loss: 0.087\n",
      "[Iteration 1076/3520] TRAIN loss: 0.076\n",
      "[Iteration 1077/3520] TRAIN loss: 0.116\n",
      "[Iteration 1078/3520] TRAIN loss: 0.065\n",
      "[Iteration 1079/3520] TRAIN loss: 0.042\n",
      "[Iteration 1080/3520] TRAIN loss: 0.093\n",
      "[Iteration 1081/3520] TRAIN loss: 0.035\n",
      "[Iteration 1082/3520] TRAIN loss: 0.109\n",
      "[Iteration 1083/3520] TRAIN loss: 0.052\n",
      "[Iteration 1084/3520] TRAIN loss: 0.136\n",
      "[Iteration 1085/3520] TRAIN loss: 0.029\n",
      "[Iteration 1086/3520] TRAIN loss: 0.070\n",
      "[Iteration 1087/3520] TRAIN loss: 0.064\n",
      "[Iteration 1088/3520] TRAIN loss: 0.274\n",
      "[Iteration 1089/3520] TRAIN loss: 0.117\n",
      "[Iteration 1090/3520] TRAIN loss: 0.078\n",
      "[Iteration 1091/3520] TRAIN loss: 0.054\n",
      "[Iteration 1092/3520] TRAIN loss: 0.103\n",
      "[Iteration 1093/3520] TRAIN loss: 0.081\n",
      "[Iteration 1094/3520] TRAIN loss: 0.180\n",
      "[Iteration 1095/3520] TRAIN loss: 0.065\n",
      "[Iteration 1096/3520] TRAIN loss: 0.044\n",
      "[Iteration 1097/3520] TRAIN loss: 0.129\n",
      "[Iteration 1098/3520] TRAIN loss: 0.113\n",
      "[Iteration 1099/3520] TRAIN loss: 0.058\n",
      "[Iteration 1100/3520] TRAIN loss: 0.106\n",
      "[Iteration 1101/3520] TRAIN loss: 0.110\n",
      "[Iteration 1102/3520] TRAIN loss: 0.140\n",
      "[Iteration 1103/3520] TRAIN loss: 0.142\n",
      "[Iteration 1104/3520] TRAIN loss: 0.082\n",
      "[Iteration 1105/3520] TRAIN loss: 0.206\n",
      "[Iteration 1106/3520] TRAIN loss: 0.047\n",
      "[Iteration 1107/3520] TRAIN loss: 0.106\n",
      "[Iteration 1108/3520] TRAIN loss: 0.084\n",
      "[Iteration 1109/3520] TRAIN loss: 0.107\n",
      "[Iteration 1110/3520] TRAIN loss: 0.109\n",
      "[Iteration 1111/3520] TRAIN loss: 0.079\n",
      "[Iteration 1112/3520] TRAIN loss: 0.026\n",
      "[Iteration 1113/3520] TRAIN loss: 0.063\n",
      "[Iteration 1114/3520] TRAIN loss: 0.051\n",
      "[Iteration 1115/3520] TRAIN loss: 0.169\n",
      "[Iteration 1116/3520] TRAIN loss: 0.064\n",
      "[Iteration 1117/3520] TRAIN loss: 0.181\n",
      "[Iteration 1118/3520] TRAIN loss: 0.127\n",
      "[Iteration 1119/3520] TRAIN loss: 0.095\n",
      "[Iteration 1120/3520] TRAIN loss: 0.134\n",
      "[Iteration 1121/3520] TRAIN loss: 0.061\n",
      "[Iteration 1122/3520] TRAIN loss: 0.129\n",
      "[Iteration 1123/3520] TRAIN loss: 0.225\n",
      "[Iteration 1124/3520] TRAIN loss: 0.037\n",
      "[Iteration 1125/3520] TRAIN loss: 0.044\n",
      "[Iteration 1126/3520] TRAIN loss: 0.045\n",
      "[Iteration 1127/3520] TRAIN loss: 0.073\n",
      "[Iteration 1128/3520] TRAIN loss: 0.072\n",
      "[Iteration 1129/3520] TRAIN loss: 0.182\n",
      "[Iteration 1130/3520] TRAIN loss: 0.092\n",
      "[Iteration 1131/3520] TRAIN loss: 0.072\n",
      "[Iteration 1132/3520] TRAIN loss: 0.044\n",
      "[Iteration 1133/3520] TRAIN loss: 0.126\n",
      "[Iteration 1134/3520] TRAIN loss: 0.074\n",
      "[Iteration 1135/3520] TRAIN loss: 0.093\n",
      "[Iteration 1136/3520] TRAIN loss: 0.029\n",
      "[Iteration 1137/3520] TRAIN loss: 0.022\n",
      "[Iteration 1138/3520] TRAIN loss: 0.023\n",
      "[Iteration 1139/3520] TRAIN loss: 0.075\n",
      "[Iteration 1140/3520] TRAIN loss: 0.118\n",
      "[Iteration 1141/3520] TRAIN loss: 0.049\n",
      "[Iteration 1142/3520] TRAIN loss: 0.145\n",
      "[Iteration 1143/3520] TRAIN loss: 0.062\n",
      "[Iteration 1144/3520] TRAIN loss: 0.050\n",
      "[Iteration 1145/3520] TRAIN loss: 0.104\n",
      "[Iteration 1146/3520] TRAIN loss: 0.058\n",
      "[Iteration 1147/3520] TRAIN loss: 0.083\n",
      "[Iteration 1148/3520] TRAIN loss: 0.025\n",
      "[Iteration 1149/3520] TRAIN loss: 0.040\n",
      "[Iteration 1150/3520] TRAIN loss: 0.112\n",
      "[Iteration 1151/3520] TRAIN loss: 0.064\n",
      "[Iteration 1152/3520] TRAIN loss: 0.063\n",
      "[Iteration 1153/3520] TRAIN loss: 0.042\n",
      "[Iteration 1154/3520] TRAIN loss: 0.074\n",
      "[Iteration 1155/3520] TRAIN loss: 0.124\n",
      "[Iteration 1156/3520] TRAIN loss: 0.053\n",
      "[Iteration 1157/3520] TRAIN loss: 0.083\n",
      "[Iteration 1158/3520] TRAIN loss: 0.119\n",
      "[Iteration 1159/3520] TRAIN loss: 0.116\n",
      "[Iteration 1160/3520] TRAIN loss: 0.063\n",
      "[Iteration 1161/3520] TRAIN loss: 0.045\n",
      "[Iteration 1162/3520] TRAIN loss: 0.037\n",
      "[Iteration 1163/3520] TRAIN loss: 0.124\n",
      "[Iteration 1164/3520] TRAIN loss: 0.097\n",
      "[Iteration 1165/3520] TRAIN loss: 0.131\n",
      "[Iteration 1166/3520] TRAIN loss: 0.121\n",
      "[Iteration 1167/3520] TRAIN loss: 0.115\n",
      "[Iteration 1168/3520] TRAIN loss: 0.139\n",
      "[Iteration 1169/3520] TRAIN loss: 0.146\n",
      "[Iteration 1170/3520] TRAIN loss: 0.064\n",
      "[Iteration 1171/3520] TRAIN loss: 0.017\n",
      "[Iteration 1172/3520] TRAIN loss: 0.043\n",
      "[Iteration 1173/3520] TRAIN loss: 0.048\n",
      "[Iteration 1174/3520] TRAIN loss: 0.071\n",
      "[Iteration 1175/3520] TRAIN loss: 0.046\n",
      "[Iteration 1176/3520] TRAIN loss: 0.107\n",
      "[Iteration 1177/3520] TRAIN loss: 0.118\n",
      "[Iteration 1178/3520] TRAIN loss: 0.037\n",
      "[Iteration 1179/3520] TRAIN loss: 0.090\n",
      "[Iteration 1180/3520] TRAIN loss: 0.051\n",
      "[Iteration 1181/3520] TRAIN loss: 0.048\n",
      "[Iteration 1182/3520] TRAIN loss: 0.113\n",
      "[Iteration 1183/3520] TRAIN loss: 0.071\n",
      "[Iteration 1184/3520] TRAIN loss: 0.047\n",
      "[Iteration 1185/3520] TRAIN loss: 0.139\n",
      "[Iteration 1186/3520] TRAIN loss: 0.077\n",
      "[Iteration 1187/3520] TRAIN loss: 0.052\n",
      "[Iteration 1188/3520] TRAIN loss: 0.095\n",
      "[Iteration 1189/3520] TRAIN loss: 0.040\n",
      "[Iteration 1190/3520] TRAIN loss: 0.056\n",
      "[Iteration 1191/3520] TRAIN loss: 0.033\n",
      "[Iteration 1192/3520] TRAIN loss: 0.057\n",
      "[Iteration 1193/3520] TRAIN loss: 0.056\n",
      "[Iteration 1194/3520] TRAIN loss: 0.055\n",
      "[Iteration 1195/3520] TRAIN loss: 0.045\n",
      "[Iteration 1196/3520] TRAIN loss: 0.064\n",
      "[Iteration 1197/3520] TRAIN loss: 0.095\n",
      "[Iteration 1198/3520] TRAIN loss: 0.133\n",
      "[Iteration 1199/3520] TRAIN loss: 0.080\n",
      "[Iteration 1200/3520] TRAIN loss: 0.115\n",
      "[Iteration 1201/3520] TRAIN loss: 0.074\n",
      "[Iteration 1202/3520] TRAIN loss: 0.029\n",
      "[Iteration 1203/3520] TRAIN loss: 0.074\n",
      "[Iteration 1204/3520] TRAIN loss: 0.040\n",
      "[Iteration 1205/3520] TRAIN loss: 0.072\n",
      "[Iteration 1206/3520] TRAIN loss: 0.179\n",
      "[Iteration 1207/3520] TRAIN loss: 0.046\n",
      "[Iteration 1208/3520] TRAIN loss: 0.120\n",
      "[Iteration 1209/3520] TRAIN loss: 0.018\n",
      "[Iteration 1210/3520] TRAIN loss: 0.064\n",
      "[Iteration 1211/3520] TRAIN loss: 0.132\n",
      "[Iteration 1212/3520] TRAIN loss: 0.061\n",
      "[Iteration 1213/3520] TRAIN loss: 0.159\n",
      "[Iteration 1214/3520] TRAIN loss: 0.029\n",
      "[Iteration 1215/3520] TRAIN loss: 0.051\n",
      "[Iteration 1216/3520] TRAIN loss: 0.114\n",
      "[Iteration 1217/3520] TRAIN loss: 0.101\n",
      "[Iteration 1218/3520] TRAIN loss: 0.097\n",
      "[Iteration 1219/3520] TRAIN loss: 0.091\n",
      "[Iteration 1220/3520] TRAIN loss: 0.057\n",
      "[Iteration 1221/3520] TRAIN loss: 0.084\n",
      "[Iteration 1222/3520] TRAIN loss: 0.175\n",
      "[Iteration 1223/3520] TRAIN loss: 0.065\n",
      "[Iteration 1224/3520] TRAIN loss: 0.110\n",
      "[Iteration 1225/3520] TRAIN loss: 0.042\n",
      "[Iteration 1226/3520] TRAIN loss: 0.069\n",
      "[Iteration 1227/3520] TRAIN loss: 0.084\n",
      "[Iteration 1228/3520] TRAIN loss: 0.076\n",
      "[Iteration 1229/3520] TRAIN loss: 0.031\n",
      "[Iteration 1230/3520] TRAIN loss: 0.046\n",
      "[Iteration 1231/3520] TRAIN loss: 0.088\n",
      "[Iteration 1232/3520] TRAIN loss: 0.078\n",
      "[Iteration 1233/3520] TRAIN loss: 0.183\n",
      "[Iteration 1234/3520] TRAIN loss: 0.090\n",
      "[Iteration 1235/3520] TRAIN loss: 0.113\n",
      "[Iteration 1236/3520] TRAIN loss: 0.060\n",
      "[Iteration 1237/3520] TRAIN loss: 0.058\n",
      "[Iteration 1238/3520] TRAIN loss: 0.145\n",
      "[Iteration 1239/3520] TRAIN loss: 0.104\n",
      "[Iteration 1240/3520] TRAIN loss: 0.082\n",
      "[Iteration 1241/3520] TRAIN loss: 0.077\n",
      "[Iteration 1242/3520] TRAIN loss: 0.061\n",
      "[Iteration 1243/3520] TRAIN loss: 0.073\n",
      "[Iteration 1244/3520] TRAIN loss: 0.045\n",
      "[Iteration 1245/3520] TRAIN loss: 0.070\n",
      "[Iteration 1246/3520] TRAIN loss: 0.102\n",
      "[Iteration 1247/3520] TRAIN loss: 0.071\n",
      "[Iteration 1248/3520] TRAIN loss: 0.178\n",
      "[Iteration 1249/3520] TRAIN loss: 0.102\n",
      "[Iteration 1250/3520] TRAIN loss: 0.073\n",
      "[Iteration 1251/3520] TRAIN loss: 0.076\n",
      "[Iteration 1252/3520] TRAIN loss: 0.027\n",
      "[Iteration 1253/3520] TRAIN loss: 0.099\n",
      "[Iteration 1254/3520] TRAIN loss: 0.118\n",
      "[Iteration 1255/3520] TRAIN loss: 0.184\n",
      "[Iteration 1256/3520] TRAIN loss: 0.048\n",
      "[Iteration 1257/3520] TRAIN loss: 0.064\n",
      "[Iteration 1258/3520] TRAIN loss: 0.171\n",
      "[Iteration 1259/3520] TRAIN loss: 0.060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1260/3520] TRAIN loss: 0.213\n",
      "[Iteration 1261/3520] TRAIN loss: 0.138\n",
      "[Iteration 1262/3520] TRAIN loss: 0.109\n",
      "[Iteration 1263/3520] TRAIN loss: 0.056\n",
      "[Iteration 1264/3520] TRAIN loss: 0.123\n",
      "[Iteration 1265/3520] TRAIN loss: 0.099\n",
      "[Iteration 1266/3520] TRAIN loss: 0.068\n",
      "[Iteration 1267/3520] TRAIN loss: 0.068\n",
      "[Iteration 1268/3520] TRAIN loss: 0.129\n",
      "[Iteration 1269/3520] TRAIN loss: 0.045\n",
      "[Iteration 1270/3520] TRAIN loss: 0.108\n",
      "[Iteration 1271/3520] TRAIN loss: 0.118\n",
      "[Iteration 1272/3520] TRAIN loss: 0.036\n",
      "[Iteration 1273/3520] TRAIN loss: 0.081\n",
      "[Iteration 1274/3520] TRAIN loss: 0.114\n",
      "[Iteration 1275/3520] TRAIN loss: 0.052\n",
      "[Iteration 1276/3520] TRAIN loss: 0.145\n",
      "[Iteration 1277/3520] TRAIN loss: 0.074\n",
      "[Iteration 1278/3520] TRAIN loss: 0.117\n",
      "[Iteration 1279/3520] TRAIN loss: 0.055\n",
      "[Iteration 1280/3520] TRAIN loss: 0.048\n",
      "[Iteration 1281/3520] TRAIN loss: 0.106\n",
      "[Iteration 1282/3520] TRAIN loss: 0.141\n",
      "[Iteration 1283/3520] TRAIN loss: 0.058\n",
      "[Iteration 1284/3520] TRAIN loss: 0.119\n",
      "[Iteration 1285/3520] TRAIN loss: 0.077\n",
      "[Iteration 1286/3520] TRAIN loss: 0.091\n",
      "[Iteration 1287/3520] TRAIN loss: 0.042\n",
      "[Iteration 1288/3520] TRAIN loss: 0.093\n",
      "[Iteration 1289/3520] TRAIN loss: 0.153\n",
      "[Iteration 1290/3520] TRAIN loss: 0.165\n",
      "[Iteration 1291/3520] TRAIN loss: 0.057\n",
      "[Iteration 1292/3520] TRAIN loss: 0.047\n",
      "[Iteration 1293/3520] TRAIN loss: 0.064\n",
      "[Iteration 1294/3520] TRAIN loss: 0.084\n",
      "[Iteration 1295/3520] TRAIN loss: 0.088\n",
      "[Iteration 1296/3520] TRAIN loss: 0.087\n",
      "[Iteration 1297/3520] TRAIN loss: 0.045\n",
      "[Iteration 1298/3520] TRAIN loss: 0.125\n",
      "[Iteration 1299/3520] TRAIN loss: 0.107\n",
      "[Iteration 1300/3520] TRAIN loss: 0.127\n",
      "[Iteration 1301/3520] TRAIN loss: 0.064\n",
      "[Iteration 1302/3520] TRAIN loss: 0.129\n",
      "[Iteration 1303/3520] TRAIN loss: 0.059\n",
      "[Iteration 1304/3520] TRAIN loss: 0.116\n",
      "[Iteration 1305/3520] TRAIN loss: 0.131\n",
      "[Iteration 1306/3520] TRAIN loss: 0.028\n",
      "[Iteration 1307/3520] TRAIN loss: 0.108\n",
      "[Iteration 1308/3520] TRAIN loss: 0.147\n",
      "[Iteration 1309/3520] TRAIN loss: 0.114\n",
      "[Iteration 1310/3520] TRAIN loss: 0.081\n",
      "[Iteration 1311/3520] TRAIN loss: 0.093\n",
      "[Iteration 1312/3520] TRAIN loss: 0.036\n",
      "[Iteration 1313/3520] TRAIN loss: 0.020\n",
      "[Iteration 1314/3520] TRAIN loss: 0.087\n",
      "[Iteration 1315/3520] TRAIN loss: 0.067\n",
      "[Iteration 1316/3520] TRAIN loss: 0.125\n",
      "[Iteration 1317/3520] TRAIN loss: 0.136\n",
      "[Iteration 1318/3520] TRAIN loss: 0.122\n",
      "[Iteration 1319/3520] TRAIN loss: 0.046\n",
      "[Iteration 1320/3520] TRAIN loss: 0.076\n",
      "[Iteration 1321/3520] TRAIN loss: 0.140\n",
      "[Iteration 1322/3520] TRAIN loss: 0.184\n",
      "[Iteration 1323/3520] TRAIN loss: 0.079\n",
      "[Iteration 1324/3520] TRAIN loss: 0.091\n",
      "[Iteration 1325/3520] TRAIN loss: 0.072\n",
      "[Iteration 1326/3520] TRAIN loss: 0.056\n",
      "[Iteration 1327/3520] TRAIN loss: 0.151\n",
      "[Iteration 1328/3520] TRAIN loss: 0.075\n",
      "[Iteration 1329/3520] TRAIN loss: 0.132\n",
      "[Iteration 1330/3520] TRAIN loss: 0.066\n",
      "[Iteration 1331/3520] TRAIN loss: 0.030\n",
      "[Iteration 1332/3520] TRAIN loss: 0.109\n",
      "[Iteration 1333/3520] TRAIN loss: 0.026\n",
      "[Iteration 1334/3520] TRAIN loss: 0.121\n",
      "[Iteration 1335/3520] TRAIN loss: 0.132\n",
      "[Iteration 1336/3520] TRAIN loss: 0.031\n",
      "[Iteration 1337/3520] TRAIN loss: 0.041\n",
      "[Iteration 1338/3520] TRAIN loss: 0.102\n",
      "[Iteration 1339/3520] TRAIN loss: 0.052\n",
      "[Iteration 1340/3520] TRAIN loss: 0.047\n",
      "[Iteration 1341/3520] TRAIN loss: 0.104\n",
      "[Iteration 1342/3520] TRAIN loss: 0.083\n",
      "[Iteration 1343/3520] TRAIN loss: 0.071\n",
      "[Iteration 1344/3520] TRAIN loss: 0.208\n",
      "[Iteration 1345/3520] TRAIN loss: 0.057\n",
      "[Iteration 1346/3520] TRAIN loss: 0.166\n",
      "[Iteration 1347/3520] TRAIN loss: 0.041\n",
      "[Iteration 1348/3520] TRAIN loss: 0.204\n",
      "[Iteration 1349/3520] TRAIN loss: 0.143\n",
      "[Iteration 1350/3520] TRAIN loss: 0.118\n",
      "[Iteration 1351/3520] TRAIN loss: 0.069\n",
      "[Iteration 1352/3520] TRAIN loss: 0.098\n",
      "[Iteration 1353/3520] TRAIN loss: 0.192\n",
      "[Iteration 1354/3520] TRAIN loss: 0.141\n",
      "[Iteration 1355/3520] TRAIN loss: 0.135\n",
      "[Iteration 1356/3520] TRAIN loss: 0.030\n",
      "[Iteration 1357/3520] TRAIN loss: 0.100\n",
      "[Iteration 1358/3520] TRAIN loss: 0.078\n",
      "[Iteration 1359/3520] TRAIN loss: 0.188\n",
      "[Iteration 1360/3520] TRAIN loss: 0.047\n",
      "[Iteration 1361/3520] TRAIN loss: 0.048\n",
      "[Iteration 1362/3520] TRAIN loss: 0.023\n",
      "[Iteration 1363/3520] TRAIN loss: 0.026\n",
      "[Iteration 1364/3520] TRAIN loss: 0.038\n",
      "[Iteration 1365/3520] TRAIN loss: 0.053\n",
      "[Iteration 1366/3520] TRAIN loss: 0.165\n",
      "[Iteration 1367/3520] TRAIN loss: 0.040\n",
      "[Iteration 1368/3520] TRAIN loss: 0.017\n",
      "[Iteration 1369/3520] TRAIN loss: 0.075\n",
      "[Iteration 1370/3520] TRAIN loss: 0.116\n",
      "[Iteration 1371/3520] TRAIN loss: 0.100\n",
      "[Iteration 1372/3520] TRAIN loss: 0.111\n",
      "[Iteration 1373/3520] TRAIN loss: 0.054\n",
      "[Iteration 1374/3520] TRAIN loss: 0.074\n",
      "[Iteration 1375/3520] TRAIN loss: 0.033\n",
      "[Iteration 1376/3520] TRAIN loss: 0.041\n",
      "[Iteration 1377/3520] TRAIN loss: 0.051\n",
      "[Iteration 1378/3520] TRAIN loss: 0.037\n",
      "[Iteration 1379/3520] TRAIN loss: 0.069\n",
      "[Iteration 1380/3520] TRAIN loss: 0.050\n",
      "[Iteration 1381/3520] TRAIN loss: 0.046\n",
      "[Iteration 1382/3520] TRAIN loss: 0.175\n",
      "[Iteration 1383/3520] TRAIN loss: 0.058\n",
      "[Iteration 1384/3520] TRAIN loss: 0.090\n",
      "[Iteration 1385/3520] TRAIN loss: 0.042\n",
      "[Iteration 1386/3520] TRAIN loss: 0.066\n",
      "[Iteration 1387/3520] TRAIN loss: 0.054\n",
      "[Iteration 1388/3520] TRAIN loss: 0.030\n",
      "[Iteration 1389/3520] TRAIN loss: 0.063\n",
      "[Iteration 1390/3520] TRAIN loss: 0.081\n",
      "[Iteration 1391/3520] TRAIN loss: 0.017\n",
      "[Iteration 1392/3520] TRAIN loss: 0.097\n",
      "[Iteration 1393/3520] TRAIN loss: 0.123\n",
      "[Iteration 1394/3520] TRAIN loss: 0.040\n",
      "[Iteration 1395/3520] TRAIN loss: 0.054\n",
      "[Iteration 1396/3520] TRAIN loss: 0.066\n",
      "[Iteration 1397/3520] TRAIN loss: 0.014\n",
      "[Iteration 1398/3520] TRAIN loss: 0.078\n",
      "[Iteration 1399/3520] TRAIN loss: 0.110\n",
      "[Iteration 1400/3520] TRAIN loss: 0.047\n",
      "[Iteration 1401/3520] TRAIN loss: 0.044\n",
      "[Iteration 1402/3520] TRAIN loss: 0.047\n",
      "[Iteration 1403/3520] TRAIN loss: 0.065\n",
      "[Iteration 1404/3520] TRAIN loss: 0.053\n",
      "[Iteration 1405/3520] TRAIN loss: 0.040\n",
      "[Iteration 1406/3520] TRAIN loss: 0.047\n",
      "[Iteration 1407/3520] TRAIN loss: 0.082\n",
      "[Iteration 1408/3520] TRAIN loss: 0.084\n",
      "[Epoch 4/10] TRAIN acc/loss: 0.958/0.084\n",
      "[Epoch 4/10] VAL   acc/loss: 0.972/0.091\n",
      "[Iteration 1409/3520] TRAIN loss: 0.055\n",
      "[Iteration 1410/3520] TRAIN loss: 0.087\n",
      "[Iteration 1411/3520] TRAIN loss: 0.033\n",
      "[Iteration 1412/3520] TRAIN loss: 0.034\n",
      "[Iteration 1413/3520] TRAIN loss: 0.033\n",
      "[Iteration 1414/3520] TRAIN loss: 0.165\n",
      "[Iteration 1415/3520] TRAIN loss: 0.075\n",
      "[Iteration 1416/3520] TRAIN loss: 0.059\n",
      "[Iteration 1417/3520] TRAIN loss: 0.128\n",
      "[Iteration 1418/3520] TRAIN loss: 0.076\n",
      "[Iteration 1419/3520] TRAIN loss: 0.066\n",
      "[Iteration 1420/3520] TRAIN loss: 0.036\n",
      "[Iteration 1421/3520] TRAIN loss: 0.067\n",
      "[Iteration 1422/3520] TRAIN loss: 0.053\n",
      "[Iteration 1423/3520] TRAIN loss: 0.069\n",
      "[Iteration 1424/3520] TRAIN loss: 0.049\n",
      "[Iteration 1425/3520] TRAIN loss: 0.032\n",
      "[Iteration 1426/3520] TRAIN loss: 0.091\n",
      "[Iteration 1427/3520] TRAIN loss: 0.040\n",
      "[Iteration 1428/3520] TRAIN loss: 0.046\n",
      "[Iteration 1429/3520] TRAIN loss: 0.057\n",
      "[Iteration 1430/3520] TRAIN loss: 0.030\n",
      "[Iteration 1431/3520] TRAIN loss: 0.239\n",
      "[Iteration 1432/3520] TRAIN loss: 0.041\n",
      "[Iteration 1433/3520] TRAIN loss: 0.098\n",
      "[Iteration 1434/3520] TRAIN loss: 0.078\n",
      "[Iteration 1435/3520] TRAIN loss: 0.042\n",
      "[Iteration 1436/3520] TRAIN loss: 0.075\n",
      "[Iteration 1437/3520] TRAIN loss: 0.138\n",
      "[Iteration 1438/3520] TRAIN loss: 0.056\n",
      "[Iteration 1439/3520] TRAIN loss: 0.105\n",
      "[Iteration 1440/3520] TRAIN loss: 0.062\n",
      "[Iteration 1441/3520] TRAIN loss: 0.101\n",
      "[Iteration 1442/3520] TRAIN loss: 0.061\n",
      "[Iteration 1443/3520] TRAIN loss: 0.024\n",
      "[Iteration 1444/3520] TRAIN loss: 0.080\n",
      "[Iteration 1445/3520] TRAIN loss: 0.042\n",
      "[Iteration 1446/3520] TRAIN loss: 0.067\n",
      "[Iteration 1447/3520] TRAIN loss: 0.129\n",
      "[Iteration 1448/3520] TRAIN loss: 0.043\n",
      "[Iteration 1449/3520] TRAIN loss: 0.065\n",
      "[Iteration 1450/3520] TRAIN loss: 0.136\n",
      "[Iteration 1451/3520] TRAIN loss: 0.068\n",
      "[Iteration 1452/3520] TRAIN loss: 0.065\n",
      "[Iteration 1453/3520] TRAIN loss: 0.086\n",
      "[Iteration 1454/3520] TRAIN loss: 0.055\n",
      "[Iteration 1455/3520] TRAIN loss: 0.039\n",
      "[Iteration 1456/3520] TRAIN loss: 0.090\n",
      "[Iteration 1457/3520] TRAIN loss: 0.014\n",
      "[Iteration 1458/3520] TRAIN loss: 0.055\n",
      "[Iteration 1459/3520] TRAIN loss: 0.085\n",
      "[Iteration 1460/3520] TRAIN loss: 0.071\n",
      "[Iteration 1461/3520] TRAIN loss: 0.083\n",
      "[Iteration 1462/3520] TRAIN loss: 0.080\n",
      "[Iteration 1463/3520] TRAIN loss: 0.077\n",
      "[Iteration 1464/3520] TRAIN loss: 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1465/3520] TRAIN loss: 0.063\n",
      "[Iteration 1466/3520] TRAIN loss: 0.104\n",
      "[Iteration 1467/3520] TRAIN loss: 0.092\n",
      "[Iteration 1468/3520] TRAIN loss: 0.141\n",
      "[Iteration 1469/3520] TRAIN loss: 0.115\n",
      "[Iteration 1470/3520] TRAIN loss: 0.094\n",
      "[Iteration 1471/3520] TRAIN loss: 0.092\n",
      "[Iteration 1472/3520] TRAIN loss: 0.067\n",
      "[Iteration 1473/3520] TRAIN loss: 0.073\n",
      "[Iteration 1474/3520] TRAIN loss: 0.030\n",
      "[Iteration 1475/3520] TRAIN loss: 0.032\n",
      "[Iteration 1476/3520] TRAIN loss: 0.084\n",
      "[Iteration 1477/3520] TRAIN loss: 0.070\n",
      "[Iteration 1478/3520] TRAIN loss: 0.112\n",
      "[Iteration 1479/3520] TRAIN loss: 0.135\n",
      "[Iteration 1480/3520] TRAIN loss: 0.051\n",
      "[Iteration 1481/3520] TRAIN loss: 0.025\n",
      "[Iteration 1482/3520] TRAIN loss: 0.112\n",
      "[Iteration 1483/3520] TRAIN loss: 0.047\n",
      "[Iteration 1484/3520] TRAIN loss: 0.077\n",
      "[Iteration 1485/3520] TRAIN loss: 0.075\n",
      "[Iteration 1486/3520] TRAIN loss: 0.084\n",
      "[Iteration 1487/3520] TRAIN loss: 0.047\n",
      "[Iteration 1488/3520] TRAIN loss: 0.041\n",
      "[Iteration 1489/3520] TRAIN loss: 0.045\n",
      "[Iteration 1490/3520] TRAIN loss: 0.084\n",
      "[Iteration 1491/3520] TRAIN loss: 0.084\n",
      "[Iteration 1492/3520] TRAIN loss: 0.107\n",
      "[Iteration 1493/3520] TRAIN loss: 0.034\n",
      "[Iteration 1494/3520] TRAIN loss: 0.151\n",
      "[Iteration 1495/3520] TRAIN loss: 0.087\n",
      "[Iteration 1496/3520] TRAIN loss: 0.021\n",
      "[Iteration 1497/3520] TRAIN loss: 0.061\n",
      "[Iteration 1498/3520] TRAIN loss: 0.094\n",
      "[Iteration 1499/3520] TRAIN loss: 0.048\n",
      "[Iteration 1500/3520] TRAIN loss: 0.017\n",
      "[Iteration 1501/3520] TRAIN loss: 0.052\n",
      "[Iteration 1502/3520] TRAIN loss: 0.049\n",
      "[Iteration 1503/3520] TRAIN loss: 0.017\n",
      "[Iteration 1504/3520] TRAIN loss: 0.107\n",
      "[Iteration 1505/3520] TRAIN loss: 0.125\n",
      "[Iteration 1506/3520] TRAIN loss: 0.027\n",
      "[Iteration 1507/3520] TRAIN loss: 0.127\n",
      "[Iteration 1508/3520] TRAIN loss: 0.032\n",
      "[Iteration 1509/3520] TRAIN loss: 0.077\n",
      "[Iteration 1510/3520] TRAIN loss: 0.060\n",
      "[Iteration 1511/3520] TRAIN loss: 0.060\n",
      "[Iteration 1512/3520] TRAIN loss: 0.036\n",
      "[Iteration 1513/3520] TRAIN loss: 0.035\n",
      "[Iteration 1514/3520] TRAIN loss: 0.166\n",
      "[Iteration 1515/3520] TRAIN loss: 0.079\n",
      "[Iteration 1516/3520] TRAIN loss: 0.021\n",
      "[Iteration 1517/3520] TRAIN loss: 0.126\n",
      "[Iteration 1518/3520] TRAIN loss: 0.036\n",
      "[Iteration 1519/3520] TRAIN loss: 0.073\n",
      "[Iteration 1520/3520] TRAIN loss: 0.038\n",
      "[Iteration 1521/3520] TRAIN loss: 0.077\n",
      "[Iteration 1522/3520] TRAIN loss: 0.057\n",
      "[Iteration 1523/3520] TRAIN loss: 0.089\n",
      "[Iteration 1524/3520] TRAIN loss: 0.039\n",
      "[Iteration 1525/3520] TRAIN loss: 0.119\n",
      "[Iteration 1526/3520] TRAIN loss: 0.032\n",
      "[Iteration 1527/3520] TRAIN loss: 0.148\n",
      "[Iteration 1528/3520] TRAIN loss: 0.081\n",
      "[Iteration 1529/3520] TRAIN loss: 0.220\n",
      "[Iteration 1530/3520] TRAIN loss: 0.016\n",
      "[Iteration 1531/3520] TRAIN loss: 0.042\n",
      "[Iteration 1532/3520] TRAIN loss: 0.061\n",
      "[Iteration 1533/3520] TRAIN loss: 0.067\n",
      "[Iteration 1534/3520] TRAIN loss: 0.068\n",
      "[Iteration 1535/3520] TRAIN loss: 0.077\n",
      "[Iteration 1536/3520] TRAIN loss: 0.097\n",
      "[Iteration 1537/3520] TRAIN loss: 0.092\n",
      "[Iteration 1538/3520] TRAIN loss: 0.052\n",
      "[Iteration 1539/3520] TRAIN loss: 0.031\n",
      "[Iteration 1540/3520] TRAIN loss: 0.089\n",
      "[Iteration 1541/3520] TRAIN loss: 0.023\n",
      "[Iteration 1542/3520] TRAIN loss: 0.093\n",
      "[Iteration 1543/3520] TRAIN loss: 0.060\n",
      "[Iteration 1544/3520] TRAIN loss: 0.078\n",
      "[Iteration 1545/3520] TRAIN loss: 0.081\n",
      "[Iteration 1546/3520] TRAIN loss: 0.056\n",
      "[Iteration 1547/3520] TRAIN loss: 0.048\n",
      "[Iteration 1548/3520] TRAIN loss: 0.037\n",
      "[Iteration 1549/3520] TRAIN loss: 0.140\n",
      "[Iteration 1550/3520] TRAIN loss: 0.099\n",
      "[Iteration 1551/3520] TRAIN loss: 0.144\n",
      "[Iteration 1552/3520] TRAIN loss: 0.068\n",
      "[Iteration 1553/3520] TRAIN loss: 0.051\n",
      "[Iteration 1554/3520] TRAIN loss: 0.052\n",
      "[Iteration 1555/3520] TRAIN loss: 0.044\n",
      "[Iteration 1556/3520] TRAIN loss: 0.071\n",
      "[Iteration 1557/3520] TRAIN loss: 0.039\n",
      "[Iteration 1558/3520] TRAIN loss: 0.115\n",
      "[Iteration 1559/3520] TRAIN loss: 0.110\n",
      "[Iteration 1560/3520] TRAIN loss: 0.099\n",
      "[Iteration 1561/3520] TRAIN loss: 0.032\n",
      "[Iteration 1562/3520] TRAIN loss: 0.034\n",
      "[Iteration 1563/3520] TRAIN loss: 0.109\n",
      "[Iteration 1564/3520] TRAIN loss: 0.082\n",
      "[Iteration 1565/3520] TRAIN loss: 0.038\n",
      "[Iteration 1566/3520] TRAIN loss: 0.158\n",
      "[Iteration 1567/3520] TRAIN loss: 0.074\n",
      "[Iteration 1568/3520] TRAIN loss: 0.036\n",
      "[Iteration 1569/3520] TRAIN loss: 0.087\n",
      "[Iteration 1570/3520] TRAIN loss: 0.051\n",
      "[Iteration 1571/3520] TRAIN loss: 0.083\n",
      "[Iteration 1572/3520] TRAIN loss: 0.116\n",
      "[Iteration 1573/3520] TRAIN loss: 0.043\n",
      "[Iteration 1574/3520] TRAIN loss: 0.118\n",
      "[Iteration 1575/3520] TRAIN loss: 0.069\n",
      "[Iteration 1576/3520] TRAIN loss: 0.053\n",
      "[Iteration 1577/3520] TRAIN loss: 0.077\n",
      "[Iteration 1578/3520] TRAIN loss: 0.093\n",
      "[Iteration 1579/3520] TRAIN loss: 0.037\n",
      "[Iteration 1580/3520] TRAIN loss: 0.110\n",
      "[Iteration 1581/3520] TRAIN loss: 0.110\n",
      "[Iteration 1582/3520] TRAIN loss: 0.045\n",
      "[Iteration 1583/3520] TRAIN loss: 0.045\n",
      "[Iteration 1584/3520] TRAIN loss: 0.119\n",
      "[Iteration 1585/3520] TRAIN loss: 0.093\n",
      "[Iteration 1586/3520] TRAIN loss: 0.061\n",
      "[Iteration 1587/3520] TRAIN loss: 0.039\n",
      "[Iteration 1588/3520] TRAIN loss: 0.094\n",
      "[Iteration 1589/3520] TRAIN loss: 0.084\n",
      "[Iteration 1590/3520] TRAIN loss: 0.166\n",
      "[Iteration 1591/3520] TRAIN loss: 0.021\n",
      "[Iteration 1592/3520] TRAIN loss: 0.015\n",
      "[Iteration 1593/3520] TRAIN loss: 0.015\n",
      "[Iteration 1594/3520] TRAIN loss: 0.174\n",
      "[Iteration 1595/3520] TRAIN loss: 0.153\n",
      "[Iteration 1596/3520] TRAIN loss: 0.092\n",
      "[Iteration 1597/3520] TRAIN loss: 0.136\n",
      "[Iteration 1598/3520] TRAIN loss: 0.037\n",
      "[Iteration 1599/3520] TRAIN loss: 0.016\n",
      "[Iteration 1600/3520] TRAIN loss: 0.031\n",
      "[Iteration 1601/3520] TRAIN loss: 0.029\n",
      "[Iteration 1602/3520] TRAIN loss: 0.039\n",
      "[Iteration 1603/3520] TRAIN loss: 0.052\n",
      "[Iteration 1604/3520] TRAIN loss: 0.051\n",
      "[Iteration 1605/3520] TRAIN loss: 0.100\n",
      "[Iteration 1606/3520] TRAIN loss: 0.048\n",
      "[Iteration 1607/3520] TRAIN loss: 0.040\n",
      "[Iteration 1608/3520] TRAIN loss: 0.072\n",
      "[Iteration 1609/3520] TRAIN loss: 0.022\n",
      "[Iteration 1610/3520] TRAIN loss: 0.102\n",
      "[Iteration 1611/3520] TRAIN loss: 0.025\n",
      "[Iteration 1612/3520] TRAIN loss: 0.032\n",
      "[Iteration 1613/3520] TRAIN loss: 0.092\n",
      "[Iteration 1614/3520] TRAIN loss: 0.018\n",
      "[Iteration 1615/3520] TRAIN loss: 0.067\n",
      "[Iteration 1616/3520] TRAIN loss: 0.120\n",
      "[Iteration 1617/3520] TRAIN loss: 0.175\n",
      "[Iteration 1618/3520] TRAIN loss: 0.087\n",
      "[Iteration 1619/3520] TRAIN loss: 0.054\n",
      "[Iteration 1620/3520] TRAIN loss: 0.096\n",
      "[Iteration 1621/3520] TRAIN loss: 0.111\n",
      "[Iteration 1622/3520] TRAIN loss: 0.040\n",
      "[Iteration 1623/3520] TRAIN loss: 0.052\n",
      "[Iteration 1624/3520] TRAIN loss: 0.155\n",
      "[Iteration 1625/3520] TRAIN loss: 0.152\n",
      "[Iteration 1626/3520] TRAIN loss: 0.120\n",
      "[Iteration 1627/3520] TRAIN loss: 0.100\n",
      "[Iteration 1628/3520] TRAIN loss: 0.041\n",
      "[Iteration 1629/3520] TRAIN loss: 0.026\n",
      "[Iteration 1630/3520] TRAIN loss: 0.136\n",
      "[Iteration 1631/3520] TRAIN loss: 0.051\n",
      "[Iteration 1632/3520] TRAIN loss: 0.026\n",
      "[Iteration 1633/3520] TRAIN loss: 0.128\n",
      "[Iteration 1634/3520] TRAIN loss: 0.095\n",
      "[Iteration 1635/3520] TRAIN loss: 0.046\n",
      "[Iteration 1636/3520] TRAIN loss: 0.019\n",
      "[Iteration 1637/3520] TRAIN loss: 0.038\n",
      "[Iteration 1638/3520] TRAIN loss: 0.081\n",
      "[Iteration 1639/3520] TRAIN loss: 0.071\n",
      "[Iteration 1640/3520] TRAIN loss: 0.088\n",
      "[Iteration 1641/3520] TRAIN loss: 0.148\n",
      "[Iteration 1642/3520] TRAIN loss: 0.102\n",
      "[Iteration 1643/3520] TRAIN loss: 0.029\n",
      "[Iteration 1644/3520] TRAIN loss: 0.079\n",
      "[Iteration 1645/3520] TRAIN loss: 0.065\n",
      "[Iteration 1646/3520] TRAIN loss: 0.031\n",
      "[Iteration 1647/3520] TRAIN loss: 0.105\n",
      "[Iteration 1648/3520] TRAIN loss: 0.067\n",
      "[Iteration 1649/3520] TRAIN loss: 0.030\n",
      "[Iteration 1650/3520] TRAIN loss: 0.060\n",
      "[Iteration 1651/3520] TRAIN loss: 0.035\n",
      "[Iteration 1652/3520] TRAIN loss: 0.059\n",
      "[Iteration 1653/3520] TRAIN loss: 0.037\n",
      "[Iteration 1654/3520] TRAIN loss: 0.093\n",
      "[Iteration 1655/3520] TRAIN loss: 0.038\n",
      "[Iteration 1656/3520] TRAIN loss: 0.047\n",
      "[Iteration 1657/3520] TRAIN loss: 0.083\n",
      "[Iteration 1658/3520] TRAIN loss: 0.030\n",
      "[Iteration 1659/3520] TRAIN loss: 0.052\n",
      "[Iteration 1660/3520] TRAIN loss: 0.060\n",
      "[Iteration 1661/3520] TRAIN loss: 0.067\n",
      "[Iteration 1662/3520] TRAIN loss: 0.019\n",
      "[Iteration 1663/3520] TRAIN loss: 0.044\n",
      "[Iteration 1664/3520] TRAIN loss: 0.035\n",
      "[Iteration 1665/3520] TRAIN loss: 0.031\n",
      "[Iteration 1666/3520] TRAIN loss: 0.058\n",
      "[Iteration 1667/3520] TRAIN loss: 0.066\n",
      "[Iteration 1668/3520] TRAIN loss: 0.101\n",
      "[Iteration 1669/3520] TRAIN loss: 0.109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1670/3520] TRAIN loss: 0.048\n",
      "[Iteration 1671/3520] TRAIN loss: 0.025\n",
      "[Iteration 1672/3520] TRAIN loss: 0.074\n",
      "[Iteration 1673/3520] TRAIN loss: 0.106\n",
      "[Iteration 1674/3520] TRAIN loss: 0.036\n",
      "[Iteration 1675/3520] TRAIN loss: 0.062\n",
      "[Iteration 1676/3520] TRAIN loss: 0.058\n",
      "[Iteration 1677/3520] TRAIN loss: 0.097\n",
      "[Iteration 1678/3520] TRAIN loss: 0.073\n",
      "[Iteration 1679/3520] TRAIN loss: 0.066\n",
      "[Iteration 1680/3520] TRAIN loss: 0.054\n",
      "[Iteration 1681/3520] TRAIN loss: 0.081\n",
      "[Iteration 1682/3520] TRAIN loss: 0.180\n",
      "[Iteration 1683/3520] TRAIN loss: 0.047\n",
      "[Iteration 1684/3520] TRAIN loss: 0.035\n",
      "[Iteration 1685/3520] TRAIN loss: 0.099\n",
      "[Iteration 1686/3520] TRAIN loss: 0.105\n",
      "[Iteration 1687/3520] TRAIN loss: 0.021\n",
      "[Iteration 1688/3520] TRAIN loss: 0.066\n",
      "[Iteration 1689/3520] TRAIN loss: 0.141\n",
      "[Iteration 1690/3520] TRAIN loss: 0.094\n",
      "[Iteration 1691/3520] TRAIN loss: 0.111\n",
      "[Iteration 1692/3520] TRAIN loss: 0.119\n",
      "[Iteration 1693/3520] TRAIN loss: 0.027\n",
      "[Iteration 1694/3520] TRAIN loss: 0.070\n",
      "[Iteration 1695/3520] TRAIN loss: 0.040\n",
      "[Iteration 1696/3520] TRAIN loss: 0.058\n",
      "[Iteration 1697/3520] TRAIN loss: 0.023\n",
      "[Iteration 1698/3520] TRAIN loss: 0.064\n",
      "[Iteration 1699/3520] TRAIN loss: 0.018\n",
      "[Iteration 1700/3520] TRAIN loss: 0.022\n",
      "[Iteration 1701/3520] TRAIN loss: 0.116\n",
      "[Iteration 1702/3520] TRAIN loss: 0.025\n",
      "[Iteration 1703/3520] TRAIN loss: 0.062\n",
      "[Iteration 1704/3520] TRAIN loss: 0.125\n",
      "[Iteration 1705/3520] TRAIN loss: 0.074\n",
      "[Iteration 1706/3520] TRAIN loss: 0.051\n",
      "[Iteration 1707/3520] TRAIN loss: 0.019\n",
      "[Iteration 1708/3520] TRAIN loss: 0.038\n",
      "[Iteration 1709/3520] TRAIN loss: 0.138\n",
      "[Iteration 1710/3520] TRAIN loss: 0.019\n",
      "[Iteration 1711/3520] TRAIN loss: 0.033\n",
      "[Iteration 1712/3520] TRAIN loss: 0.163\n",
      "[Iteration 1713/3520] TRAIN loss: 0.087\n",
      "[Iteration 1714/3520] TRAIN loss: 0.070\n",
      "[Iteration 1715/3520] TRAIN loss: 0.086\n",
      "[Iteration 1716/3520] TRAIN loss: 0.073\n",
      "[Iteration 1717/3520] TRAIN loss: 0.126\n",
      "[Iteration 1718/3520] TRAIN loss: 0.035\n",
      "[Iteration 1719/3520] TRAIN loss: 0.065\n",
      "[Iteration 1720/3520] TRAIN loss: 0.090\n",
      "[Iteration 1721/3520] TRAIN loss: 0.029\n",
      "[Iteration 1722/3520] TRAIN loss: 0.140\n",
      "[Iteration 1723/3520] TRAIN loss: 0.108\n",
      "[Iteration 1724/3520] TRAIN loss: 0.048\n",
      "[Iteration 1725/3520] TRAIN loss: 0.172\n",
      "[Iteration 1726/3520] TRAIN loss: 0.148\n",
      "[Iteration 1727/3520] TRAIN loss: 0.151\n",
      "[Iteration 1728/3520] TRAIN loss: 0.056\n",
      "[Iteration 1729/3520] TRAIN loss: 0.139\n",
      "[Iteration 1730/3520] TRAIN loss: 0.041\n",
      "[Iteration 1731/3520] TRAIN loss: 0.036\n",
      "[Iteration 1732/3520] TRAIN loss: 0.086\n",
      "[Iteration 1733/3520] TRAIN loss: 0.127\n",
      "[Iteration 1734/3520] TRAIN loss: 0.045\n",
      "[Iteration 1735/3520] TRAIN loss: 0.022\n",
      "[Iteration 1736/3520] TRAIN loss: 0.086\n",
      "[Iteration 1737/3520] TRAIN loss: 0.039\n",
      "[Iteration 1738/3520] TRAIN loss: 0.028\n",
      "[Iteration 1739/3520] TRAIN loss: 0.067\n",
      "[Iteration 1740/3520] TRAIN loss: 0.103\n",
      "[Iteration 1741/3520] TRAIN loss: 0.076\n",
      "[Iteration 1742/3520] TRAIN loss: 0.029\n",
      "[Iteration 1743/3520] TRAIN loss: 0.048\n",
      "[Iteration 1744/3520] TRAIN loss: 0.070\n",
      "[Iteration 1745/3520] TRAIN loss: 0.065\n",
      "[Iteration 1746/3520] TRAIN loss: 0.054\n",
      "[Iteration 1747/3520] TRAIN loss: 0.015\n",
      "[Iteration 1748/3520] TRAIN loss: 0.112\n",
      "[Iteration 1749/3520] TRAIN loss: 0.026\n",
      "[Iteration 1750/3520] TRAIN loss: 0.120\n",
      "[Iteration 1751/3520] TRAIN loss: 0.097\n",
      "[Iteration 1752/3520] TRAIN loss: 0.018\n",
      "[Iteration 1753/3520] TRAIN loss: 0.127\n",
      "[Iteration 1754/3520] TRAIN loss: 0.112\n",
      "[Iteration 1755/3520] TRAIN loss: 0.100\n",
      "[Iteration 1756/3520] TRAIN loss: 0.076\n",
      "[Iteration 1757/3520] TRAIN loss: 0.086\n",
      "[Iteration 1758/3520] TRAIN loss: 0.056\n",
      "[Iteration 1759/3520] TRAIN loss: 0.069\n",
      "[Iteration 1760/3520] TRAIN loss: 0.061\n",
      "[Epoch 5/10] TRAIN acc/loss: 0.986/0.061\n",
      "[Epoch 5/10] VAL   acc/loss: 0.975/0.084\n",
      "[Iteration 1761/3520] TRAIN loss: 0.062\n",
      "[Iteration 1762/3520] TRAIN loss: 0.044\n",
      "[Iteration 1763/3520] TRAIN loss: 0.039\n",
      "[Iteration 1764/3520] TRAIN loss: 0.047\n",
      "[Iteration 1765/3520] TRAIN loss: 0.087\n",
      "[Iteration 1766/3520] TRAIN loss: 0.048\n",
      "[Iteration 1767/3520] TRAIN loss: 0.014\n",
      "[Iteration 1768/3520] TRAIN loss: 0.076\n",
      "[Iteration 1769/3520] TRAIN loss: 0.063\n",
      "[Iteration 1770/3520] TRAIN loss: 0.125\n",
      "[Iteration 1771/3520] TRAIN loss: 0.059\n",
      "[Iteration 1772/3520] TRAIN loss: 0.121\n",
      "[Iteration 1773/3520] TRAIN loss: 0.034\n",
      "[Iteration 1774/3520] TRAIN loss: 0.060\n",
      "[Iteration 1775/3520] TRAIN loss: 0.064\n",
      "[Iteration 1776/3520] TRAIN loss: 0.077\n",
      "[Iteration 1777/3520] TRAIN loss: 0.034\n",
      "[Iteration 1778/3520] TRAIN loss: 0.037\n",
      "[Iteration 1779/3520] TRAIN loss: 0.092\n",
      "[Iteration 1780/3520] TRAIN loss: 0.030\n",
      "[Iteration 1781/3520] TRAIN loss: 0.040\n",
      "[Iteration 1782/3520] TRAIN loss: 0.080\n",
      "[Iteration 1783/3520] TRAIN loss: 0.080\n",
      "[Iteration 1784/3520] TRAIN loss: 0.109\n",
      "[Iteration 1785/3520] TRAIN loss: 0.034\n",
      "[Iteration 1786/3520] TRAIN loss: 0.072\n",
      "[Iteration 1787/3520] TRAIN loss: 0.064\n",
      "[Iteration 1788/3520] TRAIN loss: 0.024\n",
      "[Iteration 1789/3520] TRAIN loss: 0.062\n",
      "[Iteration 1790/3520] TRAIN loss: 0.068\n",
      "[Iteration 1791/3520] TRAIN loss: 0.034\n",
      "[Iteration 1792/3520] TRAIN loss: 0.028\n",
      "[Iteration 1793/3520] TRAIN loss: 0.037\n",
      "[Iteration 1794/3520] TRAIN loss: 0.069\n",
      "[Iteration 1795/3520] TRAIN loss: 0.060\n",
      "[Iteration 1796/3520] TRAIN loss: 0.060\n",
      "[Iteration 1797/3520] TRAIN loss: 0.058\n",
      "[Iteration 1798/3520] TRAIN loss: 0.047\n",
      "[Iteration 1799/3520] TRAIN loss: 0.045\n",
      "[Iteration 1800/3520] TRAIN loss: 0.079\n",
      "[Iteration 1801/3520] TRAIN loss: 0.014\n",
      "[Iteration 1802/3520] TRAIN loss: 0.088\n",
      "[Iteration 1803/3520] TRAIN loss: 0.021\n",
      "[Iteration 1804/3520] TRAIN loss: 0.032\n",
      "[Iteration 1805/3520] TRAIN loss: 0.020\n",
      "[Iteration 1806/3520] TRAIN loss: 0.095\n",
      "[Iteration 1807/3520] TRAIN loss: 0.118\n",
      "[Iteration 1808/3520] TRAIN loss: 0.036\n",
      "[Iteration 1809/3520] TRAIN loss: 0.055\n",
      "[Iteration 1810/3520] TRAIN loss: 0.057\n",
      "[Iteration 1811/3520] TRAIN loss: 0.042\n",
      "[Iteration 1812/3520] TRAIN loss: 0.028\n",
      "[Iteration 1813/3520] TRAIN loss: 0.071\n",
      "[Iteration 1814/3520] TRAIN loss: 0.027\n",
      "[Iteration 1815/3520] TRAIN loss: 0.046\n",
      "[Iteration 1816/3520] TRAIN loss: 0.048\n",
      "[Iteration 1817/3520] TRAIN loss: 0.073\n",
      "[Iteration 1818/3520] TRAIN loss: 0.088\n",
      "[Iteration 1819/3520] TRAIN loss: 0.059\n",
      "[Iteration 1820/3520] TRAIN loss: 0.072\n",
      "[Iteration 1821/3520] TRAIN loss: 0.086\n",
      "[Iteration 1822/3520] TRAIN loss: 0.076\n",
      "[Iteration 1823/3520] TRAIN loss: 0.013\n",
      "[Iteration 1824/3520] TRAIN loss: 0.020\n",
      "[Iteration 1825/3520] TRAIN loss: 0.031\n",
      "[Iteration 1826/3520] TRAIN loss: 0.020\n",
      "[Iteration 1827/3520] TRAIN loss: 0.052\n",
      "[Iteration 1828/3520] TRAIN loss: 0.047\n",
      "[Iteration 1829/3520] TRAIN loss: 0.020\n",
      "[Iteration 1830/3520] TRAIN loss: 0.040\n",
      "[Iteration 1831/3520] TRAIN loss: 0.093\n",
      "[Iteration 1832/3520] TRAIN loss: 0.018\n",
      "[Iteration 1833/3520] TRAIN loss: 0.051\n",
      "[Iteration 1834/3520] TRAIN loss: 0.016\n",
      "[Iteration 1835/3520] TRAIN loss: 0.017\n",
      "[Iteration 1836/3520] TRAIN loss: 0.022\n",
      "[Iteration 1837/3520] TRAIN loss: 0.047\n",
      "[Iteration 1838/3520] TRAIN loss: 0.056\n",
      "[Iteration 1839/3520] TRAIN loss: 0.021\n",
      "[Iteration 1840/3520] TRAIN loss: 0.015\n",
      "[Iteration 1841/3520] TRAIN loss: 0.145\n",
      "[Iteration 1842/3520] TRAIN loss: 0.033\n",
      "[Iteration 1843/3520] TRAIN loss: 0.020\n",
      "[Iteration 1844/3520] TRAIN loss: 0.016\n",
      "[Iteration 1845/3520] TRAIN loss: 0.022\n",
      "[Iteration 1846/3520] TRAIN loss: 0.082\n",
      "[Iteration 1847/3520] TRAIN loss: 0.030\n",
      "[Iteration 1848/3520] TRAIN loss: 0.057\n",
      "[Iteration 1849/3520] TRAIN loss: 0.022\n",
      "[Iteration 1850/3520] TRAIN loss: 0.100\n",
      "[Iteration 1851/3520] TRAIN loss: 0.068\n",
      "[Iteration 1852/3520] TRAIN loss: 0.087\n",
      "[Iteration 1853/3520] TRAIN loss: 0.017\n",
      "[Iteration 1854/3520] TRAIN loss: 0.052\n",
      "[Iteration 1855/3520] TRAIN loss: 0.028\n",
      "[Iteration 1856/3520] TRAIN loss: 0.035\n",
      "[Iteration 1857/3520] TRAIN loss: 0.020\n",
      "[Iteration 1858/3520] TRAIN loss: 0.030\n",
      "[Iteration 1859/3520] TRAIN loss: 0.067\n",
      "[Iteration 1860/3520] TRAIN loss: 0.098\n",
      "[Iteration 1861/3520] TRAIN loss: 0.014\n",
      "[Iteration 1862/3520] TRAIN loss: 0.050\n",
      "[Iteration 1863/3520] TRAIN loss: 0.067\n",
      "[Iteration 1864/3520] TRAIN loss: 0.076\n",
      "[Iteration 1865/3520] TRAIN loss: 0.065\n",
      "[Iteration 1866/3520] TRAIN loss: 0.043\n",
      "[Iteration 1867/3520] TRAIN loss: 0.054\n",
      "[Iteration 1868/3520] TRAIN loss: 0.028\n",
      "[Iteration 1869/3520] TRAIN loss: 0.049\n",
      "[Iteration 1870/3520] TRAIN loss: 0.054\n",
      "[Iteration 1871/3520] TRAIN loss: 0.107\n",
      "[Iteration 1872/3520] TRAIN loss: 0.027\n",
      "[Iteration 1873/3520] TRAIN loss: 0.027\n",
      "[Iteration 1874/3520] TRAIN loss: 0.020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1875/3520] TRAIN loss: 0.065\n",
      "[Iteration 1876/3520] TRAIN loss: 0.093\n",
      "[Iteration 1877/3520] TRAIN loss: 0.144\n",
      "[Iteration 1878/3520] TRAIN loss: 0.033\n",
      "[Iteration 1879/3520] TRAIN loss: 0.023\n",
      "[Iteration 1880/3520] TRAIN loss: 0.091\n",
      "[Iteration 1881/3520] TRAIN loss: 0.042\n",
      "[Iteration 1882/3520] TRAIN loss: 0.041\n",
      "[Iteration 1883/3520] TRAIN loss: 0.107\n",
      "[Iteration 1884/3520] TRAIN loss: 0.052\n",
      "[Iteration 1885/3520] TRAIN loss: 0.011\n",
      "[Iteration 1886/3520] TRAIN loss: 0.100\n",
      "[Iteration 1887/3520] TRAIN loss: 0.017\n",
      "[Iteration 1888/3520] TRAIN loss: 0.037\n",
      "[Iteration 1889/3520] TRAIN loss: 0.028\n",
      "[Iteration 1890/3520] TRAIN loss: 0.076\n",
      "[Iteration 1891/3520] TRAIN loss: 0.142\n",
      "[Iteration 1892/3520] TRAIN loss: 0.072\n",
      "[Iteration 1893/3520] TRAIN loss: 0.113\n",
      "[Iteration 1894/3520] TRAIN loss: 0.034\n",
      "[Iteration 1895/3520] TRAIN loss: 0.092\n",
      "[Iteration 1896/3520] TRAIN loss: 0.051\n",
      "[Iteration 1897/3520] TRAIN loss: 0.038\n",
      "[Iteration 1898/3520] TRAIN loss: 0.056\n",
      "[Iteration 1899/3520] TRAIN loss: 0.063\n",
      "[Iteration 1900/3520] TRAIN loss: 0.029\n",
      "[Iteration 1901/3520] TRAIN loss: 0.034\n",
      "[Iteration 1902/3520] TRAIN loss: 0.018\n",
      "[Iteration 1903/3520] TRAIN loss: 0.054\n",
      "[Iteration 1904/3520] TRAIN loss: 0.011\n",
      "[Iteration 1905/3520] TRAIN loss: 0.136\n",
      "[Iteration 1906/3520] TRAIN loss: 0.035\n",
      "[Iteration 1907/3520] TRAIN loss: 0.027\n",
      "[Iteration 1908/3520] TRAIN loss: 0.028\n",
      "[Iteration 1909/3520] TRAIN loss: 0.106\n",
      "[Iteration 1910/3520] TRAIN loss: 0.079\n",
      "[Iteration 1911/3520] TRAIN loss: 0.048\n",
      "[Iteration 1912/3520] TRAIN loss: 0.105\n",
      "[Iteration 1913/3520] TRAIN loss: 0.053\n",
      "[Iteration 1914/3520] TRAIN loss: 0.004\n",
      "[Iteration 1915/3520] TRAIN loss: 0.026\n",
      "[Iteration 1916/3520] TRAIN loss: 0.028\n",
      "[Iteration 1917/3520] TRAIN loss: 0.061\n",
      "[Iteration 1918/3520] TRAIN loss: 0.017\n",
      "[Iteration 1919/3520] TRAIN loss: 0.022\n",
      "[Iteration 1920/3520] TRAIN loss: 0.081\n",
      "[Iteration 1921/3520] TRAIN loss: 0.020\n",
      "[Iteration 1922/3520] TRAIN loss: 0.093\n",
      "[Iteration 1923/3520] TRAIN loss: 0.096\n",
      "[Iteration 1924/3520] TRAIN loss: 0.050\n",
      "[Iteration 1925/3520] TRAIN loss: 0.112\n",
      "[Iteration 1926/3520] TRAIN loss: 0.083\n",
      "[Iteration 1927/3520] TRAIN loss: 0.043\n",
      "[Iteration 1928/3520] TRAIN loss: 0.057\n",
      "[Iteration 1929/3520] TRAIN loss: 0.087\n",
      "[Iteration 1930/3520] TRAIN loss: 0.013\n",
      "[Iteration 1931/3520] TRAIN loss: 0.048\n",
      "[Iteration 1932/3520] TRAIN loss: 0.099\n",
      "[Iteration 1933/3520] TRAIN loss: 0.058\n",
      "[Iteration 1934/3520] TRAIN loss: 0.083\n",
      "[Iteration 1935/3520] TRAIN loss: 0.038\n",
      "[Iteration 1936/3520] TRAIN loss: 0.083\n",
      "[Iteration 1937/3520] TRAIN loss: 0.065\n",
      "[Iteration 1938/3520] TRAIN loss: 0.037\n",
      "[Iteration 1939/3520] TRAIN loss: 0.035\n",
      "[Iteration 1940/3520] TRAIN loss: 0.041\n",
      "[Iteration 1941/3520] TRAIN loss: 0.034\n",
      "[Iteration 1942/3520] TRAIN loss: 0.115\n",
      "[Iteration 1943/3520] TRAIN loss: 0.036\n",
      "[Iteration 1944/3520] TRAIN loss: 0.029\n",
      "[Iteration 1945/3520] TRAIN loss: 0.034\n",
      "[Iteration 1946/3520] TRAIN loss: 0.088\n",
      "[Iteration 1947/3520] TRAIN loss: 0.021\n",
      "[Iteration 1948/3520] TRAIN loss: 0.039\n",
      "[Iteration 1949/3520] TRAIN loss: 0.041\n",
      "[Iteration 1950/3520] TRAIN loss: 0.036\n",
      "[Iteration 1951/3520] TRAIN loss: 0.046\n",
      "[Iteration 1952/3520] TRAIN loss: 0.128\n",
      "[Iteration 1953/3520] TRAIN loss: 0.028\n",
      "[Iteration 1954/3520] TRAIN loss: 0.143\n",
      "[Iteration 1955/3520] TRAIN loss: 0.029\n",
      "[Iteration 1956/3520] TRAIN loss: 0.080\n",
      "[Iteration 1957/3520] TRAIN loss: 0.032\n",
      "[Iteration 1958/3520] TRAIN loss: 0.063\n",
      "[Iteration 1959/3520] TRAIN loss: 0.043\n",
      "[Iteration 1960/3520] TRAIN loss: 0.031\n",
      "[Iteration 1961/3520] TRAIN loss: 0.037\n",
      "[Iteration 1962/3520] TRAIN loss: 0.042\n",
      "[Iteration 1963/3520] TRAIN loss: 0.053\n",
      "[Iteration 1964/3520] TRAIN loss: 0.028\n",
      "[Iteration 1965/3520] TRAIN loss: 0.119\n",
      "[Iteration 1966/3520] TRAIN loss: 0.052\n",
      "[Iteration 1967/3520] TRAIN loss: 0.044\n",
      "[Iteration 1968/3520] TRAIN loss: 0.022\n",
      "[Iteration 1969/3520] TRAIN loss: 0.056\n",
      "[Iteration 1970/3520] TRAIN loss: 0.047\n",
      "[Iteration 1971/3520] TRAIN loss: 0.030\n",
      "[Iteration 1972/3520] TRAIN loss: 0.058\n",
      "[Iteration 1973/3520] TRAIN loss: 0.038\n",
      "[Iteration 1974/3520] TRAIN loss: 0.035\n",
      "[Iteration 1975/3520] TRAIN loss: 0.039\n",
      "[Iteration 1976/3520] TRAIN loss: 0.028\n",
      "[Iteration 1977/3520] TRAIN loss: 0.086\n",
      "[Iteration 1978/3520] TRAIN loss: 0.064\n",
      "[Iteration 1979/3520] TRAIN loss: 0.130\n",
      "[Iteration 1980/3520] TRAIN loss: 0.056\n",
      "[Iteration 1981/3520] TRAIN loss: 0.100\n",
      "[Iteration 1982/3520] TRAIN loss: 0.042\n",
      "[Iteration 1983/3520] TRAIN loss: 0.038\n",
      "[Iteration 1984/3520] TRAIN loss: 0.078\n",
      "[Iteration 1985/3520] TRAIN loss: 0.173\n",
      "[Iteration 1986/3520] TRAIN loss: 0.121\n",
      "[Iteration 1987/3520] TRAIN loss: 0.032\n",
      "[Iteration 1988/3520] TRAIN loss: 0.062\n",
      "[Iteration 1989/3520] TRAIN loss: 0.053\n",
      "[Iteration 1990/3520] TRAIN loss: 0.053\n",
      "[Iteration 1991/3520] TRAIN loss: 0.061\n",
      "[Iteration 1992/3520] TRAIN loss: 0.046\n",
      "[Iteration 1993/3520] TRAIN loss: 0.099\n",
      "[Iteration 1994/3520] TRAIN loss: 0.017\n",
      "[Iteration 1995/3520] TRAIN loss: 0.118\n",
      "[Iteration 1996/3520] TRAIN loss: 0.150\n",
      "[Iteration 1997/3520] TRAIN loss: 0.032\n",
      "[Iteration 1998/3520] TRAIN loss: 0.052\n",
      "[Iteration 1999/3520] TRAIN loss: 0.087\n",
      "[Iteration 2000/3520] TRAIN loss: 0.056\n",
      "[Iteration 2001/3520] TRAIN loss: 0.062\n",
      "[Iteration 2002/3520] TRAIN loss: 0.067\n",
      "[Iteration 2003/3520] TRAIN loss: 0.023\n",
      "[Iteration 2004/3520] TRAIN loss: 0.048\n",
      "[Iteration 2005/3520] TRAIN loss: 0.013\n",
      "[Iteration 2006/3520] TRAIN loss: 0.016\n",
      "[Iteration 2007/3520] TRAIN loss: 0.038\n",
      "[Iteration 2008/3520] TRAIN loss: 0.016\n",
      "[Iteration 2009/3520] TRAIN loss: 0.027\n",
      "[Iteration 2010/3520] TRAIN loss: 0.199\n",
      "[Iteration 2011/3520] TRAIN loss: 0.017\n",
      "[Iteration 2012/3520] TRAIN loss: 0.076\n",
      "[Iteration 2013/3520] TRAIN loss: 0.036\n",
      "[Iteration 2014/3520] TRAIN loss: 0.023\n",
      "[Iteration 2015/3520] TRAIN loss: 0.061\n",
      "[Iteration 2016/3520] TRAIN loss: 0.037\n",
      "[Iteration 2017/3520] TRAIN loss: 0.009\n",
      "[Iteration 2018/3520] TRAIN loss: 0.138\n",
      "[Iteration 2019/3520] TRAIN loss: 0.050\n",
      "[Iteration 2020/3520] TRAIN loss: 0.096\n",
      "[Iteration 2021/3520] TRAIN loss: 0.104\n",
      "[Iteration 2022/3520] TRAIN loss: 0.061\n",
      "[Iteration 2023/3520] TRAIN loss: 0.034\n",
      "[Iteration 2024/3520] TRAIN loss: 0.091\n",
      "[Iteration 2025/3520] TRAIN loss: 0.046\n",
      "[Iteration 2026/3520] TRAIN loss: 0.075\n",
      "[Iteration 2027/3520] TRAIN loss: 0.100\n",
      "[Iteration 2028/3520] TRAIN loss: 0.045\n",
      "[Iteration 2029/3520] TRAIN loss: 0.026\n",
      "[Iteration 2030/3520] TRAIN loss: 0.055\n",
      "[Iteration 2031/3520] TRAIN loss: 0.036\n",
      "[Iteration 2032/3520] TRAIN loss: 0.061\n",
      "[Iteration 2033/3520] TRAIN loss: 0.107\n",
      "[Iteration 2034/3520] TRAIN loss: 0.042\n",
      "[Iteration 2035/3520] TRAIN loss: 0.079\n",
      "[Iteration 2036/3520] TRAIN loss: 0.047\n",
      "[Iteration 2037/3520] TRAIN loss: 0.083\n",
      "[Iteration 2038/3520] TRAIN loss: 0.024\n",
      "[Iteration 2039/3520] TRAIN loss: 0.015\n",
      "[Iteration 2040/3520] TRAIN loss: 0.053\n",
      "[Iteration 2041/3520] TRAIN loss: 0.087\n",
      "[Iteration 2042/3520] TRAIN loss: 0.083\n",
      "[Iteration 2043/3520] TRAIN loss: 0.072\n",
      "[Iteration 2044/3520] TRAIN loss: 0.062\n",
      "[Iteration 2045/3520] TRAIN loss: 0.026\n",
      "[Iteration 2046/3520] TRAIN loss: 0.130\n",
      "[Iteration 2047/3520] TRAIN loss: 0.020\n",
      "[Iteration 2048/3520] TRAIN loss: 0.108\n",
      "[Iteration 2049/3520] TRAIN loss: 0.028\n",
      "[Iteration 2050/3520] TRAIN loss: 0.069\n",
      "[Iteration 2051/3520] TRAIN loss: 0.072\n",
      "[Iteration 2052/3520] TRAIN loss: 0.023\n",
      "[Iteration 2053/3520] TRAIN loss: 0.071\n",
      "[Iteration 2054/3520] TRAIN loss: 0.084\n",
      "[Iteration 2055/3520] TRAIN loss: 0.121\n",
      "[Iteration 2056/3520] TRAIN loss: 0.053\n",
      "[Iteration 2057/3520] TRAIN loss: 0.045\n",
      "[Iteration 2058/3520] TRAIN loss: 0.041\n",
      "[Iteration 2059/3520] TRAIN loss: 0.010\n",
      "[Iteration 2060/3520] TRAIN loss: 0.059\n",
      "[Iteration 2061/3520] TRAIN loss: 0.051\n",
      "[Iteration 2062/3520] TRAIN loss: 0.057\n",
      "[Iteration 2063/3520] TRAIN loss: 0.076\n",
      "[Iteration 2064/3520] TRAIN loss: 0.084\n",
      "[Iteration 2065/3520] TRAIN loss: 0.034\n",
      "[Iteration 2066/3520] TRAIN loss: 0.069\n",
      "[Iteration 2067/3520] TRAIN loss: 0.097\n",
      "[Iteration 2068/3520] TRAIN loss: 0.058\n",
      "[Iteration 2069/3520] TRAIN loss: 0.023\n",
      "[Iteration 2070/3520] TRAIN loss: 0.052\n",
      "[Iteration 2071/3520] TRAIN loss: 0.049\n",
      "[Iteration 2072/3520] TRAIN loss: 0.062\n",
      "[Iteration 2073/3520] TRAIN loss: 0.019\n",
      "[Iteration 2074/3520] TRAIN loss: 0.096\n",
      "[Iteration 2075/3520] TRAIN loss: 0.026\n",
      "[Iteration 2076/3520] TRAIN loss: 0.096\n",
      "[Iteration 2077/3520] TRAIN loss: 0.045\n",
      "[Iteration 2078/3520] TRAIN loss: 0.067\n",
      "[Iteration 2079/3520] TRAIN loss: 0.105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 2080/3520] TRAIN loss: 0.021\n",
      "[Iteration 2081/3520] TRAIN loss: 0.042\n",
      "[Iteration 2082/3520] TRAIN loss: 0.058\n",
      "[Iteration 2083/3520] TRAIN loss: 0.020\n",
      "[Iteration 2084/3520] TRAIN loss: 0.038\n",
      "[Iteration 2085/3520] TRAIN loss: 0.019\n",
      "[Iteration 2086/3520] TRAIN loss: 0.029\n",
      "[Iteration 2087/3520] TRAIN loss: 0.020\n",
      "[Iteration 2088/3520] TRAIN loss: 0.045\n",
      "[Iteration 2089/3520] TRAIN loss: 0.116\n",
      "[Iteration 2090/3520] TRAIN loss: 0.075\n",
      "[Iteration 2091/3520] TRAIN loss: 0.016\n",
      "[Iteration 2092/3520] TRAIN loss: 0.060\n",
      "[Iteration 2093/3520] TRAIN loss: 0.070\n",
      "[Iteration 2094/3520] TRAIN loss: 0.101\n",
      "[Iteration 2095/3520] TRAIN loss: 0.149\n",
      "[Iteration 2096/3520] TRAIN loss: 0.074\n",
      "[Iteration 2097/3520] TRAIN loss: 0.147\n",
      "[Iteration 2098/3520] TRAIN loss: 0.086\n",
      "[Iteration 2099/3520] TRAIN loss: 0.057\n",
      "[Iteration 2100/3520] TRAIN loss: 0.061\n",
      "[Iteration 2101/3520] TRAIN loss: 0.037\n",
      "[Iteration 2102/3520] TRAIN loss: 0.081\n",
      "[Iteration 2103/3520] TRAIN loss: 0.053\n",
      "[Iteration 2104/3520] TRAIN loss: 0.052\n",
      "[Iteration 2105/3520] TRAIN loss: 0.083\n",
      "[Iteration 2106/3520] TRAIN loss: 0.037\n",
      "[Iteration 2107/3520] TRAIN loss: 0.156\n",
      "[Iteration 2108/3520] TRAIN loss: 0.012\n",
      "[Iteration 2109/3520] TRAIN loss: 0.032\n",
      "[Iteration 2110/3520] TRAIN loss: 0.012\n",
      "[Iteration 2111/3520] TRAIN loss: 0.077\n",
      "[Iteration 2112/3520] TRAIN loss: 0.068\n",
      "[Epoch 6/10] TRAIN acc/loss: 0.972/0.068\n",
      "[Epoch 6/10] VAL   acc/loss: 0.976/0.079\n",
      "[Iteration 2113/3520] TRAIN loss: 0.026\n",
      "[Iteration 2114/3520] TRAIN loss: 0.073\n",
      "[Iteration 2115/3520] TRAIN loss: 0.061\n",
      "[Iteration 2116/3520] TRAIN loss: 0.059\n",
      "[Iteration 2117/3520] TRAIN loss: 0.024\n",
      "[Iteration 2118/3520] TRAIN loss: 0.084\n",
      "[Iteration 2119/3520] TRAIN loss: 0.035\n",
      "[Iteration 2120/3520] TRAIN loss: 0.063\n",
      "[Iteration 2121/3520] TRAIN loss: 0.112\n",
      "[Iteration 2122/3520] TRAIN loss: 0.080\n",
      "[Iteration 2123/3520] TRAIN loss: 0.015\n",
      "[Iteration 2124/3520] TRAIN loss: 0.036\n",
      "[Iteration 2125/3520] TRAIN loss: 0.061\n",
      "[Iteration 2126/3520] TRAIN loss: 0.041\n",
      "[Iteration 2127/3520] TRAIN loss: 0.017\n",
      "[Iteration 2128/3520] TRAIN loss: 0.078\n",
      "[Iteration 2129/3520] TRAIN loss: 0.008\n",
      "[Iteration 2130/3520] TRAIN loss: 0.038\n",
      "[Iteration 2131/3520] TRAIN loss: 0.061\n",
      "[Iteration 2132/3520] TRAIN loss: 0.025\n",
      "[Iteration 2133/3520] TRAIN loss: 0.050\n",
      "[Iteration 2134/3520] TRAIN loss: 0.049\n",
      "[Iteration 2135/3520] TRAIN loss: 0.085\n",
      "[Iteration 2136/3520] TRAIN loss: 0.034\n",
      "[Iteration 2137/3520] TRAIN loss: 0.018\n",
      "[Iteration 2138/3520] TRAIN loss: 0.026\n",
      "[Iteration 2139/3520] TRAIN loss: 0.082\n",
      "[Iteration 2140/3520] TRAIN loss: 0.060\n",
      "[Iteration 2141/3520] TRAIN loss: 0.033\n",
      "[Iteration 2142/3520] TRAIN loss: 0.014\n",
      "[Iteration 2143/3520] TRAIN loss: 0.051\n",
      "[Iteration 2144/3520] TRAIN loss: 0.108\n",
      "[Iteration 2145/3520] TRAIN loss: 0.060\n",
      "[Iteration 2146/3520] TRAIN loss: 0.030\n",
      "[Iteration 2147/3520] TRAIN loss: 0.097\n",
      "[Iteration 2148/3520] TRAIN loss: 0.043\n",
      "[Iteration 2149/3520] TRAIN loss: 0.046\n",
      "[Iteration 2150/3520] TRAIN loss: 0.054\n",
      "[Iteration 2151/3520] TRAIN loss: 0.027\n",
      "[Iteration 2152/3520] TRAIN loss: 0.047\n",
      "[Iteration 2153/3520] TRAIN loss: 0.035\n",
      "[Iteration 2154/3520] TRAIN loss: 0.039\n",
      "[Iteration 2155/3520] TRAIN loss: 0.017\n",
      "[Iteration 2156/3520] TRAIN loss: 0.021\n",
      "[Iteration 2157/3520] TRAIN loss: 0.095\n",
      "[Iteration 2158/3520] TRAIN loss: 0.020\n",
      "[Iteration 2159/3520] TRAIN loss: 0.010\n",
      "[Iteration 2160/3520] TRAIN loss: 0.081\n",
      "[Iteration 2161/3520] TRAIN loss: 0.010\n",
      "[Iteration 2162/3520] TRAIN loss: 0.056\n",
      "[Iteration 2163/3520] TRAIN loss: 0.039\n",
      "[Iteration 2164/3520] TRAIN loss: 0.022\n",
      "[Iteration 2165/3520] TRAIN loss: 0.049\n",
      "[Iteration 2166/3520] TRAIN loss: 0.067\n",
      "[Iteration 2167/3520] TRAIN loss: 0.020\n",
      "[Iteration 2168/3520] TRAIN loss: 0.056\n",
      "[Iteration 2169/3520] TRAIN loss: 0.028\n",
      "[Iteration 2170/3520] TRAIN loss: 0.029\n",
      "[Iteration 2171/3520] TRAIN loss: 0.010\n",
      "[Iteration 2172/3520] TRAIN loss: 0.024\n",
      "[Iteration 2173/3520] TRAIN loss: 0.079\n",
      "[Iteration 2174/3520] TRAIN loss: 0.076\n",
      "[Iteration 2175/3520] TRAIN loss: 0.012\n",
      "[Iteration 2176/3520] TRAIN loss: 0.021\n",
      "[Iteration 2177/3520] TRAIN loss: 0.024\n",
      "[Iteration 2178/3520] TRAIN loss: 0.019\n",
      "[Iteration 2179/3520] TRAIN loss: 0.037\n",
      "[Iteration 2180/3520] TRAIN loss: 0.016\n",
      "[Iteration 2181/3520] TRAIN loss: 0.055\n",
      "[Iteration 2182/3520] TRAIN loss: 0.027\n",
      "[Iteration 2183/3520] TRAIN loss: 0.072\n",
      "[Iteration 2184/3520] TRAIN loss: 0.071\n",
      "[Iteration 2185/3520] TRAIN loss: 0.048\n",
      "[Iteration 2186/3520] TRAIN loss: 0.040\n",
      "[Iteration 2187/3520] TRAIN loss: 0.106\n",
      "[Iteration 2188/3520] TRAIN loss: 0.004\n",
      "[Iteration 2189/3520] TRAIN loss: 0.050\n",
      "[Iteration 2190/3520] TRAIN loss: 0.070\n",
      "[Iteration 2191/3520] TRAIN loss: 0.060\n",
      "[Iteration 2192/3520] TRAIN loss: 0.044\n",
      "[Iteration 2193/3520] TRAIN loss: 0.031\n",
      "[Iteration 2194/3520] TRAIN loss: 0.010\n",
      "[Iteration 2195/3520] TRAIN loss: 0.071\n",
      "[Iteration 2196/3520] TRAIN loss: 0.155\n",
      "[Iteration 2197/3520] TRAIN loss: 0.029\n",
      "[Iteration 2198/3520] TRAIN loss: 0.057\n",
      "[Iteration 2199/3520] TRAIN loss: 0.043\n",
      "[Iteration 2200/3520] TRAIN loss: 0.048\n",
      "[Iteration 2201/3520] TRAIN loss: 0.088\n",
      "[Iteration 2202/3520] TRAIN loss: 0.033\n",
      "[Iteration 2203/3520] TRAIN loss: 0.078\n",
      "[Iteration 2204/3520] TRAIN loss: 0.018\n",
      "[Iteration 2205/3520] TRAIN loss: 0.044\n",
      "[Iteration 2206/3520] TRAIN loss: 0.028\n",
      "[Iteration 2207/3520] TRAIN loss: 0.036\n",
      "[Iteration 2208/3520] TRAIN loss: 0.045\n",
      "[Iteration 2209/3520] TRAIN loss: 0.042\n",
      "[Iteration 2210/3520] TRAIN loss: 0.052\n",
      "[Iteration 2211/3520] TRAIN loss: 0.067\n",
      "[Iteration 2212/3520] TRAIN loss: 0.008\n",
      "[Iteration 2213/3520] TRAIN loss: 0.043\n",
      "[Iteration 2214/3520] TRAIN loss: 0.043\n",
      "[Iteration 2215/3520] TRAIN loss: 0.056\n",
      "[Iteration 2216/3520] TRAIN loss: 0.030\n",
      "[Iteration 2217/3520] TRAIN loss: 0.072\n",
      "[Iteration 2218/3520] TRAIN loss: 0.042\n",
      "[Iteration 2219/3520] TRAIN loss: 0.116\n",
      "[Iteration 2220/3520] TRAIN loss: 0.070\n",
      "[Iteration 2221/3520] TRAIN loss: 0.022\n",
      "[Iteration 2222/3520] TRAIN loss: 0.070\n",
      "[Iteration 2223/3520] TRAIN loss: 0.080\n",
      "[Iteration 2224/3520] TRAIN loss: 0.011\n",
      "[Iteration 2225/3520] TRAIN loss: 0.064\n",
      "[Iteration 2226/3520] TRAIN loss: 0.089\n",
      "[Iteration 2227/3520] TRAIN loss: 0.011\n",
      "[Iteration 2228/3520] TRAIN loss: 0.049\n",
      "[Iteration 2229/3520] TRAIN loss: 0.034\n",
      "[Iteration 2230/3520] TRAIN loss: 0.066\n",
      "[Iteration 2231/3520] TRAIN loss: 0.053\n",
      "[Iteration 2232/3520] TRAIN loss: 0.045\n",
      "[Iteration 2233/3520] TRAIN loss: 0.039\n",
      "[Iteration 2234/3520] TRAIN loss: 0.020\n",
      "[Iteration 2235/3520] TRAIN loss: 0.033\n",
      "[Iteration 2236/3520] TRAIN loss: 0.032\n",
      "[Iteration 2237/3520] TRAIN loss: 0.017\n",
      "[Iteration 2238/3520] TRAIN loss: 0.038\n",
      "[Iteration 2239/3520] TRAIN loss: 0.063\n",
      "[Iteration 2240/3520] TRAIN loss: 0.009\n",
      "[Iteration 2241/3520] TRAIN loss: 0.108\n",
      "[Iteration 2242/3520] TRAIN loss: 0.010\n",
      "[Iteration 2243/3520] TRAIN loss: 0.019\n",
      "[Iteration 2244/3520] TRAIN loss: 0.096\n",
      "[Iteration 2245/3520] TRAIN loss: 0.028\n",
      "[Iteration 2246/3520] TRAIN loss: 0.067\n",
      "[Iteration 2247/3520] TRAIN loss: 0.076\n",
      "[Iteration 2248/3520] TRAIN loss: 0.144\n",
      "[Iteration 2249/3520] TRAIN loss: 0.010\n",
      "[Iteration 2250/3520] TRAIN loss: 0.029\n",
      "[Iteration 2251/3520] TRAIN loss: 0.042\n",
      "[Iteration 2252/3520] TRAIN loss: 0.027\n",
      "[Iteration 2253/3520] TRAIN loss: 0.064\n",
      "[Iteration 2254/3520] TRAIN loss: 0.113\n",
      "[Iteration 2255/3520] TRAIN loss: 0.084\n",
      "[Iteration 2256/3520] TRAIN loss: 0.147\n",
      "[Iteration 2257/3520] TRAIN loss: 0.013\n",
      "[Iteration 2258/3520] TRAIN loss: 0.019\n",
      "[Iteration 2259/3520] TRAIN loss: 0.047\n",
      "[Iteration 2260/3520] TRAIN loss: 0.023\n",
      "[Iteration 2261/3520] TRAIN loss: 0.007\n",
      "[Iteration 2262/3520] TRAIN loss: 0.008\n",
      "[Iteration 2263/3520] TRAIN loss: 0.050\n",
      "[Iteration 2264/3520] TRAIN loss: 0.079\n",
      "[Iteration 2265/3520] TRAIN loss: 0.066\n",
      "[Iteration 2266/3520] TRAIN loss: 0.093\n",
      "[Iteration 2267/3520] TRAIN loss: 0.128\n",
      "[Iteration 2268/3520] TRAIN loss: 0.092\n",
      "[Iteration 2269/3520] TRAIN loss: 0.064\n",
      "[Iteration 2270/3520] TRAIN loss: 0.108\n",
      "[Iteration 2271/3520] TRAIN loss: 0.042\n",
      "[Iteration 2272/3520] TRAIN loss: 0.100\n",
      "[Iteration 2273/3520] TRAIN loss: 0.039\n",
      "[Iteration 2274/3520] TRAIN loss: 0.022\n",
      "[Iteration 2275/3520] TRAIN loss: 0.026\n",
      "[Iteration 2276/3520] TRAIN loss: 0.115\n",
      "[Iteration 2277/3520] TRAIN loss: 0.115\n",
      "[Iteration 2278/3520] TRAIN loss: 0.044\n",
      "[Iteration 2279/3520] TRAIN loss: 0.062\n",
      "[Iteration 2280/3520] TRAIN loss: 0.030\n",
      "[Iteration 2281/3520] TRAIN loss: 0.010\n",
      "[Iteration 2282/3520] TRAIN loss: 0.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 2283/3520] TRAIN loss: 0.081\n",
      "[Iteration 2284/3520] TRAIN loss: 0.090\n",
      "[Iteration 2285/3520] TRAIN loss: 0.013\n",
      "[Iteration 2286/3520] TRAIN loss: 0.024\n",
      "[Iteration 2287/3520] TRAIN loss: 0.044\n",
      "[Iteration 2288/3520] TRAIN loss: 0.015\n",
      "[Iteration 2289/3520] TRAIN loss: 0.090\n",
      "[Iteration 2290/3520] TRAIN loss: 0.017\n",
      "[Iteration 2291/3520] TRAIN loss: 0.025\n",
      "[Iteration 2292/3520] TRAIN loss: 0.094\n",
      "[Iteration 2293/3520] TRAIN loss: 0.007\n",
      "[Iteration 2294/3520] TRAIN loss: 0.050\n",
      "[Iteration 2295/3520] TRAIN loss: 0.122\n",
      "[Iteration 2296/3520] TRAIN loss: 0.102\n",
      "[Iteration 2297/3520] TRAIN loss: 0.140\n",
      "[Iteration 2298/3520] TRAIN loss: 0.042\n",
      "[Iteration 2299/3520] TRAIN loss: 0.019\n",
      "[Iteration 2300/3520] TRAIN loss: 0.022\n",
      "[Iteration 2301/3520] TRAIN loss: 0.025\n",
      "[Iteration 2302/3520] TRAIN loss: 0.078\n",
      "[Iteration 2303/3520] TRAIN loss: 0.053\n",
      "[Iteration 2304/3520] TRAIN loss: 0.049\n",
      "[Iteration 2305/3520] TRAIN loss: 0.067\n",
      "[Iteration 2306/3520] TRAIN loss: 0.088\n",
      "[Iteration 2307/3520] TRAIN loss: 0.114\n",
      "[Iteration 2308/3520] TRAIN loss: 0.028\n",
      "[Iteration 2309/3520] TRAIN loss: 0.013\n",
      "[Iteration 2310/3520] TRAIN loss: 0.066\n",
      "[Iteration 2311/3520] TRAIN loss: 0.048\n",
      "[Iteration 2312/3520] TRAIN loss: 0.018\n",
      "[Iteration 2313/3520] TRAIN loss: 0.115\n",
      "[Iteration 2314/3520] TRAIN loss: 0.030\n",
      "[Iteration 2315/3520] TRAIN loss: 0.040\n",
      "[Iteration 2316/3520] TRAIN loss: 0.082\n",
      "[Iteration 2317/3520] TRAIN loss: 0.027\n",
      "[Iteration 2318/3520] TRAIN loss: 0.033\n",
      "[Iteration 2319/3520] TRAIN loss: 0.031\n",
      "[Iteration 2320/3520] TRAIN loss: 0.024\n",
      "[Iteration 2321/3520] TRAIN loss: 0.041\n",
      "[Iteration 2322/3520] TRAIN loss: 0.040\n",
      "[Iteration 2323/3520] TRAIN loss: 0.087\n",
      "[Iteration 2324/3520] TRAIN loss: 0.057\n",
      "[Iteration 2325/3520] TRAIN loss: 0.035\n",
      "[Iteration 2326/3520] TRAIN loss: 0.096\n",
      "[Iteration 2327/3520] TRAIN loss: 0.094\n",
      "[Iteration 2328/3520] TRAIN loss: 0.082\n",
      "[Iteration 2329/3520] TRAIN loss: 0.078\n",
      "[Iteration 2330/3520] TRAIN loss: 0.024\n",
      "[Iteration 2331/3520] TRAIN loss: 0.033\n",
      "[Iteration 2332/3520] TRAIN loss: 0.049\n",
      "[Iteration 2333/3520] TRAIN loss: 0.111\n",
      "[Iteration 2334/3520] TRAIN loss: 0.050\n",
      "[Iteration 2335/3520] TRAIN loss: 0.017\n",
      "[Iteration 2336/3520] TRAIN loss: 0.072\n",
      "[Iteration 2337/3520] TRAIN loss: 0.102\n",
      "[Iteration 2338/3520] TRAIN loss: 0.082\n",
      "[Iteration 2339/3520] TRAIN loss: 0.057\n",
      "[Iteration 2340/3520] TRAIN loss: 0.063\n",
      "[Iteration 2341/3520] TRAIN loss: 0.158\n",
      "[Iteration 2342/3520] TRAIN loss: 0.050\n",
      "[Iteration 2343/3520] TRAIN loss: 0.106\n",
      "[Iteration 2344/3520] TRAIN loss: 0.048\n",
      "[Iteration 2345/3520] TRAIN loss: 0.038\n",
      "[Iteration 2346/3520] TRAIN loss: 0.063\n",
      "[Iteration 2347/3520] TRAIN loss: 0.101\n",
      "[Iteration 2348/3520] TRAIN loss: 0.026\n",
      "[Iteration 2349/3520] TRAIN loss: 0.055\n",
      "[Iteration 2350/3520] TRAIN loss: 0.023\n",
      "[Iteration 2351/3520] TRAIN loss: 0.063\n",
      "[Iteration 2352/3520] TRAIN loss: 0.045\n",
      "[Iteration 2353/3520] TRAIN loss: 0.014\n",
      "[Iteration 2354/3520] TRAIN loss: 0.018\n",
      "[Iteration 2355/3520] TRAIN loss: 0.010\n",
      "[Iteration 2356/3520] TRAIN loss: 0.045\n",
      "[Iteration 2357/3520] TRAIN loss: 0.034\n",
      "[Iteration 2358/3520] TRAIN loss: 0.103\n",
      "[Iteration 2359/3520] TRAIN loss: 0.056\n",
      "[Iteration 2360/3520] TRAIN loss: 0.049\n",
      "[Iteration 2361/3520] TRAIN loss: 0.042\n",
      "[Iteration 2362/3520] TRAIN loss: 0.018\n",
      "[Iteration 2363/3520] TRAIN loss: 0.008\n",
      "[Iteration 2364/3520] TRAIN loss: 0.145\n",
      "[Iteration 2365/3520] TRAIN loss: 0.065\n",
      "[Iteration 2366/3520] TRAIN loss: 0.080\n",
      "[Iteration 2367/3520] TRAIN loss: 0.035\n",
      "[Iteration 2368/3520] TRAIN loss: 0.067\n",
      "[Iteration 2369/3520] TRAIN loss: 0.039\n",
      "[Iteration 2370/3520] TRAIN loss: 0.050\n",
      "[Iteration 2371/3520] TRAIN loss: 0.102\n",
      "[Iteration 2372/3520] TRAIN loss: 0.052\n",
      "[Iteration 2373/3520] TRAIN loss: 0.058\n",
      "[Iteration 2374/3520] TRAIN loss: 0.041\n",
      "[Iteration 2375/3520] TRAIN loss: 0.012\n",
      "[Iteration 2376/3520] TRAIN loss: 0.048\n",
      "[Iteration 2377/3520] TRAIN loss: 0.063\n",
      "[Iteration 2378/3520] TRAIN loss: 0.105\n",
      "[Iteration 2379/3520] TRAIN loss: 0.037\n",
      "[Iteration 2380/3520] TRAIN loss: 0.026\n",
      "[Iteration 2381/3520] TRAIN loss: 0.057\n",
      "[Iteration 2382/3520] TRAIN loss: 0.098\n",
      "[Iteration 2383/3520] TRAIN loss: 0.074\n",
      "[Iteration 2384/3520] TRAIN loss: 0.041\n",
      "[Iteration 2385/3520] TRAIN loss: 0.025\n",
      "[Iteration 2386/3520] TRAIN loss: 0.067\n",
      "[Iteration 2387/3520] TRAIN loss: 0.068\n",
      "[Iteration 2388/3520] TRAIN loss: 0.053\n",
      "[Iteration 2389/3520] TRAIN loss: 0.120\n",
      "[Iteration 2390/3520] TRAIN loss: 0.111\n",
      "[Iteration 2391/3520] TRAIN loss: 0.073\n",
      "[Iteration 2392/3520] TRAIN loss: 0.046\n",
      "[Iteration 2393/3520] TRAIN loss: 0.063\n",
      "[Iteration 2394/3520] TRAIN loss: 0.029\n",
      "[Iteration 2395/3520] TRAIN loss: 0.035\n",
      "[Iteration 2396/3520] TRAIN loss: 0.040\n",
      "[Iteration 2397/3520] TRAIN loss: 0.059\n",
      "[Iteration 2398/3520] TRAIN loss: 0.129\n",
      "[Iteration 2399/3520] TRAIN loss: 0.111\n",
      "[Iteration 2400/3520] TRAIN loss: 0.086\n",
      "[Iteration 2401/3520] TRAIN loss: 0.040\n",
      "[Iteration 2402/3520] TRAIN loss: 0.009\n",
      "[Iteration 2403/3520] TRAIN loss: 0.064\n",
      "[Iteration 2404/3520] TRAIN loss: 0.059\n",
      "[Iteration 2405/3520] TRAIN loss: 0.107\n",
      "[Iteration 2406/3520] TRAIN loss: 0.010\n",
      "[Iteration 2407/3520] TRAIN loss: 0.090\n",
      "[Iteration 2408/3520] TRAIN loss: 0.048\n",
      "[Iteration 2409/3520] TRAIN loss: 0.011\n",
      "[Iteration 2410/3520] TRAIN loss: 0.018\n",
      "[Iteration 2411/3520] TRAIN loss: 0.068\n",
      "[Iteration 2412/3520] TRAIN loss: 0.019\n",
      "[Iteration 2413/3520] TRAIN loss: 0.005\n",
      "[Iteration 2414/3520] TRAIN loss: 0.045\n",
      "[Iteration 2415/3520] TRAIN loss: 0.016\n",
      "[Iteration 2416/3520] TRAIN loss: 0.131\n",
      "[Iteration 2417/3520] TRAIN loss: 0.080\n",
      "[Iteration 2418/3520] TRAIN loss: 0.014\n",
      "[Iteration 2419/3520] TRAIN loss: 0.018\n",
      "[Iteration 2420/3520] TRAIN loss: 0.064\n",
      "[Iteration 2421/3520] TRAIN loss: 0.049\n",
      "[Iteration 2422/3520] TRAIN loss: 0.009\n",
      "[Iteration 2423/3520] TRAIN loss: 0.013\n",
      "[Iteration 2424/3520] TRAIN loss: 0.044\n",
      "[Iteration 2425/3520] TRAIN loss: 0.031\n",
      "[Iteration 2426/3520] TRAIN loss: 0.091\n",
      "[Iteration 2427/3520] TRAIN loss: 0.025\n",
      "[Iteration 2428/3520] TRAIN loss: 0.025\n",
      "[Iteration 2429/3520] TRAIN loss: 0.022\n",
      "[Iteration 2430/3520] TRAIN loss: 0.026\n",
      "[Iteration 2431/3520] TRAIN loss: 0.056\n",
      "[Iteration 2432/3520] TRAIN loss: 0.130\n",
      "[Iteration 2433/3520] TRAIN loss: 0.055\n",
      "[Iteration 2434/3520] TRAIN loss: 0.075\n",
      "[Iteration 2435/3520] TRAIN loss: 0.071\n",
      "[Iteration 2436/3520] TRAIN loss: 0.008\n",
      "[Iteration 2437/3520] TRAIN loss: 0.022\n",
      "[Iteration 2438/3520] TRAIN loss: 0.073\n",
      "[Iteration 2439/3520] TRAIN loss: 0.046\n",
      "[Iteration 2440/3520] TRAIN loss: 0.148\n",
      "[Iteration 2441/3520] TRAIN loss: 0.030\n",
      "[Iteration 2442/3520] TRAIN loss: 0.052\n",
      "[Iteration 2443/3520] TRAIN loss: 0.036\n",
      "[Iteration 2444/3520] TRAIN loss: 0.028\n",
      "[Iteration 2445/3520] TRAIN loss: 0.041\n",
      "[Iteration 2446/3520] TRAIN loss: 0.010\n",
      "[Iteration 2447/3520] TRAIN loss: 0.007\n",
      "[Iteration 2448/3520] TRAIN loss: 0.053\n",
      "[Iteration 2449/3520] TRAIN loss: 0.061\n",
      "[Iteration 2450/3520] TRAIN loss: 0.010\n",
      "[Iteration 2451/3520] TRAIN loss: 0.130\n",
      "[Iteration 2452/3520] TRAIN loss: 0.113\n",
      "[Iteration 2453/3520] TRAIN loss: 0.017\n",
      "[Iteration 2454/3520] TRAIN loss: 0.091\n",
      "[Iteration 2455/3520] TRAIN loss: 0.051\n",
      "[Iteration 2456/3520] TRAIN loss: 0.124\n",
      "[Iteration 2457/3520] TRAIN loss: 0.045\n",
      "[Iteration 2458/3520] TRAIN loss: 0.100\n",
      "[Iteration 2459/3520] TRAIN loss: 0.079\n",
      "[Iteration 2460/3520] TRAIN loss: 0.148\n",
      "[Iteration 2461/3520] TRAIN loss: 0.149\n",
      "[Iteration 2462/3520] TRAIN loss: 0.058\n",
      "[Iteration 2463/3520] TRAIN loss: 0.046\n",
      "[Iteration 2464/3520] TRAIN loss: 0.072\n",
      "[Epoch 7/10] TRAIN acc/loss: 0.972/0.072\n",
      "[Epoch 7/10] VAL   acc/loss: 0.978/0.073\n",
      "[Iteration 2465/3520] TRAIN loss: 0.033\n",
      "[Iteration 2466/3520] TRAIN loss: 0.092\n",
      "[Iteration 2467/3520] TRAIN loss: 0.031\n",
      "[Iteration 2468/3520] TRAIN loss: 0.041\n",
      "[Iteration 2469/3520] TRAIN loss: 0.030\n",
      "[Iteration 2470/3520] TRAIN loss: 0.092\n",
      "[Iteration 2471/3520] TRAIN loss: 0.039\n",
      "[Iteration 2472/3520] TRAIN loss: 0.022\n",
      "[Iteration 2473/3520] TRAIN loss: 0.064\n",
      "[Iteration 2474/3520] TRAIN loss: 0.037\n",
      "[Iteration 2475/3520] TRAIN loss: 0.062\n",
      "[Iteration 2476/3520] TRAIN loss: 0.058\n",
      "[Iteration 2477/3520] TRAIN loss: 0.017\n",
      "[Iteration 2478/3520] TRAIN loss: 0.101\n",
      "[Iteration 2479/3520] TRAIN loss: 0.059\n",
      "[Iteration 2480/3520] TRAIN loss: 0.050\n",
      "[Iteration 2481/3520] TRAIN loss: 0.012\n",
      "[Iteration 2482/3520] TRAIN loss: 0.055\n",
      "[Iteration 2483/3520] TRAIN loss: 0.022\n",
      "[Iteration 2484/3520] TRAIN loss: 0.013\n",
      "[Iteration 2485/3520] TRAIN loss: 0.043\n",
      "[Iteration 2486/3520] TRAIN loss: 0.055\n",
      "[Iteration 2487/3520] TRAIN loss: 0.043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 2488/3520] TRAIN loss: 0.103\n",
      "[Iteration 2489/3520] TRAIN loss: 0.013\n",
      "[Iteration 2490/3520] TRAIN loss: 0.017\n",
      "[Iteration 2491/3520] TRAIN loss: 0.046\n",
      "[Iteration 2492/3520] TRAIN loss: 0.117\n",
      "[Iteration 2493/3520] TRAIN loss: 0.028\n",
      "[Iteration 2494/3520] TRAIN loss: 0.100\n",
      "[Iteration 2495/3520] TRAIN loss: 0.094\n",
      "[Iteration 2496/3520] TRAIN loss: 0.049\n",
      "[Iteration 2497/3520] TRAIN loss: 0.083\n",
      "[Iteration 2498/3520] TRAIN loss: 0.058\n",
      "[Iteration 2499/3520] TRAIN loss: 0.038\n",
      "[Iteration 2500/3520] TRAIN loss: 0.052\n",
      "[Iteration 2501/3520] TRAIN loss: 0.042\n",
      "[Iteration 2502/3520] TRAIN loss: 0.053\n",
      "[Iteration 2503/3520] TRAIN loss: 0.061\n",
      "[Iteration 2504/3520] TRAIN loss: 0.116\n",
      "[Iteration 2505/3520] TRAIN loss: 0.093\n",
      "[Iteration 2506/3520] TRAIN loss: 0.049\n",
      "[Iteration 2507/3520] TRAIN loss: 0.073\n",
      "[Iteration 2508/3520] TRAIN loss: 0.061\n",
      "[Iteration 2509/3520] TRAIN loss: 0.032\n",
      "[Iteration 2510/3520] TRAIN loss: 0.049\n",
      "[Iteration 2511/3520] TRAIN loss: 0.068\n",
      "[Iteration 2512/3520] TRAIN loss: 0.023\n",
      "[Iteration 2513/3520] TRAIN loss: 0.146\n",
      "[Iteration 2514/3520] TRAIN loss: 0.023\n",
      "[Iteration 2515/3520] TRAIN loss: 0.056\n",
      "[Iteration 2516/3520] TRAIN loss: 0.063\n",
      "[Iteration 2517/3520] TRAIN loss: 0.113\n",
      "[Iteration 2518/3520] TRAIN loss: 0.039\n",
      "[Iteration 2519/3520] TRAIN loss: 0.019\n",
      "[Iteration 2520/3520] TRAIN loss: 0.013\n",
      "[Iteration 2521/3520] TRAIN loss: 0.042\n",
      "[Iteration 2522/3520] TRAIN loss: 0.020\n",
      "[Iteration 2523/3520] TRAIN loss: 0.032\n",
      "[Iteration 2524/3520] TRAIN loss: 0.032\n",
      "[Iteration 2525/3520] TRAIN loss: 0.029\n",
      "[Iteration 2526/3520] TRAIN loss: 0.035\n",
      "[Iteration 2527/3520] TRAIN loss: 0.017\n",
      "[Iteration 2528/3520] TRAIN loss: 0.039\n",
      "[Iteration 2529/3520] TRAIN loss: 0.024\n",
      "[Iteration 2530/3520] TRAIN loss: 0.025\n",
      "[Iteration 2531/3520] TRAIN loss: 0.123\n",
      "[Iteration 2532/3520] TRAIN loss: 0.009\n",
      "[Iteration 2533/3520] TRAIN loss: 0.065\n",
      "[Iteration 2534/3520] TRAIN loss: 0.024\n",
      "[Iteration 2535/3520] TRAIN loss: 0.078\n",
      "[Iteration 2536/3520] TRAIN loss: 0.042\n",
      "[Iteration 2537/3520] TRAIN loss: 0.042\n",
      "[Iteration 2538/3520] TRAIN loss: 0.021\n",
      "[Iteration 2539/3520] TRAIN loss: 0.040\n",
      "[Iteration 2540/3520] TRAIN loss: 0.017\n",
      "[Iteration 2541/3520] TRAIN loss: 0.026\n",
      "[Iteration 2542/3520] TRAIN loss: 0.129\n",
      "[Iteration 2543/3520] TRAIN loss: 0.035\n",
      "[Iteration 2544/3520] TRAIN loss: 0.051\n",
      "[Iteration 2545/3520] TRAIN loss: 0.060\n",
      "[Iteration 2546/3520] TRAIN loss: 0.007\n",
      "[Iteration 2547/3520] TRAIN loss: 0.022\n",
      "[Iteration 2548/3520] TRAIN loss: 0.055\n",
      "[Iteration 2549/3520] TRAIN loss: 0.015\n",
      "[Iteration 2550/3520] TRAIN loss: 0.008\n",
      "[Iteration 2551/3520] TRAIN loss: 0.020\n",
      "[Iteration 2552/3520] TRAIN loss: 0.023\n",
      "[Iteration 2553/3520] TRAIN loss: 0.053\n",
      "[Iteration 2554/3520] TRAIN loss: 0.042\n",
      "[Iteration 2555/3520] TRAIN loss: 0.073\n",
      "[Iteration 2556/3520] TRAIN loss: 0.046\n",
      "[Iteration 2557/3520] TRAIN loss: 0.009\n",
      "[Iteration 2558/3520] TRAIN loss: 0.052\n",
      "[Iteration 2559/3520] TRAIN loss: 0.082\n",
      "[Iteration 2560/3520] TRAIN loss: 0.008\n",
      "[Iteration 2561/3520] TRAIN loss: 0.016\n",
      "[Iteration 2562/3520] TRAIN loss: 0.033\n",
      "[Iteration 2563/3520] TRAIN loss: 0.036\n",
      "[Iteration 2564/3520] TRAIN loss: 0.017\n",
      "[Iteration 2565/3520] TRAIN loss: 0.043\n",
      "[Iteration 2566/3520] TRAIN loss: 0.010\n",
      "[Iteration 2567/3520] TRAIN loss: 0.054\n",
      "[Iteration 2568/3520] TRAIN loss: 0.042\n",
      "[Iteration 2569/3520] TRAIN loss: 0.017\n",
      "[Iteration 2570/3520] TRAIN loss: 0.023\n",
      "[Iteration 2571/3520] TRAIN loss: 0.063\n",
      "[Iteration 2572/3520] TRAIN loss: 0.033\n",
      "[Iteration 2573/3520] TRAIN loss: 0.100\n",
      "[Iteration 2574/3520] TRAIN loss: 0.022\n",
      "[Iteration 2575/3520] TRAIN loss: 0.029\n",
      "[Iteration 2576/3520] TRAIN loss: 0.014\n",
      "[Iteration 2577/3520] TRAIN loss: 0.103\n",
      "[Iteration 2578/3520] TRAIN loss: 0.034\n",
      "[Iteration 2579/3520] TRAIN loss: 0.016\n",
      "[Iteration 2580/3520] TRAIN loss: 0.076\n",
      "[Iteration 2581/3520] TRAIN loss: 0.059\n",
      "[Iteration 2582/3520] TRAIN loss: 0.099\n",
      "[Iteration 2583/3520] TRAIN loss: 0.020\n",
      "[Iteration 2584/3520] TRAIN loss: 0.016\n",
      "[Iteration 2585/3520] TRAIN loss: 0.026\n",
      "[Iteration 2586/3520] TRAIN loss: 0.083\n",
      "[Iteration 2587/3520] TRAIN loss: 0.059\n",
      "[Iteration 2588/3520] TRAIN loss: 0.027\n",
      "[Iteration 2589/3520] TRAIN loss: 0.018\n",
      "[Iteration 2590/3520] TRAIN loss: 0.012\n",
      "[Iteration 2591/3520] TRAIN loss: 0.017\n",
      "[Iteration 2592/3520] TRAIN loss: 0.046\n",
      "[Iteration 2593/3520] TRAIN loss: 0.086\n",
      "[Iteration 2594/3520] TRAIN loss: 0.023\n",
      "[Iteration 2595/3520] TRAIN loss: 0.087\n",
      "[Iteration 2596/3520] TRAIN loss: 0.012\n",
      "[Iteration 2597/3520] TRAIN loss: 0.054\n",
      "[Iteration 2598/3520] TRAIN loss: 0.010\n",
      "[Iteration 2599/3520] TRAIN loss: 0.006\n",
      "[Iteration 2600/3520] TRAIN loss: 0.021\n",
      "[Iteration 2601/3520] TRAIN loss: 0.093\n",
      "[Iteration 2602/3520] TRAIN loss: 0.005\n",
      "[Iteration 2603/3520] TRAIN loss: 0.022\n",
      "[Iteration 2604/3520] TRAIN loss: 0.030\n",
      "[Iteration 2605/3520] TRAIN loss: 0.033\n",
      "[Iteration 2606/3520] TRAIN loss: 0.025\n",
      "[Iteration 2607/3520] TRAIN loss: 0.070\n",
      "[Iteration 2608/3520] TRAIN loss: 0.051\n",
      "[Iteration 2609/3520] TRAIN loss: 0.044\n",
      "[Iteration 2610/3520] TRAIN loss: 0.024\n",
      "[Iteration 2611/3520] TRAIN loss: 0.082\n",
      "[Iteration 2612/3520] TRAIN loss: 0.127\n",
      "[Iteration 2613/3520] TRAIN loss: 0.177\n",
      "[Iteration 2614/3520] TRAIN loss: 0.045\n",
      "[Iteration 2615/3520] TRAIN loss: 0.019\n",
      "[Iteration 2616/3520] TRAIN loss: 0.078\n",
      "[Iteration 2617/3520] TRAIN loss: 0.022\n",
      "[Iteration 2618/3520] TRAIN loss: 0.068\n",
      "[Iteration 2619/3520] TRAIN loss: 0.069\n",
      "[Iteration 2620/3520] TRAIN loss: 0.008\n",
      "[Iteration 2621/3520] TRAIN loss: 0.025\n",
      "[Iteration 2622/3520] TRAIN loss: 0.033\n",
      "[Iteration 2623/3520] TRAIN loss: 0.030\n",
      "[Iteration 2624/3520] TRAIN loss: 0.011\n",
      "[Iteration 2625/3520] TRAIN loss: 0.030\n",
      "[Iteration 2626/3520] TRAIN loss: 0.064\n",
      "[Iteration 2627/3520] TRAIN loss: 0.053\n",
      "[Iteration 2628/3520] TRAIN loss: 0.014\n",
      "[Iteration 2629/3520] TRAIN loss: 0.069\n",
      "[Iteration 2630/3520] TRAIN loss: 0.020\n",
      "[Iteration 2631/3520] TRAIN loss: 0.081\n",
      "[Iteration 2632/3520] TRAIN loss: 0.091\n",
      "[Iteration 2633/3520] TRAIN loss: 0.030\n",
      "[Iteration 2634/3520] TRAIN loss: 0.072\n",
      "[Iteration 2635/3520] TRAIN loss: 0.018\n",
      "[Iteration 2636/3520] TRAIN loss: 0.025\n",
      "[Iteration 2637/3520] TRAIN loss: 0.006\n",
      "[Iteration 2638/3520] TRAIN loss: 0.010\n",
      "[Iteration 2639/3520] TRAIN loss: 0.097\n",
      "[Iteration 2640/3520] TRAIN loss: 0.009\n",
      "[Iteration 2641/3520] TRAIN loss: 0.074\n",
      "[Iteration 2642/3520] TRAIN loss: 0.021\n",
      "[Iteration 2643/3520] TRAIN loss: 0.018\n",
      "[Iteration 2644/3520] TRAIN loss: 0.011\n",
      "[Iteration 2645/3520] TRAIN loss: 0.049\n",
      "[Iteration 2646/3520] TRAIN loss: 0.010\n",
      "[Iteration 2647/3520] TRAIN loss: 0.038\n",
      "[Iteration 2648/3520] TRAIN loss: 0.039\n",
      "[Iteration 2649/3520] TRAIN loss: 0.023\n",
      "[Iteration 2650/3520] TRAIN loss: 0.110\n",
      "[Iteration 2651/3520] TRAIN loss: 0.011\n",
      "[Iteration 2652/3520] TRAIN loss: 0.008\n",
      "[Iteration 2653/3520] TRAIN loss: 0.035\n",
      "[Iteration 2654/3520] TRAIN loss: 0.029\n",
      "[Iteration 2655/3520] TRAIN loss: 0.038\n",
      "[Iteration 2656/3520] TRAIN loss: 0.014\n",
      "[Iteration 2657/3520] TRAIN loss: 0.012\n",
      "[Iteration 2658/3520] TRAIN loss: 0.025\n",
      "[Iteration 2659/3520] TRAIN loss: 0.029\n",
      "[Iteration 2660/3520] TRAIN loss: 0.060\n",
      "[Iteration 2661/3520] TRAIN loss: 0.073\n",
      "[Iteration 2662/3520] TRAIN loss: 0.035\n",
      "[Iteration 2663/3520] TRAIN loss: 0.039\n",
      "[Iteration 2664/3520] TRAIN loss: 0.016\n",
      "[Iteration 2665/3520] TRAIN loss: 0.022\n",
      "[Iteration 2666/3520] TRAIN loss: 0.013\n",
      "[Iteration 2667/3520] TRAIN loss: 0.022\n",
      "[Iteration 2668/3520] TRAIN loss: 0.136\n",
      "[Iteration 2669/3520] TRAIN loss: 0.046\n",
      "[Iteration 2670/3520] TRAIN loss: 0.026\n",
      "[Iteration 2671/3520] TRAIN loss: 0.033\n",
      "[Iteration 2672/3520] TRAIN loss: 0.024\n",
      "[Iteration 2673/3520] TRAIN loss: 0.016\n",
      "[Iteration 2674/3520] TRAIN loss: 0.042\n",
      "[Iteration 2675/3520] TRAIN loss: 0.046\n",
      "[Iteration 2676/3520] TRAIN loss: 0.077\n",
      "[Iteration 2677/3520] TRAIN loss: 0.067\n",
      "[Iteration 2678/3520] TRAIN loss: 0.053\n",
      "[Iteration 2679/3520] TRAIN loss: 0.019\n",
      "[Iteration 2680/3520] TRAIN loss: 0.076\n",
      "[Iteration 2681/3520] TRAIN loss: 0.078\n",
      "[Iteration 2682/3520] TRAIN loss: 0.101\n",
      "[Iteration 2683/3520] TRAIN loss: 0.027\n",
      "[Iteration 2684/3520] TRAIN loss: 0.040\n",
      "[Iteration 2685/3520] TRAIN loss: 0.094\n",
      "[Iteration 2686/3520] TRAIN loss: 0.053\n",
      "[Iteration 2687/3520] TRAIN loss: 0.018\n",
      "[Iteration 2688/3520] TRAIN loss: 0.057\n",
      "[Iteration 2689/3520] TRAIN loss: 0.010\n",
      "[Iteration 2690/3520] TRAIN loss: 0.056\n",
      "[Iteration 2691/3520] TRAIN loss: 0.076\n",
      "[Iteration 2692/3520] TRAIN loss: 0.061\n",
      "[Iteration 2693/3520] TRAIN loss: 0.021\n",
      "[Iteration 2694/3520] TRAIN loss: 0.066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 2695/3520] TRAIN loss: 0.201\n",
      "[Iteration 2696/3520] TRAIN loss: 0.066\n",
      "[Iteration 2697/3520] TRAIN loss: 0.027\n",
      "[Iteration 2698/3520] TRAIN loss: 0.061\n",
      "[Iteration 2699/3520] TRAIN loss: 0.043\n",
      "[Iteration 2700/3520] TRAIN loss: 0.110\n",
      "[Iteration 2701/3520] TRAIN loss: 0.058\n",
      "[Iteration 2702/3520] TRAIN loss: 0.023\n",
      "[Iteration 2703/3520] TRAIN loss: 0.056\n",
      "[Iteration 2704/3520] TRAIN loss: 0.048\n",
      "[Iteration 2705/3520] TRAIN loss: 0.100\n",
      "[Iteration 2706/3520] TRAIN loss: 0.057\n",
      "[Iteration 2707/3520] TRAIN loss: 0.100\n",
      "[Iteration 2708/3520] TRAIN loss: 0.035\n",
      "[Iteration 2709/3520] TRAIN loss: 0.053\n",
      "[Iteration 2710/3520] TRAIN loss: 0.103\n",
      "[Iteration 2711/3520] TRAIN loss: 0.021\n",
      "[Iteration 2712/3520] TRAIN loss: 0.041\n",
      "[Iteration 2713/3520] TRAIN loss: 0.062\n",
      "[Iteration 2714/3520] TRAIN loss: 0.089\n",
      "[Iteration 2715/3520] TRAIN loss: 0.061\n",
      "[Iteration 2716/3520] TRAIN loss: 0.044\n",
      "[Iteration 2717/3520] TRAIN loss: 0.011\n",
      "[Iteration 2718/3520] TRAIN loss: 0.063\n",
      "[Iteration 2719/3520] TRAIN loss: 0.043\n",
      "[Iteration 2720/3520] TRAIN loss: 0.016\n",
      "[Iteration 2721/3520] TRAIN loss: 0.010\n",
      "[Iteration 2722/3520] TRAIN loss: 0.038\n",
      "[Iteration 2723/3520] TRAIN loss: 0.046\n",
      "[Iteration 2724/3520] TRAIN loss: 0.055\n",
      "[Iteration 2725/3520] TRAIN loss: 0.022\n",
      "[Iteration 2726/3520] TRAIN loss: 0.080\n",
      "[Iteration 2727/3520] TRAIN loss: 0.013\n",
      "[Iteration 2728/3520] TRAIN loss: 0.042\n",
      "[Iteration 2729/3520] TRAIN loss: 0.028\n",
      "[Iteration 2730/3520] TRAIN loss: 0.084\n",
      "[Iteration 2731/3520] TRAIN loss: 0.040\n",
      "[Iteration 2732/3520] TRAIN loss: 0.013\n",
      "[Iteration 2733/3520] TRAIN loss: 0.015\n",
      "[Iteration 2734/3520] TRAIN loss: 0.017\n",
      "[Iteration 2735/3520] TRAIN loss: 0.073\n",
      "[Iteration 2736/3520] TRAIN loss: 0.004\n",
      "[Iteration 2737/3520] TRAIN loss: 0.012\n",
      "[Iteration 2738/3520] TRAIN loss: 0.046\n",
      "[Iteration 2739/3520] TRAIN loss: 0.013\n",
      "[Iteration 2740/3520] TRAIN loss: 0.020\n",
      "[Iteration 2741/3520] TRAIN loss: 0.074\n",
      "[Iteration 2742/3520] TRAIN loss: 0.013\n",
      "[Iteration 2743/3520] TRAIN loss: 0.044\n",
      "[Iteration 2744/3520] TRAIN loss: 0.008\n",
      "[Iteration 2745/3520] TRAIN loss: 0.092\n",
      "[Iteration 2746/3520] TRAIN loss: 0.075\n",
      "[Iteration 2747/3520] TRAIN loss: 0.037\n",
      "[Iteration 2748/3520] TRAIN loss: 0.058\n",
      "[Iteration 2749/3520] TRAIN loss: 0.009\n",
      "[Iteration 2750/3520] TRAIN loss: 0.050\n",
      "[Iteration 2751/3520] TRAIN loss: 0.019\n",
      "[Iteration 2752/3520] TRAIN loss: 0.014\n",
      "[Iteration 2753/3520] TRAIN loss: 0.035\n",
      "[Iteration 2754/3520] TRAIN loss: 0.020\n",
      "[Iteration 2755/3520] TRAIN loss: 0.021\n",
      "[Iteration 2756/3520] TRAIN loss: 0.023\n",
      "[Iteration 2757/3520] TRAIN loss: 0.011\n",
      "[Iteration 2758/3520] TRAIN loss: 0.025\n",
      "[Iteration 2759/3520] TRAIN loss: 0.049\n",
      "[Iteration 2760/3520] TRAIN loss: 0.020\n",
      "[Iteration 2761/3520] TRAIN loss: 0.052\n",
      "[Iteration 2762/3520] TRAIN loss: 0.035\n",
      "[Iteration 2763/3520] TRAIN loss: 0.041\n",
      "[Iteration 2764/3520] TRAIN loss: 0.055\n",
      "[Iteration 2765/3520] TRAIN loss: 0.020\n",
      "[Iteration 2766/3520] TRAIN loss: 0.087\n",
      "[Iteration 2767/3520] TRAIN loss: 0.019\n",
      "[Iteration 2768/3520] TRAIN loss: 0.046\n",
      "[Iteration 2769/3520] TRAIN loss: 0.036\n",
      "[Iteration 2770/3520] TRAIN loss: 0.089\n",
      "[Iteration 2771/3520] TRAIN loss: 0.073\n",
      "[Iteration 2772/3520] TRAIN loss: 0.036\n",
      "[Iteration 2773/3520] TRAIN loss: 0.028\n",
      "[Iteration 2774/3520] TRAIN loss: 0.018\n",
      "[Iteration 2775/3520] TRAIN loss: 0.071\n",
      "[Iteration 2776/3520] TRAIN loss: 0.060\n",
      "[Iteration 2777/3520] TRAIN loss: 0.033\n",
      "[Iteration 2778/3520] TRAIN loss: 0.009\n",
      "[Iteration 2779/3520] TRAIN loss: 0.029\n",
      "[Iteration 2780/3520] TRAIN loss: 0.035\n",
      "[Iteration 2781/3520] TRAIN loss: 0.067\n",
      "[Iteration 2782/3520] TRAIN loss: 0.004\n",
      "[Iteration 2783/3520] TRAIN loss: 0.034\n",
      "[Iteration 2784/3520] TRAIN loss: 0.028\n",
      "[Iteration 2785/3520] TRAIN loss: 0.024\n",
      "[Iteration 2786/3520] TRAIN loss: 0.099\n",
      "[Iteration 2787/3520] TRAIN loss: 0.017\n",
      "[Iteration 2788/3520] TRAIN loss: 0.008\n",
      "[Iteration 2789/3520] TRAIN loss: 0.050\n",
      "[Iteration 2790/3520] TRAIN loss: 0.057\n",
      "[Iteration 2791/3520] TRAIN loss: 0.023\n",
      "[Iteration 2792/3520] TRAIN loss: 0.017\n",
      "[Iteration 2793/3520] TRAIN loss: 0.006\n",
      "[Iteration 2794/3520] TRAIN loss: 0.030\n",
      "[Iteration 2795/3520] TRAIN loss: 0.042\n",
      "[Iteration 2796/3520] TRAIN loss: 0.007\n",
      "[Iteration 2797/3520] TRAIN loss: 0.063\n",
      "[Iteration 2798/3520] TRAIN loss: 0.017\n",
      "[Iteration 2799/3520] TRAIN loss: 0.010\n",
      "[Iteration 2800/3520] TRAIN loss: 0.038\n",
      "[Iteration 2801/3520] TRAIN loss: 0.003\n",
      "[Iteration 2802/3520] TRAIN loss: 0.007\n",
      "[Iteration 2803/3520] TRAIN loss: 0.039\n",
      "[Iteration 2804/3520] TRAIN loss: 0.016\n",
      "[Iteration 2805/3520] TRAIN loss: 0.045\n",
      "[Iteration 2806/3520] TRAIN loss: 0.057\n",
      "[Iteration 2807/3520] TRAIN loss: 0.011\n",
      "[Iteration 2808/3520] TRAIN loss: 0.046\n",
      "[Iteration 2809/3520] TRAIN loss: 0.021\n",
      "[Iteration 2810/3520] TRAIN loss: 0.016\n",
      "[Iteration 2811/3520] TRAIN loss: 0.014\n",
      "[Iteration 2812/3520] TRAIN loss: 0.042\n",
      "[Iteration 2813/3520] TRAIN loss: 0.081\n",
      "[Iteration 2814/3520] TRAIN loss: 0.011\n",
      "[Iteration 2815/3520] TRAIN loss: 0.038\n",
      "[Iteration 2816/3520] TRAIN loss: 0.066\n",
      "[Epoch 8/10] TRAIN acc/loss: 0.972/0.066\n",
      "[Epoch 8/10] VAL   acc/loss: 0.981/0.063\n",
      "[Iteration 2817/3520] TRAIN loss: 0.006\n",
      "[Iteration 2818/3520] TRAIN loss: 0.019\n",
      "[Iteration 2819/3520] TRAIN loss: 0.018\n",
      "[Iteration 2820/3520] TRAIN loss: 0.007\n",
      "[Iteration 2821/3520] TRAIN loss: 0.014\n",
      "[Iteration 2822/3520] TRAIN loss: 0.019\n",
      "[Iteration 2823/3520] TRAIN loss: 0.009\n",
      "[Iteration 2824/3520] TRAIN loss: 0.085\n",
      "[Iteration 2825/3520] TRAIN loss: 0.036\n",
      "[Iteration 2826/3520] TRAIN loss: 0.026\n",
      "[Iteration 2827/3520] TRAIN loss: 0.013\n",
      "[Iteration 2828/3520] TRAIN loss: 0.016\n",
      "[Iteration 2829/3520] TRAIN loss: 0.014\n",
      "[Iteration 2830/3520] TRAIN loss: 0.019\n",
      "[Iteration 2831/3520] TRAIN loss: 0.014\n",
      "[Iteration 2832/3520] TRAIN loss: 0.036\n",
      "[Iteration 2833/3520] TRAIN loss: 0.012\n",
      "[Iteration 2834/3520] TRAIN loss: 0.116\n",
      "[Iteration 2835/3520] TRAIN loss: 0.011\n",
      "[Iteration 2836/3520] TRAIN loss: 0.021\n",
      "[Iteration 2837/3520] TRAIN loss: 0.067\n",
      "[Iteration 2838/3520] TRAIN loss: 0.027\n",
      "[Iteration 2839/3520] TRAIN loss: 0.036\n",
      "[Iteration 2840/3520] TRAIN loss: 0.023\n",
      "[Iteration 2841/3520] TRAIN loss: 0.056\n",
      "[Iteration 2842/3520] TRAIN loss: 0.023\n",
      "[Iteration 2843/3520] TRAIN loss: 0.027\n",
      "[Iteration 2844/3520] TRAIN loss: 0.007\n",
      "[Iteration 2845/3520] TRAIN loss: 0.009\n",
      "[Iteration 2846/3520] TRAIN loss: 0.032\n",
      "[Iteration 2847/3520] TRAIN loss: 0.034\n",
      "[Iteration 2848/3520] TRAIN loss: 0.013\n",
      "[Iteration 2849/3520] TRAIN loss: 0.053\n",
      "[Iteration 2850/3520] TRAIN loss: 0.093\n",
      "[Iteration 2851/3520] TRAIN loss: 0.008\n",
      "[Iteration 2852/3520] TRAIN loss: 0.011\n",
      "[Iteration 2853/3520] TRAIN loss: 0.038\n",
      "[Iteration 2854/3520] TRAIN loss: 0.052\n",
      "[Iteration 2855/3520] TRAIN loss: 0.013\n",
      "[Iteration 2856/3520] TRAIN loss: 0.016\n",
      "[Iteration 2857/3520] TRAIN loss: 0.001\n",
      "[Iteration 2858/3520] TRAIN loss: 0.020\n",
      "[Iteration 2859/3520] TRAIN loss: 0.052\n",
      "[Iteration 2860/3520] TRAIN loss: 0.006\n",
      "[Iteration 2861/3520] TRAIN loss: 0.079\n",
      "[Iteration 2862/3520] TRAIN loss: 0.047\n",
      "[Iteration 2863/3520] TRAIN loss: 0.040\n",
      "[Iteration 2864/3520] TRAIN loss: 0.057\n",
      "[Iteration 2865/3520] TRAIN loss: 0.021\n",
      "[Iteration 2866/3520] TRAIN loss: 0.007\n",
      "[Iteration 2867/3520] TRAIN loss: 0.012\n",
      "[Iteration 2868/3520] TRAIN loss: 0.030\n",
      "[Iteration 2869/3520] TRAIN loss: 0.077\n",
      "[Iteration 2870/3520] TRAIN loss: 0.011\n",
      "[Iteration 2871/3520] TRAIN loss: 0.053\n",
      "[Iteration 2872/3520] TRAIN loss: 0.076\n",
      "[Iteration 2873/3520] TRAIN loss: 0.119\n",
      "[Iteration 2874/3520] TRAIN loss: 0.100\n",
      "[Iteration 2875/3520] TRAIN loss: 0.048\n",
      "[Iteration 2876/3520] TRAIN loss: 0.064\n",
      "[Iteration 2877/3520] TRAIN loss: 0.009\n",
      "[Iteration 2878/3520] TRAIN loss: 0.013\n",
      "[Iteration 2879/3520] TRAIN loss: 0.048\n",
      "[Iteration 2880/3520] TRAIN loss: 0.045\n",
      "[Iteration 2881/3520] TRAIN loss: 0.030\n",
      "[Iteration 2882/3520] TRAIN loss: 0.042\n",
      "[Iteration 2883/3520] TRAIN loss: 0.034\n",
      "[Iteration 2884/3520] TRAIN loss: 0.066\n",
      "[Iteration 2885/3520] TRAIN loss: 0.133\n",
      "[Iteration 2886/3520] TRAIN loss: 0.017\n",
      "[Iteration 2887/3520] TRAIN loss: 0.036\n",
      "[Iteration 2888/3520] TRAIN loss: 0.070\n",
      "[Iteration 2889/3520] TRAIN loss: 0.038\n",
      "[Iteration 2890/3520] TRAIN loss: 0.062\n",
      "[Iteration 2891/3520] TRAIN loss: 0.065\n",
      "[Iteration 2892/3520] TRAIN loss: 0.013\n",
      "[Iteration 2893/3520] TRAIN loss: 0.143\n",
      "[Iteration 2894/3520] TRAIN loss: 0.073\n",
      "[Iteration 2895/3520] TRAIN loss: 0.076\n",
      "[Iteration 2896/3520] TRAIN loss: 0.051\n",
      "[Iteration 2897/3520] TRAIN loss: 0.030\n",
      "[Iteration 2898/3520] TRAIN loss: 0.079\n",
      "[Iteration 2899/3520] TRAIN loss: 0.059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 2900/3520] TRAIN loss: 0.028\n",
      "[Iteration 2901/3520] TRAIN loss: 0.074\n",
      "[Iteration 2902/3520] TRAIN loss: 0.026\n",
      "[Iteration 2903/3520] TRAIN loss: 0.007\n",
      "[Iteration 2904/3520] TRAIN loss: 0.034\n",
      "[Iteration 2905/3520] TRAIN loss: 0.009\n",
      "[Iteration 2906/3520] TRAIN loss: 0.010\n",
      "[Iteration 2907/3520] TRAIN loss: 0.050\n",
      "[Iteration 2908/3520] TRAIN loss: 0.089\n",
      "[Iteration 2909/3520] TRAIN loss: 0.039\n",
      "[Iteration 2910/3520] TRAIN loss: 0.066\n",
      "[Iteration 2911/3520] TRAIN loss: 0.040\n",
      "[Iteration 2912/3520] TRAIN loss: 0.025\n",
      "[Iteration 2913/3520] TRAIN loss: 0.011\n",
      "[Iteration 2914/3520] TRAIN loss: 0.038\n",
      "[Iteration 2915/3520] TRAIN loss: 0.052\n",
      "[Iteration 2916/3520] TRAIN loss: 0.022\n",
      "[Iteration 2917/3520] TRAIN loss: 0.034\n",
      "[Iteration 2918/3520] TRAIN loss: 0.022\n",
      "[Iteration 2919/3520] TRAIN loss: 0.084\n",
      "[Iteration 2920/3520] TRAIN loss: 0.090\n",
      "[Iteration 2921/3520] TRAIN loss: 0.027\n",
      "[Iteration 2922/3520] TRAIN loss: 0.079\n",
      "[Iteration 2923/3520] TRAIN loss: 0.099\n",
      "[Iteration 2924/3520] TRAIN loss: 0.028\n",
      "[Iteration 2925/3520] TRAIN loss: 0.035\n",
      "[Iteration 2926/3520] TRAIN loss: 0.032\n",
      "[Iteration 2927/3520] TRAIN loss: 0.018\n",
      "[Iteration 2928/3520] TRAIN loss: 0.021\n",
      "[Iteration 2929/3520] TRAIN loss: 0.009\n",
      "[Iteration 2930/3520] TRAIN loss: 0.018\n",
      "[Iteration 2931/3520] TRAIN loss: 0.059\n",
      "[Iteration 2932/3520] TRAIN loss: 0.023\n",
      "[Iteration 2933/3520] TRAIN loss: 0.030\n",
      "[Iteration 2934/3520] TRAIN loss: 0.055\n",
      "[Iteration 2935/3520] TRAIN loss: 0.034\n",
      "[Iteration 2936/3520] TRAIN loss: 0.081\n",
      "[Iteration 2937/3520] TRAIN loss: 0.074\n",
      "[Iteration 2938/3520] TRAIN loss: 0.022\n",
      "[Iteration 2939/3520] TRAIN loss: 0.010\n",
      "[Iteration 2940/3520] TRAIN loss: 0.014\n",
      "[Iteration 2941/3520] TRAIN loss: 0.021\n",
      "[Iteration 2942/3520] TRAIN loss: 0.054\n",
      "[Iteration 2943/3520] TRAIN loss: 0.051\n",
      "[Iteration 2944/3520] TRAIN loss: 0.032\n",
      "[Iteration 2945/3520] TRAIN loss: 0.043\n",
      "[Iteration 2946/3520] TRAIN loss: 0.013\n",
      "[Iteration 2947/3520] TRAIN loss: 0.121\n",
      "[Iteration 2948/3520] TRAIN loss: 0.075\n",
      "[Iteration 2949/3520] TRAIN loss: 0.037\n",
      "[Iteration 2950/3520] TRAIN loss: 0.019\n",
      "[Iteration 2951/3520] TRAIN loss: 0.091\n",
      "[Iteration 2952/3520] TRAIN loss: 0.049\n",
      "[Iteration 2953/3520] TRAIN loss: 0.043\n",
      "[Iteration 2954/3520] TRAIN loss: 0.062\n",
      "[Iteration 2955/3520] TRAIN loss: 0.021\n",
      "[Iteration 2956/3520] TRAIN loss: 0.024\n",
      "[Iteration 2957/3520] TRAIN loss: 0.011\n",
      "[Iteration 2958/3520] TRAIN loss: 0.051\n",
      "[Iteration 2959/3520] TRAIN loss: 0.064\n",
      "[Iteration 2960/3520] TRAIN loss: 0.015\n",
      "[Iteration 2961/3520] TRAIN loss: 0.014\n",
      "[Iteration 2962/3520] TRAIN loss: 0.069\n",
      "[Iteration 2963/3520] TRAIN loss: 0.031\n",
      "[Iteration 2964/3520] TRAIN loss: 0.024\n",
      "[Iteration 2965/3520] TRAIN loss: 0.021\n",
      "[Iteration 2966/3520] TRAIN loss: 0.032\n",
      "[Iteration 2967/3520] TRAIN loss: 0.019\n",
      "[Iteration 2968/3520] TRAIN loss: 0.019\n",
      "[Iteration 2969/3520] TRAIN loss: 0.045\n",
      "[Iteration 2970/3520] TRAIN loss: 0.028\n",
      "[Iteration 2971/3520] TRAIN loss: 0.043\n",
      "[Iteration 2972/3520] TRAIN loss: 0.022\n",
      "[Iteration 2973/3520] TRAIN loss: 0.206\n",
      "[Iteration 2974/3520] TRAIN loss: 0.025\n",
      "[Iteration 2975/3520] TRAIN loss: 0.083\n",
      "[Iteration 2976/3520] TRAIN loss: 0.042\n",
      "[Iteration 2977/3520] TRAIN loss: 0.031\n",
      "[Iteration 2978/3520] TRAIN loss: 0.053\n",
      "[Iteration 2979/3520] TRAIN loss: 0.025\n",
      "[Iteration 2980/3520] TRAIN loss: 0.038\n",
      "[Iteration 2981/3520] TRAIN loss: 0.038\n",
      "[Iteration 2982/3520] TRAIN loss: 0.067\n",
      "[Iteration 2983/3520] TRAIN loss: 0.024\n",
      "[Iteration 2984/3520] TRAIN loss: 0.031\n",
      "[Iteration 2985/3520] TRAIN loss: 0.013\n",
      "[Iteration 2986/3520] TRAIN loss: 0.008\n",
      "[Iteration 2987/3520] TRAIN loss: 0.056\n",
      "[Iteration 2988/3520] TRAIN loss: 0.087\n",
      "[Iteration 2989/3520] TRAIN loss: 0.076\n",
      "[Iteration 2990/3520] TRAIN loss: 0.030\n",
      "[Iteration 2991/3520] TRAIN loss: 0.066\n",
      "[Iteration 2992/3520] TRAIN loss: 0.081\n",
      "[Iteration 2993/3520] TRAIN loss: 0.071\n",
      "[Iteration 2994/3520] TRAIN loss: 0.078\n",
      "[Iteration 2995/3520] TRAIN loss: 0.060\n",
      "[Iteration 2996/3520] TRAIN loss: 0.025\n",
      "[Iteration 2997/3520] TRAIN loss: 0.003\n",
      "[Iteration 2998/3520] TRAIN loss: 0.015\n",
      "[Iteration 2999/3520] TRAIN loss: 0.084\n",
      "[Iteration 3000/3520] TRAIN loss: 0.012\n",
      "[Iteration 3001/3520] TRAIN loss: 0.038\n",
      "[Iteration 3002/3520] TRAIN loss: 0.006\n",
      "[Iteration 3003/3520] TRAIN loss: 0.057\n",
      "[Iteration 3004/3520] TRAIN loss: 0.088\n",
      "[Iteration 3005/3520] TRAIN loss: 0.012\n",
      "[Iteration 3006/3520] TRAIN loss: 0.035\n",
      "[Iteration 3007/3520] TRAIN loss: 0.006\n",
      "[Iteration 3008/3520] TRAIN loss: 0.013\n",
      "[Iteration 3009/3520] TRAIN loss: 0.034\n",
      "[Iteration 3010/3520] TRAIN loss: 0.031\n",
      "[Iteration 3011/3520] TRAIN loss: 0.048\n",
      "[Iteration 3012/3520] TRAIN loss: 0.028\n",
      "[Iteration 3013/3520] TRAIN loss: 0.076\n",
      "[Iteration 3014/3520] TRAIN loss: 0.021\n",
      "[Iteration 3015/3520] TRAIN loss: 0.014\n",
      "[Iteration 3016/3520] TRAIN loss: 0.025\n",
      "[Iteration 3017/3520] TRAIN loss: 0.007\n",
      "[Iteration 3018/3520] TRAIN loss: 0.067\n",
      "[Iteration 3019/3520] TRAIN loss: 0.005\n",
      "[Iteration 3020/3520] TRAIN loss: 0.045\n",
      "[Iteration 3021/3520] TRAIN loss: 0.048\n",
      "[Iteration 3022/3520] TRAIN loss: 0.039\n",
      "[Iteration 3023/3520] TRAIN loss: 0.034\n",
      "[Iteration 3024/3520] TRAIN loss: 0.090\n",
      "[Iteration 3025/3520] TRAIN loss: 0.064\n",
      "[Iteration 3026/3520] TRAIN loss: 0.071\n",
      "[Iteration 3027/3520] TRAIN loss: 0.034\n",
      "[Iteration 3028/3520] TRAIN loss: 0.051\n",
      "[Iteration 3029/3520] TRAIN loss: 0.068\n",
      "[Iteration 3030/3520] TRAIN loss: 0.023\n",
      "[Iteration 3031/3520] TRAIN loss: 0.016\n",
      "[Iteration 3032/3520] TRAIN loss: 0.039\n",
      "[Iteration 3033/3520] TRAIN loss: 0.032\n",
      "[Iteration 3034/3520] TRAIN loss: 0.045\n",
      "[Iteration 3035/3520] TRAIN loss: 0.046\n",
      "[Iteration 3036/3520] TRAIN loss: 0.021\n",
      "[Iteration 3037/3520] TRAIN loss: 0.080\n",
      "[Iteration 3038/3520] TRAIN loss: 0.022\n",
      "[Iteration 3039/3520] TRAIN loss: 0.050\n",
      "[Iteration 3040/3520] TRAIN loss: 0.045\n",
      "[Iteration 3041/3520] TRAIN loss: 0.063\n",
      "[Iteration 3042/3520] TRAIN loss: 0.032\n",
      "[Iteration 3043/3520] TRAIN loss: 0.008\n",
      "[Iteration 3044/3520] TRAIN loss: 0.034\n",
      "[Iteration 3045/3520] TRAIN loss: 0.022\n",
      "[Iteration 3046/3520] TRAIN loss: 0.080\n",
      "[Iteration 3047/3520] TRAIN loss: 0.034\n",
      "[Iteration 3048/3520] TRAIN loss: 0.015\n",
      "[Iteration 3049/3520] TRAIN loss: 0.079\n",
      "[Iteration 3050/3520] TRAIN loss: 0.083\n",
      "[Iteration 3051/3520] TRAIN loss: 0.048\n",
      "[Iteration 3052/3520] TRAIN loss: 0.026\n",
      "[Iteration 3053/3520] TRAIN loss: 0.079\n",
      "[Iteration 3054/3520] TRAIN loss: 0.006\n",
      "[Iteration 3055/3520] TRAIN loss: 0.024\n",
      "[Iteration 3056/3520] TRAIN loss: 0.013\n",
      "[Iteration 3057/3520] TRAIN loss: 0.037\n",
      "[Iteration 3058/3520] TRAIN loss: 0.032\n",
      "[Iteration 3059/3520] TRAIN loss: 0.006\n",
      "[Iteration 3060/3520] TRAIN loss: 0.115\n",
      "[Iteration 3061/3520] TRAIN loss: 0.036\n",
      "[Iteration 3062/3520] TRAIN loss: 0.101\n",
      "[Iteration 3063/3520] TRAIN loss: 0.019\n",
      "[Iteration 3064/3520] TRAIN loss: 0.053\n",
      "[Iteration 3065/3520] TRAIN loss: 0.032\n",
      "[Iteration 3066/3520] TRAIN loss: 0.024\n",
      "[Iteration 3067/3520] TRAIN loss: 0.032\n",
      "[Iteration 3068/3520] TRAIN loss: 0.007\n",
      "[Iteration 3069/3520] TRAIN loss: 0.012\n",
      "[Iteration 3070/3520] TRAIN loss: 0.019\n",
      "[Iteration 3071/3520] TRAIN loss: 0.031\n",
      "[Iteration 3072/3520] TRAIN loss: 0.035\n",
      "[Iteration 3073/3520] TRAIN loss: 0.027\n",
      "[Iteration 3074/3520] TRAIN loss: 0.013\n",
      "[Iteration 3075/3520] TRAIN loss: 0.015\n",
      "[Iteration 3076/3520] TRAIN loss: 0.039\n",
      "[Iteration 3077/3520] TRAIN loss: 0.019\n",
      "[Iteration 3078/3520] TRAIN loss: 0.038\n",
      "[Iteration 3079/3520] TRAIN loss: 0.034\n",
      "[Iteration 3080/3520] TRAIN loss: 0.047\n",
      "[Iteration 3081/3520] TRAIN loss: 0.037\n",
      "[Iteration 3082/3520] TRAIN loss: 0.089\n",
      "[Iteration 3083/3520] TRAIN loss: 0.029\n",
      "[Iteration 3084/3520] TRAIN loss: 0.059\n",
      "[Iteration 3085/3520] TRAIN loss: 0.058\n",
      "[Iteration 3086/3520] TRAIN loss: 0.031\n",
      "[Iteration 3087/3520] TRAIN loss: 0.005\n",
      "[Iteration 3088/3520] TRAIN loss: 0.025\n",
      "[Iteration 3089/3520] TRAIN loss: 0.095\n",
      "[Iteration 3090/3520] TRAIN loss: 0.114\n",
      "[Iteration 3091/3520] TRAIN loss: 0.009\n",
      "[Iteration 3092/3520] TRAIN loss: 0.005\n",
      "[Iteration 3093/3520] TRAIN loss: 0.045\n",
      "[Iteration 3094/3520] TRAIN loss: 0.053\n",
      "[Iteration 3095/3520] TRAIN loss: 0.104\n",
      "[Iteration 3096/3520] TRAIN loss: 0.027\n",
      "[Iteration 3097/3520] TRAIN loss: 0.009\n",
      "[Iteration 3098/3520] TRAIN loss: 0.105\n",
      "[Iteration 3099/3520] TRAIN loss: 0.007\n",
      "[Iteration 3100/3520] TRAIN loss: 0.036\n",
      "[Iteration 3101/3520] TRAIN loss: 0.042\n",
      "[Iteration 3102/3520] TRAIN loss: 0.037\n",
      "[Iteration 3103/3520] TRAIN loss: 0.029\n",
      "[Iteration 3104/3520] TRAIN loss: 0.027\n",
      "[Iteration 3105/3520] TRAIN loss: 0.107\n",
      "[Iteration 3106/3520] TRAIN loss: 0.060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 3107/3520] TRAIN loss: 0.019\n",
      "[Iteration 3108/3520] TRAIN loss: 0.016\n",
      "[Iteration 3109/3520] TRAIN loss: 0.043\n",
      "[Iteration 3110/3520] TRAIN loss: 0.042\n",
      "[Iteration 3111/3520] TRAIN loss: 0.044\n",
      "[Iteration 3112/3520] TRAIN loss: 0.056\n",
      "[Iteration 3113/3520] TRAIN loss: 0.005\n",
      "[Iteration 3114/3520] TRAIN loss: 0.097\n",
      "[Iteration 3115/3520] TRAIN loss: 0.005\n",
      "[Iteration 3116/3520] TRAIN loss: 0.014\n",
      "[Iteration 3117/3520] TRAIN loss: 0.103\n",
      "[Iteration 3118/3520] TRAIN loss: 0.020\n",
      "[Iteration 3119/3520] TRAIN loss: 0.039\n",
      "[Iteration 3120/3520] TRAIN loss: 0.045\n",
      "[Iteration 3121/3520] TRAIN loss: 0.021\n",
      "[Iteration 3122/3520] TRAIN loss: 0.016\n",
      "[Iteration 3123/3520] TRAIN loss: 0.037\n",
      "[Iteration 3124/3520] TRAIN loss: 0.042\n",
      "[Iteration 3125/3520] TRAIN loss: 0.100\n",
      "[Iteration 3126/3520] TRAIN loss: 0.084\n",
      "[Iteration 3127/3520] TRAIN loss: 0.059\n",
      "[Iteration 3128/3520] TRAIN loss: 0.025\n",
      "[Iteration 3129/3520] TRAIN loss: 0.092\n",
      "[Iteration 3130/3520] TRAIN loss: 0.041\n",
      "[Iteration 3131/3520] TRAIN loss: 0.051\n",
      "[Iteration 3132/3520] TRAIN loss: 0.077\n",
      "[Iteration 3133/3520] TRAIN loss: 0.097\n",
      "[Iteration 3134/3520] TRAIN loss: 0.025\n",
      "[Iteration 3135/3520] TRAIN loss: 0.034\n",
      "[Iteration 3136/3520] TRAIN loss: 0.017\n",
      "[Iteration 3137/3520] TRAIN loss: 0.042\n",
      "[Iteration 3138/3520] TRAIN loss: 0.037\n",
      "[Iteration 3139/3520] TRAIN loss: 0.059\n",
      "[Iteration 3140/3520] TRAIN loss: 0.096\n",
      "[Iteration 3141/3520] TRAIN loss: 0.044\n",
      "[Iteration 3142/3520] TRAIN loss: 0.069\n",
      "[Iteration 3143/3520] TRAIN loss: 0.006\n",
      "[Iteration 3144/3520] TRAIN loss: 0.078\n",
      "[Iteration 3145/3520] TRAIN loss: 0.058\n",
      "[Iteration 3146/3520] TRAIN loss: 0.021\n",
      "[Iteration 3147/3520] TRAIN loss: 0.031\n",
      "[Iteration 3148/3520] TRAIN loss: 0.041\n",
      "[Iteration 3149/3520] TRAIN loss: 0.142\n",
      "[Iteration 3150/3520] TRAIN loss: 0.006\n",
      "[Iteration 3151/3520] TRAIN loss: 0.066\n",
      "[Iteration 3152/3520] TRAIN loss: 0.052\n",
      "[Iteration 3153/3520] TRAIN loss: 0.037\n",
      "[Iteration 3154/3520] TRAIN loss: 0.037\n",
      "[Iteration 3155/3520] TRAIN loss: 0.011\n",
      "[Iteration 3156/3520] TRAIN loss: 0.034\n",
      "[Iteration 3157/3520] TRAIN loss: 0.069\n",
      "[Iteration 3158/3520] TRAIN loss: 0.026\n",
      "[Iteration 3159/3520] TRAIN loss: 0.017\n",
      "[Iteration 3160/3520] TRAIN loss: 0.037\n",
      "[Iteration 3161/3520] TRAIN loss: 0.009\n",
      "[Iteration 3162/3520] TRAIN loss: 0.050\n",
      "[Iteration 3163/3520] TRAIN loss: 0.105\n",
      "[Iteration 3164/3520] TRAIN loss: 0.038\n",
      "[Iteration 3165/3520] TRAIN loss: 0.018\n",
      "[Iteration 3166/3520] TRAIN loss: 0.100\n",
      "[Iteration 3167/3520] TRAIN loss: 0.021\n",
      "[Iteration 3168/3520] TRAIN loss: 0.104\n",
      "[Epoch 9/10] TRAIN acc/loss: 0.972/0.104\n",
      "[Epoch 9/10] VAL   acc/loss: 0.980/0.067\n",
      "[Iteration 3169/3520] TRAIN loss: 0.026\n",
      "[Iteration 3170/3520] TRAIN loss: 0.061\n",
      "[Iteration 3171/3520] TRAIN loss: 0.032\n",
      "[Iteration 3172/3520] TRAIN loss: 0.011\n",
      "[Iteration 3173/3520] TRAIN loss: 0.037\n",
      "[Iteration 3174/3520] TRAIN loss: 0.010\n",
      "[Iteration 3175/3520] TRAIN loss: 0.047\n",
      "[Iteration 3176/3520] TRAIN loss: 0.031\n",
      "[Iteration 3177/3520] TRAIN loss: 0.008\n",
      "[Iteration 3178/3520] TRAIN loss: 0.030\n",
      "[Iteration 3179/3520] TRAIN loss: 0.005\n",
      "[Iteration 3180/3520] TRAIN loss: 0.048\n",
      "[Iteration 3181/3520] TRAIN loss: 0.013\n",
      "[Iteration 3182/3520] TRAIN loss: 0.011\n",
      "[Iteration 3183/3520] TRAIN loss: 0.011\n",
      "[Iteration 3184/3520] TRAIN loss: 0.024\n",
      "[Iteration 3185/3520] TRAIN loss: 0.018\n",
      "[Iteration 3186/3520] TRAIN loss: 0.008\n",
      "[Iteration 3187/3520] TRAIN loss: 0.005\n",
      "[Iteration 3188/3520] TRAIN loss: 0.051\n",
      "[Iteration 3189/3520] TRAIN loss: 0.041\n",
      "[Iteration 3190/3520] TRAIN loss: 0.016\n",
      "[Iteration 3191/3520] TRAIN loss: 0.034\n",
      "[Iteration 3192/3520] TRAIN loss: 0.011\n",
      "[Iteration 3193/3520] TRAIN loss: 0.031\n",
      "[Iteration 3194/3520] TRAIN loss: 0.008\n",
      "[Iteration 3195/3520] TRAIN loss: 0.058\n",
      "[Iteration 3196/3520] TRAIN loss: 0.007\n",
      "[Iteration 3197/3520] TRAIN loss: 0.009\n",
      "[Iteration 3198/3520] TRAIN loss: 0.026\n",
      "[Iteration 3199/3520] TRAIN loss: 0.088\n",
      "[Iteration 3200/3520] TRAIN loss: 0.075\n",
      "[Iteration 3201/3520] TRAIN loss: 0.052\n",
      "[Iteration 3202/3520] TRAIN loss: 0.060\n",
      "[Iteration 3203/3520] TRAIN loss: 0.005\n",
      "[Iteration 3204/3520] TRAIN loss: 0.021\n",
      "[Iteration 3205/3520] TRAIN loss: 0.012\n",
      "[Iteration 3206/3520] TRAIN loss: 0.024\n",
      "[Iteration 3207/3520] TRAIN loss: 0.078\n",
      "[Iteration 3208/3520] TRAIN loss: 0.068\n",
      "[Iteration 3209/3520] TRAIN loss: 0.035\n",
      "[Iteration 3210/3520] TRAIN loss: 0.051\n",
      "[Iteration 3211/3520] TRAIN loss: 0.059\n",
      "[Iteration 3212/3520] TRAIN loss: 0.045\n",
      "[Iteration 3213/3520] TRAIN loss: 0.027\n",
      "[Iteration 3214/3520] TRAIN loss: 0.004\n",
      "[Iteration 3215/3520] TRAIN loss: 0.010\n",
      "[Iteration 3216/3520] TRAIN loss: 0.057\n",
      "[Iteration 3217/3520] TRAIN loss: 0.046\n",
      "[Iteration 3218/3520] TRAIN loss: 0.039\n",
      "[Iteration 3219/3520] TRAIN loss: 0.031\n",
      "[Iteration 3220/3520] TRAIN loss: 0.033\n",
      "[Iteration 3221/3520] TRAIN loss: 0.043\n",
      "[Iteration 3222/3520] TRAIN loss: 0.016\n",
      "[Iteration 3223/3520] TRAIN loss: 0.017\n",
      "[Iteration 3224/3520] TRAIN loss: 0.015\n",
      "[Iteration 3225/3520] TRAIN loss: 0.034\n",
      "[Iteration 3226/3520] TRAIN loss: 0.050\n",
      "[Iteration 3227/3520] TRAIN loss: 0.025\n",
      "[Iteration 3228/3520] TRAIN loss: 0.016\n",
      "[Iteration 3229/3520] TRAIN loss: 0.011\n",
      "[Iteration 3230/3520] TRAIN loss: 0.037\n",
      "[Iteration 3231/3520] TRAIN loss: 0.025\n",
      "[Iteration 3232/3520] TRAIN loss: 0.032\n",
      "[Iteration 3233/3520] TRAIN loss: 0.007\n",
      "[Iteration 3234/3520] TRAIN loss: 0.047\n",
      "[Iteration 3235/3520] TRAIN loss: 0.075\n",
      "[Iteration 3236/3520] TRAIN loss: 0.044\n",
      "[Iteration 3237/3520] TRAIN loss: 0.031\n",
      "[Iteration 3238/3520] TRAIN loss: 0.022\n",
      "[Iteration 3239/3520] TRAIN loss: 0.057\n",
      "[Iteration 3240/3520] TRAIN loss: 0.045\n",
      "[Iteration 3241/3520] TRAIN loss: 0.021\n",
      "[Iteration 3242/3520] TRAIN loss: 0.020\n",
      "[Iteration 3243/3520] TRAIN loss: 0.014\n",
      "[Iteration 3244/3520] TRAIN loss: 0.034\n",
      "[Iteration 3245/3520] TRAIN loss: 0.060\n",
      "[Iteration 3246/3520] TRAIN loss: 0.023\n",
      "[Iteration 3247/3520] TRAIN loss: 0.014\n",
      "[Iteration 3248/3520] TRAIN loss: 0.013\n",
      "[Iteration 3249/3520] TRAIN loss: 0.009\n",
      "[Iteration 3250/3520] TRAIN loss: 0.034\n",
      "[Iteration 3251/3520] TRAIN loss: 0.016\n",
      "[Iteration 3252/3520] TRAIN loss: 0.067\n",
      "[Iteration 3253/3520] TRAIN loss: 0.017\n",
      "[Iteration 3254/3520] TRAIN loss: 0.053\n",
      "[Iteration 3255/3520] TRAIN loss: 0.066\n",
      "[Iteration 3256/3520] TRAIN loss: 0.060\n",
      "[Iteration 3257/3520] TRAIN loss: 0.034\n",
      "[Iteration 3258/3520] TRAIN loss: 0.025\n",
      "[Iteration 3259/3520] TRAIN loss: 0.033\n",
      "[Iteration 3260/3520] TRAIN loss: 0.010\n",
      "[Iteration 3261/3520] TRAIN loss: 0.012\n",
      "[Iteration 3262/3520] TRAIN loss: 0.011\n",
      "[Iteration 3263/3520] TRAIN loss: 0.043\n",
      "[Iteration 3264/3520] TRAIN loss: 0.019\n",
      "[Iteration 3265/3520] TRAIN loss: 0.016\n",
      "[Iteration 3266/3520] TRAIN loss: 0.023\n",
      "[Iteration 3267/3520] TRAIN loss: 0.003\n",
      "[Iteration 3268/3520] TRAIN loss: 0.024\n",
      "[Iteration 3269/3520] TRAIN loss: 0.075\n",
      "[Iteration 3270/3520] TRAIN loss: 0.030\n",
      "[Iteration 3271/3520] TRAIN loss: 0.027\n",
      "[Iteration 3272/3520] TRAIN loss: 0.065\n",
      "[Iteration 3273/3520] TRAIN loss: 0.118\n",
      "[Iteration 3274/3520] TRAIN loss: 0.022\n",
      "[Iteration 3275/3520] TRAIN loss: 0.036\n",
      "[Iteration 3276/3520] TRAIN loss: 0.030\n",
      "[Iteration 3277/3520] TRAIN loss: 0.032\n",
      "[Iteration 3278/3520] TRAIN loss: 0.023\n",
      "[Iteration 3279/3520] TRAIN loss: 0.066\n",
      "[Iteration 3280/3520] TRAIN loss: 0.009\n",
      "[Iteration 3281/3520] TRAIN loss: 0.014\n",
      "[Iteration 3282/3520] TRAIN loss: 0.082\n",
      "[Iteration 3283/3520] TRAIN loss: 0.007\n",
      "[Iteration 3284/3520] TRAIN loss: 0.038\n",
      "[Iteration 3285/3520] TRAIN loss: 0.015\n",
      "[Iteration 3286/3520] TRAIN loss: 0.079\n",
      "[Iteration 3287/3520] TRAIN loss: 0.036\n",
      "[Iteration 3288/3520] TRAIN loss: 0.003\n",
      "[Iteration 3289/3520] TRAIN loss: 0.021\n",
      "[Iteration 3290/3520] TRAIN loss: 0.007\n",
      "[Iteration 3291/3520] TRAIN loss: 0.010\n",
      "[Iteration 3292/3520] TRAIN loss: 0.032\n",
      "[Iteration 3293/3520] TRAIN loss: 0.049\n",
      "[Iteration 3294/3520] TRAIN loss: 0.015\n",
      "[Iteration 3295/3520] TRAIN loss: 0.023\n",
      "[Iteration 3296/3520] TRAIN loss: 0.050\n",
      "[Iteration 3297/3520] TRAIN loss: 0.017\n",
      "[Iteration 3298/3520] TRAIN loss: 0.038\n",
      "[Iteration 3299/3520] TRAIN loss: 0.013\n",
      "[Iteration 3300/3520] TRAIN loss: 0.011\n",
      "[Iteration 3301/3520] TRAIN loss: 0.021\n",
      "[Iteration 3302/3520] TRAIN loss: 0.040\n",
      "[Iteration 3303/3520] TRAIN loss: 0.006\n",
      "[Iteration 3304/3520] TRAIN loss: 0.007\n",
      "[Iteration 3305/3520] TRAIN loss: 0.012\n",
      "[Iteration 3306/3520] TRAIN loss: 0.037\n",
      "[Iteration 3307/3520] TRAIN loss: 0.013\n",
      "[Iteration 3308/3520] TRAIN loss: 0.017\n",
      "[Iteration 3309/3520] TRAIN loss: 0.014\n",
      "[Iteration 3310/3520] TRAIN loss: 0.034\n",
      "[Iteration 3311/3520] TRAIN loss: 0.051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 3312/3520] TRAIN loss: 0.054\n",
      "[Iteration 3313/3520] TRAIN loss: 0.010\n",
      "[Iteration 3314/3520] TRAIN loss: 0.021\n",
      "[Iteration 3315/3520] TRAIN loss: 0.057\n",
      "[Iteration 3316/3520] TRAIN loss: 0.015\n",
      "[Iteration 3317/3520] TRAIN loss: 0.030\n",
      "[Iteration 3318/3520] TRAIN loss: 0.048\n",
      "[Iteration 3319/3520] TRAIN loss: 0.004\n",
      "[Iteration 3320/3520] TRAIN loss: 0.034\n",
      "[Iteration 3321/3520] TRAIN loss: 0.053\n",
      "[Iteration 3322/3520] TRAIN loss: 0.014\n",
      "[Iteration 3323/3520] TRAIN loss: 0.056\n",
      "[Iteration 3324/3520] TRAIN loss: 0.025\n",
      "[Iteration 3325/3520] TRAIN loss: 0.086\n",
      "[Iteration 3326/3520] TRAIN loss: 0.051\n",
      "[Iteration 3327/3520] TRAIN loss: 0.027\n",
      "[Iteration 3328/3520] TRAIN loss: 0.021\n",
      "[Iteration 3329/3520] TRAIN loss: 0.115\n",
      "[Iteration 3330/3520] TRAIN loss: 0.075\n",
      "[Iteration 3331/3520] TRAIN loss: 0.026\n",
      "[Iteration 3332/3520] TRAIN loss: 0.116\n",
      "[Iteration 3333/3520] TRAIN loss: 0.054\n",
      "[Iteration 3334/3520] TRAIN loss: 0.034\n",
      "[Iteration 3335/3520] TRAIN loss: 0.050\n",
      "[Iteration 3336/3520] TRAIN loss: 0.065\n",
      "[Iteration 3337/3520] TRAIN loss: 0.065\n",
      "[Iteration 3338/3520] TRAIN loss: 0.080\n",
      "[Iteration 3339/3520] TRAIN loss: 0.080\n",
      "[Iteration 3340/3520] TRAIN loss: 0.092\n",
      "[Iteration 3341/3520] TRAIN loss: 0.089\n",
      "[Iteration 3342/3520] TRAIN loss: 0.099\n",
      "[Iteration 3343/3520] TRAIN loss: 0.064\n",
      "[Iteration 3344/3520] TRAIN loss: 0.008\n",
      "[Iteration 3345/3520] TRAIN loss: 0.028\n",
      "[Iteration 3346/3520] TRAIN loss: 0.030\n",
      "[Iteration 3347/3520] TRAIN loss: 0.067\n",
      "[Iteration 3348/3520] TRAIN loss: 0.071\n",
      "[Iteration 3349/3520] TRAIN loss: 0.163\n",
      "[Iteration 3350/3520] TRAIN loss: 0.103\n",
      "[Iteration 3351/3520] TRAIN loss: 0.073\n",
      "[Iteration 3352/3520] TRAIN loss: 0.056\n",
      "[Iteration 3353/3520] TRAIN loss: 0.037\n",
      "[Iteration 3354/3520] TRAIN loss: 0.020\n",
      "[Iteration 3355/3520] TRAIN loss: 0.054\n",
      "[Iteration 3356/3520] TRAIN loss: 0.053\n",
      "[Iteration 3357/3520] TRAIN loss: 0.028\n",
      "[Iteration 3358/3520] TRAIN loss: 0.042\n",
      "[Iteration 3359/3520] TRAIN loss: 0.073\n",
      "[Iteration 3360/3520] TRAIN loss: 0.035\n",
      "[Iteration 3361/3520] TRAIN loss: 0.098\n",
      "[Iteration 3362/3520] TRAIN loss: 0.029\n",
      "[Iteration 3363/3520] TRAIN loss: 0.023\n",
      "[Iteration 3364/3520] TRAIN loss: 0.046\n",
      "[Iteration 3365/3520] TRAIN loss: 0.021\n",
      "[Iteration 3366/3520] TRAIN loss: 0.033\n",
      "[Iteration 3367/3520] TRAIN loss: 0.012\n",
      "[Iteration 3368/3520] TRAIN loss: 0.053\n",
      "[Iteration 3369/3520] TRAIN loss: 0.034\n",
      "[Iteration 3370/3520] TRAIN loss: 0.016\n",
      "[Iteration 3371/3520] TRAIN loss: 0.045\n",
      "[Iteration 3372/3520] TRAIN loss: 0.029\n",
      "[Iteration 3373/3520] TRAIN loss: 0.022\n",
      "[Iteration 3374/3520] TRAIN loss: 0.053\n",
      "[Iteration 3375/3520] TRAIN loss: 0.024\n",
      "[Iteration 3376/3520] TRAIN loss: 0.058\n",
      "[Iteration 3377/3520] TRAIN loss: 0.038\n",
      "[Iteration 3378/3520] TRAIN loss: 0.018\n",
      "[Iteration 3379/3520] TRAIN loss: 0.066\n",
      "[Iteration 3380/3520] TRAIN loss: 0.004\n",
      "[Iteration 3381/3520] TRAIN loss: 0.036\n",
      "[Iteration 3382/3520] TRAIN loss: 0.087\n",
      "[Iteration 3383/3520] TRAIN loss: 0.007\n",
      "[Iteration 3384/3520] TRAIN loss: 0.018\n",
      "[Iteration 3385/3520] TRAIN loss: 0.063\n",
      "[Iteration 3386/3520] TRAIN loss: 0.057\n",
      "[Iteration 3387/3520] TRAIN loss: 0.027\n",
      "[Iteration 3388/3520] TRAIN loss: 0.052\n",
      "[Iteration 3389/3520] TRAIN loss: 0.021\n",
      "[Iteration 3390/3520] TRAIN loss: 0.060\n",
      "[Iteration 3391/3520] TRAIN loss: 0.023\n",
      "[Iteration 3392/3520] TRAIN loss: 0.022\n",
      "[Iteration 3393/3520] TRAIN loss: 0.057\n",
      "[Iteration 3394/3520] TRAIN loss: 0.017\n",
      "[Iteration 3395/3520] TRAIN loss: 0.055\n",
      "[Iteration 3396/3520] TRAIN loss: 0.021\n",
      "[Iteration 3397/3520] TRAIN loss: 0.018\n",
      "[Iteration 3398/3520] TRAIN loss: 0.039\n",
      "[Iteration 3399/3520] TRAIN loss: 0.160\n",
      "[Iteration 3400/3520] TRAIN loss: 0.091\n",
      "[Iteration 3401/3520] TRAIN loss: 0.018\n",
      "[Iteration 3402/3520] TRAIN loss: 0.005\n",
      "[Iteration 3403/3520] TRAIN loss: 0.005\n",
      "[Iteration 3404/3520] TRAIN loss: 0.024\n",
      "[Iteration 3405/3520] TRAIN loss: 0.031\n",
      "[Iteration 3406/3520] TRAIN loss: 0.079\n",
      "[Iteration 3407/3520] TRAIN loss: 0.066\n",
      "[Iteration 3408/3520] TRAIN loss: 0.068\n",
      "[Iteration 3409/3520] TRAIN loss: 0.030\n",
      "[Iteration 3410/3520] TRAIN loss: 0.044\n",
      "[Iteration 3411/3520] TRAIN loss: 0.042\n",
      "[Iteration 3412/3520] TRAIN loss: 0.021\n",
      "[Iteration 3413/3520] TRAIN loss: 0.033\n",
      "[Iteration 3414/3520] TRAIN loss: 0.018\n",
      "[Iteration 3415/3520] TRAIN loss: 0.016\n",
      "[Iteration 3416/3520] TRAIN loss: 0.013\n",
      "[Iteration 3417/3520] TRAIN loss: 0.036\n",
      "[Iteration 3418/3520] TRAIN loss: 0.066\n",
      "[Iteration 3419/3520] TRAIN loss: 0.024\n",
      "[Iteration 3420/3520] TRAIN loss: 0.029\n",
      "[Iteration 3421/3520] TRAIN loss: 0.015\n",
      "[Iteration 3422/3520] TRAIN loss: 0.135\n",
      "[Iteration 3423/3520] TRAIN loss: 0.004\n",
      "[Iteration 3424/3520] TRAIN loss: 0.014\n",
      "[Iteration 3425/3520] TRAIN loss: 0.036\n",
      "[Iteration 3426/3520] TRAIN loss: 0.022\n",
      "[Iteration 3427/3520] TRAIN loss: 0.032\n",
      "[Iteration 3428/3520] TRAIN loss: 0.017\n",
      "[Iteration 3429/3520] TRAIN loss: 0.013\n",
      "[Iteration 3430/3520] TRAIN loss: 0.157\n",
      "[Iteration 3431/3520] TRAIN loss: 0.085\n",
      "[Iteration 3432/3520] TRAIN loss: 0.035\n",
      "[Iteration 3433/3520] TRAIN loss: 0.048\n",
      "[Iteration 3434/3520] TRAIN loss: 0.032\n",
      "[Iteration 3435/3520] TRAIN loss: 0.022\n",
      "[Iteration 3436/3520] TRAIN loss: 0.010\n",
      "[Iteration 3437/3520] TRAIN loss: 0.008\n",
      "[Iteration 3438/3520] TRAIN loss: 0.069\n",
      "[Iteration 3439/3520] TRAIN loss: 0.073\n",
      "[Iteration 3440/3520] TRAIN loss: 0.084\n",
      "[Iteration 3441/3520] TRAIN loss: 0.011\n",
      "[Iteration 3442/3520] TRAIN loss: 0.018\n",
      "[Iteration 3443/3520] TRAIN loss: 0.011\n",
      "[Iteration 3444/3520] TRAIN loss: 0.010\n",
      "[Iteration 3445/3520] TRAIN loss: 0.011\n",
      "[Iteration 3446/3520] TRAIN loss: 0.049\n",
      "[Iteration 3447/3520] TRAIN loss: 0.043\n",
      "[Iteration 3448/3520] TRAIN loss: 0.010\n",
      "[Iteration 3449/3520] TRAIN loss: 0.046\n",
      "[Iteration 3450/3520] TRAIN loss: 0.103\n",
      "[Iteration 3451/3520] TRAIN loss: 0.012\n",
      "[Iteration 3452/3520] TRAIN loss: 0.018\n",
      "[Iteration 3453/3520] TRAIN loss: 0.017\n",
      "[Iteration 3454/3520] TRAIN loss: 0.009\n",
      "[Iteration 3455/3520] TRAIN loss: 0.026\n",
      "[Iteration 3456/3520] TRAIN loss: 0.060\n",
      "[Iteration 3457/3520] TRAIN loss: 0.039\n",
      "[Iteration 3458/3520] TRAIN loss: 0.018\n",
      "[Iteration 3459/3520] TRAIN loss: 0.016\n",
      "[Iteration 3460/3520] TRAIN loss: 0.045\n",
      "[Iteration 3461/3520] TRAIN loss: 0.038\n",
      "[Iteration 3462/3520] TRAIN loss: 0.063\n",
      "[Iteration 3463/3520] TRAIN loss: 0.025\n",
      "[Iteration 3464/3520] TRAIN loss: 0.113\n",
      "[Iteration 3465/3520] TRAIN loss: 0.030\n",
      "[Iteration 3466/3520] TRAIN loss: 0.017\n",
      "[Iteration 3467/3520] TRAIN loss: 0.016\n",
      "[Iteration 3468/3520] TRAIN loss: 0.013\n",
      "[Iteration 3469/3520] TRAIN loss: 0.064\n",
      "[Iteration 3470/3520] TRAIN loss: 0.009\n",
      "[Iteration 3471/3520] TRAIN loss: 0.048\n",
      "[Iteration 3472/3520] TRAIN loss: 0.076\n",
      "[Iteration 3473/3520] TRAIN loss: 0.010\n",
      "[Iteration 3474/3520] TRAIN loss: 0.004\n",
      "[Iteration 3475/3520] TRAIN loss: 0.008\n",
      "[Iteration 3476/3520] TRAIN loss: 0.012\n",
      "[Iteration 3477/3520] TRAIN loss: 0.003\n",
      "[Iteration 3478/3520] TRAIN loss: 0.209\n",
      "[Iteration 3479/3520] TRAIN loss: 0.072\n",
      "[Iteration 3480/3520] TRAIN loss: 0.026\n",
      "[Iteration 3481/3520] TRAIN loss: 0.046\n",
      "[Iteration 3482/3520] TRAIN loss: 0.042\n",
      "[Iteration 3483/3520] TRAIN loss: 0.045\n",
      "[Iteration 3484/3520] TRAIN loss: 0.006\n",
      "[Iteration 3485/3520] TRAIN loss: 0.060\n",
      "[Iteration 3486/3520] TRAIN loss: 0.080\n",
      "[Iteration 3487/3520] TRAIN loss: 0.048\n",
      "[Iteration 3488/3520] TRAIN loss: 0.018\n",
      "[Iteration 3489/3520] TRAIN loss: 0.055\n",
      "[Iteration 3490/3520] TRAIN loss: 0.141\n",
      "[Iteration 3491/3520] TRAIN loss: 0.008\n",
      "[Iteration 3492/3520] TRAIN loss: 0.023\n",
      "[Iteration 3493/3520] TRAIN loss: 0.020\n",
      "[Iteration 3494/3520] TRAIN loss: 0.073\n",
      "[Iteration 3495/3520] TRAIN loss: 0.057\n",
      "[Iteration 3496/3520] TRAIN loss: 0.025\n",
      "[Iteration 3497/3520] TRAIN loss: 0.076\n",
      "[Iteration 3498/3520] TRAIN loss: 0.101\n",
      "[Iteration 3499/3520] TRAIN loss: 0.034\n",
      "[Iteration 3500/3520] TRAIN loss: 0.055\n",
      "[Iteration 3501/3520] TRAIN loss: 0.100\n",
      "[Iteration 3502/3520] TRAIN loss: 0.025\n",
      "[Iteration 3503/3520] TRAIN loss: 0.058\n",
      "[Iteration 3504/3520] TRAIN loss: 0.086\n",
      "[Iteration 3505/3520] TRAIN loss: 0.045\n",
      "[Iteration 3506/3520] TRAIN loss: 0.044\n",
      "[Iteration 3507/3520] TRAIN loss: 0.086\n",
      "[Iteration 3508/3520] TRAIN loss: 0.029\n",
      "[Iteration 3509/3520] TRAIN loss: 0.034\n",
      "[Iteration 3510/3520] TRAIN loss: 0.028\n",
      "[Iteration 3511/3520] TRAIN loss: 0.050\n",
      "[Iteration 3512/3520] TRAIN loss: 0.020\n",
      "[Iteration 3513/3520] TRAIN loss: 0.117\n",
      "[Iteration 3514/3520] TRAIN loss: 0.013\n",
      "[Iteration 3515/3520] TRAIN loss: 0.025\n",
      "[Iteration 3516/3520] TRAIN loss: 0.021\n",
      "[Iteration 3517/3520] TRAIN loss: 0.060\n",
      "[Iteration 3518/3520] TRAIN loss: 0.052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 3519/3520] TRAIN loss: 0.043\n",
      "[Iteration 3520/3520] TRAIN loss: 0.024\n",
      "[Epoch 10/10] TRAIN acc/loss: 0.986/0.024\n",
      "[Epoch 10/10] VAL   acc/loss: 0.983/0.054\n",
      "FINISH.\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.rnn.rnn_nn import LSTM_Classifier\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_dset,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                dataset=val_dset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "\n",
    "solver = Solver(optim_args={\"lr\": 1e-3})\n",
    "\n",
    "model= LSTM_Classifier(classes = num_classes, input_size = 28)\n",
    "\n",
    "# train rnn model\n",
    "solver.train(model, train_loader, val_loader, log_nth=100, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your LSTM model again and see wether it improves performance on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your Model\n",
    "When you are satisfied with your training, you can save the model. In order to be eligible for the bonus points you have to achieve a score higher than __95__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "When you are satisfied with your training, you can save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model... models/rnn_mnist_nn.model\n"
     ]
    }
   ],
   "source": [
    "model.save(\"models/rnn_mnist_nn.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
